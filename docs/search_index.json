[
["los-beatles-del-machine-learning.html", "7 Los beatles del Machine Learning 7.1 Machine Learning 7.2 ¬øC√≥mo funciona un √°rbol de decisi√≥n? 7.3 ¬øPodemos predecir qui√©n se muri√≥ en el Titanic? 7.4 Aplicaci√≥n en el mercado de trabajo: monotributistas y cuentapropistas informales 7.5 Algunos √°rboles no solo clasifican: √°rboles de regresi√≥n 7.6 Ejercicio 7.7 Lecturas recomendadas", " 7 Los beatles del Machine Learning Si la regresi√≥n lineal es el autom√≥vil de la estad√≠stica o, como dice Walter Sosa Escudero, los ‚ÄúRolling Stones‚Äù de esa disciplina cient√≠fica, podr√≠amos decir que los √°rboles de decisi√≥n, quiz√°s la familia de t√©cnicas de machine learning m√°s famosa del mundo, son los Beatles del aprendizaje autom√°tico (traducci√≥n al espa√±ol de *machine learning). En esta clase vamos a tener una introducci√≥n a qu√© hacen, c√≥mo lo hacen y para qu√© sirven. 7.1 Machine Learning El t√©rmino Machine Learning debe ser uno de los m√°s nombrados en los √∫ltimos a√±os, junto a Inteligencia Artificial. Aunque no hay una clara definci√≥n de ambos conceptos, vamos a definir al segundo como ‚Äúla habilidad de las maquinas de comportarse de una manera que nosotros consideramos inteligente‚Äù. Con respecto al primer concepto, mucho m√°s estrecho, lo vamos a definir como ‚ÄúLa capacidad de un programa de aprender a hacer una tarea cada vez mejor en base a la experiencia‚Äù, cerca de la definici√≥n del libro de Tom Mitchell, Machine Learning (2017). Notemos que el programa es quien aprende desde la experiencia: nosotros no intervenimos activamente en ese proceso de aprendizaje. Eso es lo que hace especial al Aprendizaje Autom√°tico (traducci√≥n de machine learning) En t√©rminos de Mitchell, ‚ÄúSe dice que un programa de computadora aprende de la experiencia (E) con respecto a una determinada clase de tarea (T) y medida de performance (P) si su performance en la tarea (T), medido por P, mejora con la experiencia E‚Äù. En definitiva: Machine Learning es la posibilidad de un programa de computadora de hacer cada vez mejor su trabajo en base a una determinada m√©trica. 7.2 ¬øC√≥mo funciona un √°rbol de decisi√≥n? Si alguna vez jugaron al ¬øQui√©n es Qui√©n? conocen la principal caracter√≠stica de un √°rbol de decisi√≥n: hace preguntas que pueden ser respondidas con ‚Äúsi o no‚Äù (binarias) de tal manera de separar a todas las observaciones (en este caso, los nombres de los personajes) en base a las distintas variables que tienen (color de pelo, si usa o no anteojos, sexo, entre otras). De esta manera, tanto nuestra estrategia en el qui√©n es qui√©n como la de los √°rboles de decisi√≥n coinciden en dividir al espacio de nuestros datos en ‚Äúsegmentos‚Äù de acuerdo a los valores que toman en las distintas variables. Lo que muestra el gr√°fico 1 es un √°rbol de decisi√≥n del Qui√©n es Qui√©n, suponiendo que el personaje que nos toc√≥ es una mujer con anteojos (y hay solo una en todo el tablero). Esto que hacemos intuitivamente en jerga estad√≠stica se conoce como Recursive Partitioning. Ahora bien, nuestro objetivo en el juego es identificar a la persona que nos toc√≥. Ac√° es donde comienzan las diferencias con respecto a los √°rboles de decisi√≥n. Por un lado, en el qui√©n es qui√©n nosotros, de manera activa, vamos haciendo las preguntas. Por otro lado, si aprendemos a jugar bien probablemente hagamos preguntas en las cuales la respuesta de s√≠ o no nos elimine a la mayor cantidad de casos. Pero un √°rbol de decisi√≥n no requiere nuestra intervenci√≥n, de all√≠ la parte de ‚Äúautom√°tico‚Äù en aprendizaje autom√°tico: tiene reglas claras para ir haciendo las preguntas necesarias para hacer la tarea de ‚Äúencontrar‚Äù a nuestro personaje cada vez mejor. Por otro lado, no le interesa conocer d√≥nde est√° esa √∫nica persona, sino que el objetivo es aprender a clasificar cada vez mejor a cierta variable objetivo. Por ejemplo, imaginen que en lugar de encontrar a ‚ÄúClara‚Äù el objetivo sea encontrar a ‚ÄúMujeres‚Äù. Quiz√°s en el Qui√©n es Qui√©n dentro de las personas que tienen pelo largo hay m√°s mujeres que hombres y pueda usarse para eso. De hecho, los √°rboles de decisi√≥n hacen exactamente esto √∫ltimo. Buscan ir segmentando el espacio de nuestras variables en distintos pedazos que logren aislar a las categorias de nuestra variable objetivo (lo que queremos predecir) de una manera m√°s homog√©nea. En nuestro caso de crear un √°rbol para encontrar a las mujeres, desear√≠amos ir segmentando a las personas seg√∫n preguntas cuya respuesta nos separe todos hombres o todas mujeres (o lo m√°s cercano a eso). Veamos todo esto con un ejemplo cinematogr√°fico. 7.3 ¬øPodemos predecir qui√©n se muri√≥ en el Titanic? En abril de 1912 el RMS Titanic choc√≥ contra un iceberg y m√°s de 800 de los pasajeros murieron, mientras que aproximadamente 500 sobrevivieron ¬øPodemos crear un √°rbol de decisi√≥n que nos permita predecir quienes sobrevivieron y quienes no en base a variables como su edad, g√©nero y clase en la que viajaron? Probemoslo con el conocido dataset que simula a los pasajeros del Titanic y las variables con las que vamos a entrenar a nuestro √°rbol de decisi√≥n. titanic &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/titanic.csv&quot;, stringsAsFactors = FALSE, sep = &#39;;&#39;) Tambi√©n vamos a cargar el paquete que nos va a permitir crear nuestro primer modelo de √°rboles de decisiones C50 (noten la C may√∫scula en C50) y tidyverse: library(tidyverse) library(C50) Exploren un poco qu√© tiene el dataset de Titanic con el siguiente c√≥digo: glimpse(titanic) ## Rows: 1,045 ## Columns: 4 ## $ survived [3m[38;5;246m&lt;int&gt;[39m[23m 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, ... ## $ age [3m[38;5;246m&lt;dbl&gt;[39m[23m 29.0000, 0.9167, 2.0000, 30.0000, 25.0000, 48.0000, 63.0000, 39.0000, 53.0000, 71.0000, 47... ## $ sex [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, ... ## $ fare [3m[38;5;246m&lt;dbl&gt;[39m[23m 211.3375, 151.5500, 151.5500, 151.5500, 151.5500, 26.5500, 77.9583, 0.0000, 51.4792, 49.50... Las variables son bastante obvias, pero antes que tenemos que hacer un poco de data wrangling, en este caso bastante menor. El paquete C5.0 trabaja mejor con factores como predictoras (las que nos van a ayudar a predecir si una persona sobrevive o no al accidente del Titanic), pero tambi√©n nos exige que este expresada en ese formato la variable objetivo (en nuestro caso, survived). Por esta raz√≥n vamos a convertir ambas variables: titanic &lt;- titanic %&gt;% mutate(survived = factor(survived), sex = factor(sex)) Ya estamos en condiciones de entrenar nuestro primer √°rbol de decisi√≥n en R. La funci√≥n que entrena al √°rbol se llama C5.0() y usa un sistema de f√≥rmula muy similar al que se vio en el cap√≠tulo 4 cuando introdujimos a las regresiones: primerArbol &lt;- C5.0(formula= survived ~., data = titanic) Para ver qu√© tiene nuestro √°rbol, primero vamos a graficarlo. Esto lo podemos hacer con la funci√≥n plot() plot(primerArbol) Un √°rbol de decisi√≥n est√° compuesto de nodos. Los que est√°n al final, cuando no se hacen m√°s bifurcaciones en nuestro dataset, se llaman hojas del √°rbol. Podemos ver que la primera pregunta que hace es si la persona es hombre o mujer. En caso que sea mujer, la siguiente pregunta es sobre distintos valores de la tarifa que se pag√≥. En caso que sea hombre, la pregunta tiene que ver con la edad. Las hojas del gr√°fico est√°n acompa√±adas de una barra que muestra la proporci√≥n que sobrevivi√≥ (gris oscuro) y la que no lo hizo (gris claro). Por ejemplo, podemos ver que la hoja donde se concentra la mayor proporci√≥n de sobrevivientes son las mujeres con una tarifa superior a USD 47.1, mientras que la mayor proporci√≥n de muertes se encuentran entre los hombres mayores a 9 a√±os. ¬øC√≥mo elige un √°rbol de decisi√≥n por cu√°l variable y por cu√°les valores de esas variables abrir? elige aquellos cortes de nuestros datos que dejan m√°s homog√©neos a la nueva clasificaci√≥n que la que hab√≠a antes de hacer el quiebre. Para esto usa el importante concepto de entropia, la cual no desarrollaremos en profundidad pero basta con decir que es una medida que describe qu√© tan homogeneo es un conjunto de datos. Mientras m√°s bajo sea m√°s homog√©neo es. Veamos c√≥mo se calcula para el total de nuestros datos proporcionSobrevivientes &lt;- table(titanic$survived)[2]/nrow(titanic) proporcionSobrevivientes # Aproximadamente un 41% de los pasajeros sobrevivieron ## 1 ## 0.4086124 # Formula de Entrop√≠a -0.59*log2(0.59)-(0.41)*log2(0.41) ## [1] 0.9765005 La entrop√≠a de nuestra base de datos es alta porque est√° muy cerca de estar distribuida como 50% y 50%, la situaci√≥n m√°s ‚Äúheterog√©nea‚Äù que puede tener nuestra variable objetivo. De hecho, si se calcula la entrop√≠a de esa situaci√≥n llegamos a lo siguiente: -0.5*log2(0.5)-(0.5)*log2(0.5) # M√°xima entrop√≠a ## [1] 1 ¬øY si tenemos todo de una sola clase (por ejemplo, solo sobrevivientes)? -0.000001*log2(0.000001)-(1)*log2(1) # Casi cero ## [1] 1.993157e-05 Bien, ahora veamos qu√© pasa con la entropia si abrimos, como hizo nuestro √°rbol, seg√∫n el g√©nero. Para esto, tenemos que sumar las proporciones al final de cada hoja: table(titanic$survived,titanic$sex) ## ## female male ## 0 96 522 ## 1 292 135 Ahora podr√≠amos calcular la entrop√≠a en cada una de las hojas del √°rbol. Vayamos primero con el de mujeres: # Entrop√≠a mujeres -(96/(292+96))*log2(96/(292+96))-(292/(292+96))*log2(292/(292+96)) ## [1] 0.8071676 entropiaMujeres &lt;- -(96/(292+96))*log2(96/(292+96))-(292/(292+96))*log2(292/(292+96)) ¬øY en los hombres? # Entrop√≠a hombres -(522/(522+135))*log2(522/(522+135))-(135/(522+135))*log2(135/(522+135)) ## [1] 0.7327525 entropiaHombres &lt;- -(522/(522+135))*log2(522/(522+135))-(135/(522+135))*log2(135/(522+135)) Ahora debemos ponderar la entrop√≠a de la variable ponderando las dos hojas que abri√≥: entropiaGenero &lt;- entropiaHombres*(657/1045)+entropiaMujeres*(388/1045) entropiaGenero ## [1] 0.7603822 La apertura por g√©nero da una entropia de 0.76, mientras que aquella que no abre por nada tiene una de 0.9765 ¬øC√≥mo medimos esta mejora? En lo que se conoce como Information Gain, que es tan solo la mejora en la entropia por abrir por una determinada variable con respecto a la entrop√≠a antes de abrir. informationGainGenero &lt;- 0.9765-entropiaGenero informationGainGenero ## [1] 0.2161178 En el caso de Genero la mejora es de 0.216 y les garantizo que es la mayor de la apertura de todas las variables, ya que as√≠ trabaja C5.0 7.3.1 ¬øC√≥mo podemos medir qu√© tan bien clasifica nuestro √°rbol? Existen diversas maneras de medir la efectividad de la clasificaci√≥n de un modelo de machine learning. Para este tipo de objetivo (clasificar) suele ser √∫til usar la matriz de confusi√≥n, que simplemente distribuye en celdas la clasificaci√≥n de un determinado caso y el valor que ten√≠a en nuestro dataset. Podemos acceder a ella mediante el m√©todo summary() aplicado a nuestro √°rbol summary(primerArbol) ## ## Call: ## C5.0.formula(formula = survived ~ ., data = titanic) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue May 12 12:30:17 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1045 cases (4 attributes) from undefined.data ## ## Decision tree: ## ## sex = male: ## :...age &lt;= 9: 1 (43/18) ## : age &gt; 9: 0 (614/110) ## sex = female: ## :...fare &gt; 47.1: 1 (118/3) ## fare &lt;= 47.1: ## :...fare &gt; 10.4625: 1 (197/56) ## fare &lt;= 10.4625: ## :...fare &lt;= 7.725: 1 (16/3) ## fare &gt; 7.725: 0 (57/23) ## ## ## Evaluation on training data (1045 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 6 213(20.4%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 538 80 (a): class 0 ## 133 294 (b): class 1 ## ## ## Attribute usage: ## ## 100.00% sex ## 62.87% age ## 37.13% fare ## ## ## Time: 0.0 secs Ya nos dice que tiene una tasa de error de 20,4% ¬øC√≥mo podemos ver esto en la tabla? si sumamos los falsos positivos y los falsos negativos, que est√°n en las celdas de abajo a la izquierda y arriba a la derecha (133+80) y lo dividimos por todos los casos que clasific√≥. ¬øEs esto mucho o poco? Para responder esta pregunta es siempre necesario pensar c√≥mo se distribu√≠a la variable en nuestro dataset. Ya sabemos que aproximadamente el 41% de las personas se salv√≥, por lo que si clasificaramos a todos como sobrevivientes, tendr√≠amos una tasa de acierto del 41% y una tasa de error del 59%. Con nuestro √°rbol de decisi√≥n ahora tenemos una tasa de error de 20,4% (y de acierto de 79,6%)! Otra forma de pensar esto es mediante el lift que es la divisi√≥n entre la proporci√≥n de acierto en nuestro √°rbol y la del dataset original: 79.6/41= 1.94. Cualquier valor mayor a uno muestra que la tasa de acierto es mayor a la del denominador. 7.3.2 Un √°rbol puede reducirse a reglas Una de las principales ventajas de los √°rboles de decisi√≥n de este estilo es que podemos reducir su complejidad a un conjunto de reglas que nos permite clasificar los casos. Para esto solo tenemos que cambiar un par√°metro al entrenar el √°rbol de decisi√≥n primerArbol &lt;- C5.0(formula= survived ~., data = titanic, rules=TRUE) summary(primerArbol) ## ## Call: ## C5.0.formula(formula = survived ~ ., data = titanic, rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue May 12 12:30:17 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1045 cases (4 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (614/110, lift 1.4) ## age &gt; 9 ## sex = male ## -&gt; class 0 [0.820] ## ## Rule 2: (246/54, lift 1.3) ## fare &gt; 7.725 ## fare &lt;= 10.4625 ## -&gt; class 0 [0.778] ## ## Rule 3: (315/59, lift 2.0) ## sex = female ## fare &gt; 10.4625 ## -&gt; class 1 [0.811] ## ## Rule 4: (16/3, lift 1.9) ## sex = female ## fare &lt;= 7.725 ## -&gt; class 1 [0.778] ## ## Rule 5: (82/32, lift 1.5) ## age &lt;= 9 ## -&gt; class 1 [0.607] ## ## Default class: 0 ## ## ## Evaluation on training data (1045 cases): ## ## Rules ## ---------------- ## No Errors ## ## 5 215(20.6%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 538 80 (a): class 0 ## 135 292 (b): class 1 ## ## ## Attribute usage: ## ## 90.43% sex ## 66.60% age ## 55.22% fare ## ## ## Time: 0.0 secs No todos los modelos de Machine Learning tienen la posiblidad de mostrar de manera tan intuitiva las reglas para clasificar o predecir un determinado caso. Esta es una importante ventaja de los √°rboles de decisi√≥n. Algo importante a aclarar de estas reglas es que no son exactamente las mismas que las que componen el √°rbol y, adem√°s, un caso puede estar cubierto m√°s de una vez por alguna de las reglas. Esto es porque al no estar ‚Äúobligado‚Äù a mostrar bifurcaciones en el √°rbol de decisi√≥n, lo que entrega son reglas y, al clasificar, elige la que tiene mayor accuracy. 7.4 Aplicaci√≥n en el mercado de trabajo: monotributistas y cuentapropistas informales El sistema estad√≠stico nacional tiene un serio problema para captar la naturaleza del trabajo independiente a lo largo del pa√≠s. Una excepci√≥n a este problema generalizado fue la ENAPROSS del a√±o 2015, en la cual se pregunt√≥ a los trabajadores independientes, entre otra cosas, si facturaban por su trabajo o no, es decir si eran monotributistas o no. Podemos aprender de esta encuesta para luego predecir, en base a variables que s√≠ est√°n en otras encuestas, como la Encuesta Permanente de Hogares (EPH). Usemos lo que aprendimos sobre el algoritmo C5.0 y los √°rboles de decisi√≥n m√°s en general. load(file=url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/independientes.RData&quot;)) str(independientes) ## &#39;data.frame&#39;: 2783 obs. of 6 variables: ## $ NIVEL_ED : Factor w/ 7 levels &quot;Sin_instruccion&quot;,..: 3 4 4 4 3 5 2 3 4 4 ... ## $ REGISTRADO: Factor w/ 2 levels &quot;No_registrado&quot;,..: 1 1 1 1 1 2 1 1 1 1 ... ## $ INGRESO : Factor w/ 10 levels &quot;Decil1&quot;,&quot;Decil2&quot;,..: 1 10 1 3 7 7 1 8 5 7 ... ## $ EDAD : int 54 30 20 21 32 38 67 27 22 25 ... ## $ CAT_OCUP : Factor w/ 2 levels &quot;Patron&quot;,&quot;Independiente&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ REGION : Factor w/ 3 levels &quot;CABA&quot;,&quot;CONURBANO&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... El data frame independientes es una muestra de la ENAPROSS 2015, una encuesta a nivel nacional cuyo objetivo fue relevar ciertas caracter√≠sticas relacionados con la cobertura y calidad de la seguridad social en la Argentina y el empleo, entre otras condiciones sociales. Ac√° tenemos seis variables: el nivel educativo, si el trabajador independiente se encuentra registrado o no, el ingreso (seg√∫n decil), la edad en a√±os cumplidos, la categor√≠a ocupacional (en este caso, si es independiente o patr√≥n) y la Regi√≥n del pa√≠s, que en este caso queda segmentada entre CABA, CONURBANO y RESTO DEL PA√çS. primerArbol &lt;- C5.0(formula = REGISTRADO ~., data = independientes) summary(primerArbol) ## ## Call: ## C5.0.formula(formula = REGISTRADO ~ ., data = independientes) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue May 12 12:30:18 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 2783 cases (6 attributes) from undefined.data ## ## Decision tree: ## ## NIVEL_ED = Superior_completo: Registrado (235/38) ## NIVEL_ED in {Sin_instruccion,Primaria_incompleta,Primaria_completa, ## : Secundaria_incompleta,Secundaria_completa,Superior_incompleta}: ## :...INGRESO in {Decil1,Decil2,Decil3,Decil4,Decil5,Decil6, ## : Decil7}: No_registrado (1984/320) ## INGRESO in {Decil8,Decil9,Decil10}: ## :...NIVEL_ED = Sin_instruccion: Registrado (0) ## NIVEL_ED in {Secundaria_completa,Superior_incompleta}: ## :...CAT_OCUP = Patron: Registrado (68/7) ## : CAT_OCUP = Independiente: ## : :...EDAD &lt;= 34: No_registrado (59/24) ## : EDAD &gt; 34: Registrado (194/48) ## NIVEL_ED in {Primaria_incompleta,Primaria_completa, ## : Secundaria_incompleta}: ## :...INGRESO = Decil8: No_registrado (100/25) ## INGRESO in {Decil9,Decil10}: ## :...CAT_OCUP = Patron: Registrado (17/4) ## CAT_OCUP = Independiente: ## :...INGRESO = Decil9: No_registrado (80/32) ## INGRESO = Decil10: Registrado (46/18) ## ## ## Evaluation on training data (2783 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 9 516(18.5%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 1822 115 (a): class No_registrado ## 401 445 (b): class Registrado ## ## ## Attribute usage: ## ## 100.00% NIVEL_ED ## 91.56% INGRESO ## 16.67% CAT_OCUP ## 9.09% EDAD ## ## ## Time: 0.0 secs Si les es m√°s f√°cil para entenderlo, pueden plotearlo ¬øQu√© podemos decir del √°rbol que se cre√≥? Enfoqu√©monos en Attribute usage: lo que hace es asignar la importancia de las variables seg√∫n cu√°ntos casos fueron clasificados usando a esa varaible. Por ejemplo, EDAD es usado en 253 (59+194) casos, que dividido por los 2783 casos que tenemos en este dataset dan 9,09%. De manera trivial, la primera de las variables es usada para clasificar todos los casos, por lo cual tiene 100% de importancia. Podr√≠amos concluir que para nuestro modelo el nivel educativo y los ingresos son variables claves para asignar a un trabajador independiente como formal o no. Por otro lado podemos ver que tiene una tasa de error de 18,5% ¬øEs mucho o poco? De nuevo, averiguemos cu√°ntos trabajadores no registrados hay en nuestro dataset: table(independientes$REGISTRADO)/nrow(independientes) ## ## No_registrado Registrado ## 0.6960115 0.3039885 El 69.6% de los trabajadores independientes en nuestro dataset no se encuentra registrado, con lo cual si dijeramos que todos los trabajadores independientes son no regisitrados nos equivocar√≠amos en 30,4%. Nuestro √°rbol de decisi√≥n lleg√≥ a reducirlo al 18,5% Usemos nuestro modelo, ahora, para predecir nuestro dataset. La funci√≥n predict hace exactamente esto: independientes &lt;- independientes %&gt;% mutate(PREDICCION = predict(primerArbol, newdata = independientes %&gt;% select(-REGISTRADO))) table(independientes$REGISTRADO, independientes$PREDICCION) ## ## No_registrado Registrado ## No_registrado 1822 115 ## Registrado 401 445 Esta es una tabla de confusi√≥n, como la que anteriormente vimos en el ejemplo del Titanic con summary(). Las filas indican la clasificaci√≥n ‚Äúreal‚Äù de los casos, mientras que las columnas indican la que asign√≥ nuestro modelo. La diagonal principal indica los casos correctamente clasificados, mientras que el que est√° arriba a la derecha nos marcan los falsos positivos, mientras que el elemento de abajo a la izquierda indica los falsos negativos. Si sumamos la diagonal (los correctamente clasificados) y lo dividimos por el total de casos obtenemos una importa medida de la performance de nuestro modelo: la accuracy sum(diag(table(independientes$REGISTRADO, independientes$PREDICCION))) / nrow(independientes) * 100 ## [1] 81.45886 La accuracy es solo una forma de medir la performance de nuestro modelo y nos va a servir en este tutorial para elegir entre modelos: el que tenga mayor accuracy es el que vamos a elegir. En este caso tenemos una accuracy de 81,5%, lo que implica que nuestro modelo tiene un lift de 81,5/69,6=1,18. Nuestro modelo es un 18% mejor que haber asignados a todos los casos con la proporci√≥n que conocemos de nuestra muestra. 7.4.1 Overfitting: aprendiendo demasiado de nuestra muestra Si tienen que estudiar para un examen en alg√∫n momento de su formaci√≥n es muy probable que lo hayan hecho a trav√©s de modelos de ex√°menes anteriores. El objetivo no es solo pr√°cticar lo que vieron en el curso, sino aprender sobre c√≥mo toma examen la docemente. En general, esto suele funcionar, pero tiene un l√≠mite al cual probablemente llegaron: si aprenden estrictamente a resolver los parciales que tuvieron como prueba es sumamente probable que solo sepan responder con eficiencia esos parciales pero no otros con peque√±as diferencias. Como los humanos, los algoritmos pueden caer en el problema de aprender demasiado las especificidades de una muestra. El overfitting es uno de los principales problemas al entrenar un modelo de aprendizaje autom√°tico. Debemos garantizar que nuestro modelo NO funciona solo para la muestra, sino que los nuevos casos - los que queremos producir - tambi√©n ser√°n predichos de una manera razonable. De hecho, lo √∫nico que nos importa es la accuracy sobre una parte de la muestra que separamos y llamamos dataset de testing. Lo que pasa sobre nuestro dataset de training es secundario y solo lo utilizamos para detectar signos de overfiting. Vamos a ver un caso en el que la mejora en la eficiencia en training no redunda en mejoras en testing, es decir un caso de overfitting. No se preocupen por el c√≥digo, es un poco complejo pero m√°s adelante vamos a usar a la librer√≠a caret para que haga todo este trabajo de una manera m√°s eficiente que nosotros. El c√≥digo lo que hace es ir realizando una grid search en alguno de los par√°metros de nuestro modelo. Los par√°metros de los modelos definen, entre otras cosas, la estructura y la ‚Äúvelocidad‚Äù de aprendizaje del √°rbol, aunque siempre son espec√≠ficas a los modelos. Una b√∫squeda en grid search (b√∫squeda en grilla) solo prueba un mont√≥n de valores para distintos par√°metros y testea su accuracy. Una vez que se encuentra el valor m√°ximo, esos ser√°n los par√°metros elegidos del modelo. No se preocupen si les lleva un tiempo, es natural ya que est√° entrenando muchos modelos # Eliminamos la variable que tiene la predecci√≥n independientes &lt;- independientes %&gt;% select(-PREDICCION) set.seed(2) # Creamos la &quot;Grid Search&quot; de dos par√°metros cfOpciones &lt;- seq(0.8,1,0.01) minCasesOpciones &lt;- seq(0,50,1) # Generamos los √≠ndices (n√∫meros de filas) que van a ser de testing indexTest &lt;- sample.int(n = nrow(independientes),size = 0.3*nrow(independientes)) independientesTest &lt;- independientes[indexTest, ] independientesTraining &lt;- independientes[-indexTest, ] modelPerformance &lt;- list() for(cf in cfOpciones){ for(minCases in minCasesOpciones) { # Para cambiar los par√°metros presten atenci√≥n a que debemos usar la funci√≥n C5.0Control model &lt;- C5.0(REGISTRADO ~., data = independientesTraining, control= C5.0Control(CF = cf, minCases = minCases)) prediccionesTrain &lt;- predict(model, independientesTraining) trainAcc &lt;- sum(prediccionesTrain==independientesTraining$REGISTRADO)/nrow(independientesTraining) prediccionesTest &lt;- predict(model, newdata = independientesTest) testAcc &lt;- sum(prediccionesTest==independientesTest$REGISTRADO)/nrow(independientesTest) salida &lt;- data.frame(cf, minCases,trainAcc,testAcc) modelPerformance &lt;- c(modelPerformance, list(salida)) } } modelPerformance &lt;- plyr::rbind.fill(modelPerformance) Ahora veamos c√≥mo fue la evoluci√≥n de la accuracy tanto en training como testing (y de paso aprendemos un poco m√°s sobre ggplot2) modelPerformance &lt;- modelPerformance %&gt;% group_by(minCases) %&gt;% summarise(Training = mean(trainAcc), Testing = mean(testAcc)) %&gt;% gather(key = &quot;dataset&quot;,value=&quot;acc&quot;,-minCases) # Esta librer√≠a nos da la opci√≥n de agregar nuevos &quot;temas&quot; de ggplot # que no vienen con la librer√≠a library(ggthemes) ggplot(modelPerformance) + geom_line(aes(x = minCases,y = acc, color = dataset), size = 1.5) + theme_fivethirtyeight() + scale_color_fivethirtyeight() + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + scale_x_reverse() + labs(title = &quot;La forma del overfitting&quot;, subtitle = &quot;Accuracy seg√∫n el valor del par√°metro minCases&quot;, caption = &quot;Elaboraci√≥n propia con base en datos de ENAPROSS 2015&quot;) + theme(legend.title = element_blank()) En el gr√°fico queda bastante claro como desde aproximadamente el valor minCases = 15 la accuracy en el dataset de testing crece sin parar pasando de aproximadamente 81% a 86%, mientras que la de testing tiene una leve tendencia a la ca√≠da. En este caso, estos par√°metros no muestran un elevado overfitting. En otras situaciones, el overfitting puede ser tal que la accuracy sobre el dataset de testing caiga (y mucho) siempre hay que tenerlo en cuenta. 7.5 Algunos √°rboles no solo clasifican: √°rboles de regresi√≥n Aunque suene contraintuitivo, algunos √°rboles de decisi√≥n pueden dividir el espacio de nuestras variables en base a valores no solo categ√≥ricos (como cuando clasificamos), sino en valores num√©ricos continuos. Aunque suene raro, veremos que lo que hace es relativamente f√°cil de comprender. Para esto, vamos a trabajar con un dataset sobre el precio de los inmuebles en la Ciudad de Buenos Aires que descargu√© desde la divisi√≥n de datos de Properati. Pero para eso vamos a tener que hacer un Data Wrangling un poco m√°s intenso. 7.5.1 Poniendo en forma los datos Los datos que descargu√© pueden bajarlos ustedes, como siempre, con read.table(): avisosInmuebles &lt;-read.table(file = url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/datosProperati.csv&quot;), sep=&#39;;&#39;,header = TRUE,stringsAsFactors = FALSE) Tenemos unas cuantas variables, usemos glimpse() para ver cu√°les son y su t√≠tulo: glimpse(avisosInmuebles) ## Rows: 62,009 ## Columns: 12 ## $ created_on [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;,... ## $ rooms [3m[38;5;246m&lt;int&gt;[39m[23m 3, 4, 1, 3, 4, 2, 5, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 1, ... ## $ bathrooms [3m[38;5;246m&lt;int&gt;[39m[23m 1, 2, 1, 1, 2, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, 1, 1, 1, 1, 1,... ## $ surface_total [3m[38;5;246m&lt;int&gt;[39m[23m 62, 200, 28, 55, 200, 54, 113, 441, 1296, 13, 12, 10, 12, 12, 13, 12, 29, 39, 180, ... ## $ surface_covered [3m[38;5;246m&lt;int&gt;[39m[23m 62, 100, 28, 55, 100, 44, 88, NA, NA, 13, 12, NA, 12, 12, 13, 12, 29, 39, 180, 305,... ## $ price [3m[38;5;246m&lt;int&gt;[39m[23m 170000, 237000, 83000, 85000, 237000, 75000, 690000, 1100000, 40000, 16000, 22000, ... ## $ currency [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;,... ## $ title [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;PH - Almagro&quot;, &quot;PH En Venta - Valez Sarsfield&quot;, &quot;Monoambiente Caballito. Excelente... ## $ description [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;&lt;br&gt;Lind\\355smo PH de 62 m2. Renovado. Sin Expensas. Apto Cr\\351dito.&lt;br&gt;&lt;br&gt;Lind\\... ## $ property_type [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Lote&quot;, &quot;Lote&quot;, &quot;Cochera&quot;, &quot;Cochera&quot;,... ## $ operation_type [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;V... ## $ BARRIO [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;ALMAGRO&quot;, &quot;VELEZ SARSFIELD&quot;, &quot;VILLA GRAL. MITRE&quot;, &quot;MATADEROS&quot;, &quot;VELEZ SARSFIELD&quot;, ... Los nombres de las variables parecen bastante descriptivos. Podemos ver, adem√°s, que nuestro data frame cuenta con informaci√≥n sobre diversos tipos de propiedades: nosotros queremos trabajar con inmuebles aptos para vivienda ya que son los √∫nicos para los que aplican varias de las variables del dataset: avisosInmuebles &lt;- avisosInmuebles %&gt;% filter(property_type %in% c(&quot;Casa&quot;,&quot;Departamento&quot;,&quot;PH&quot;)) Adem√°s, con glimpse() pudimos ver que algunas de nuestras variables tienen datos faltantes: rooms, bathrooms y surface_covered. Veamos cu√°ntos de cada uno sum(is.na(avisosInmuebles$rooms)) ## [1] 3827 sum(is.na(avisosInmuebles$bathrooms)) ## [1] 2088 sum(is.na(avisosInmuebles$surface_covered)) ## [1] 1409 La que parece tener m√°s datos faltantes es rooms, hagamos un poco de data wrangling para poder completar estos casos en base al t√≠tulo o descripci√≥n del inmueble: avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(ambientes=str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string= title)) %&gt;% mutate(ambientes=ifelse(is.na(ambientes), str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string=description), ambientes)) %&gt;% mutate(ambientes=as.numeric(str_extract(pattern=&#39;\\\\d&#39;,ambientes))) %&gt;% mutate(ambientes=ifelse(ambientes == 0,NA,ambientes)) ¬øQu√© es lo que hicimos? Varias cosas, pero vayamos por partes. En primer lugar, creamos una variable ambientes para la que usamos la funci√≥n str_extract(), ya sea en title o description usando el pattern ‚Äò(?i)\\d.amb‚Äô ¬øQu√© es lo que hace? (?i) dice que no le preste atenci√≥n si una parte del texto est√° en may√∫scula o min√∫scula (es decir, que haga una b√∫squeda que no sea case sensitive). Luego, \\d.amb devuelve el primer d√≠gito que encuentra a la izquierda de las palabras ‚Äúamb‚Äù ¬øPara qu√© hacemos esto? para que si un t√≠tulo dice ‚Äú3 Ambientes‚Äù, levante el ‚Äú3 Amb‚Äù, o si dice 2 AMB, que retenga todo. Luego de que creamos esta variable, nos quedamos solo con el n√∫mero aplicando str_extract(pattern=‚Äú\\d‚Äù,‚Ä¶). Finalmente, si lo que devolvi√≥ de ambientes fue igual 0, entonces que le ponga NA porque eso no es un n√∫mero v√°lido de ambientes. Si se fijan cu√°ntos datos faltantes tiene nuestra variable van a ver que son muchos (13.313, para ser exactos). Pero en el resto de los casos ¬øcu√°ntos coincide con la variable rooms, provista por Properati? table(avisosInmuebles$ambientes==avisosInmuebles$rooms) ## ## FALSE TRUE ## 2853 32890 No parece estar nada mal ! en 32890 de los 35743 casos donde coinciden arrojan la misma cantidad de ambientes. Vamos a completar la varaible rooms con estos datos: avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(rooms = ifelse(is.na(rooms), ambientes, rooms)) sum(is.na(avisosInmuebles$rooms)) ## [1] 1011 Pueden replicar la misma idea para superficies cubierta o para los ba√±os. Para lo que sigue de este cap√≠tulo podemos trabajar simplemente qued√°ndonos con los casos completos de nuestro data frame, pero antes vamos a eliminar tambien algunas variables que no usaremos para la predicci√≥n: avisosInmuebles &lt;- avisosInmuebles %&gt;% select(-created_on,-currency,-title,-description,-operation_type,-ambientes) %&gt;% filter(complete.cases(.)) Listo, ya estamos en condiciones de crear nuestro primer √°rbol de regresi√≥n, pero esta vez deberemos usar otra implementaci√≥n de los √°rboles de regresi√≥n que nos brinda el paquete rpart() Si prestaron atenci√≥n, el √∫ltimo c√≥digo de R usamos la funci√≥n complete.cases() dentro del verb filter(). Pero cuando lo hicimos, dentro de la primera funci√≥n usamos un punto ¬øQu√© representa ese punto en ese contexto? los datos hasta ese momento. Es decir, le estamos diciendo que aplique la funci√≥n complete.cases() a todas las filas y las columnas que quedaron luego de select y que las filtre. Esta forma de usar funciones nos ahorra tener que asignar nuevamente los datos y encadenar todo en un mismo conjunto de pipes. 7.5.2 Recursive PARTitioning (RPART) El paquete RPart nos brinda otra implementaci√≥n de los √°rboles de decisi√≥n, una que nos permite trabajar con una variable num√©rica como variable a la que queremos predecir. Como siempre, debemos instalar nuestros paquetes antes de usarlos. Una vez que lo tengan instalado, solo tienen que cargarlo. Para hacer gr√°ficos de rplot, van a tener que instalar otro paquete: rpart.plot(). require(rpart) require(rpart.plot) avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(USDm2=price/surface_total) arbolRegresion &lt;- rpart(formula = USDm2 ~ rooms + BARRIO + bathrooms + property_type, data = avisosInmuebles,control = rpart.control(cp = 0.01)) rpart.plot(arbolRegresion) En mi experiencia, la mejor forma de entender los √°rboles de RPart no son sus gr√°ficos, sino usar rpart.rules(). Pero antes de hacer eso, usemos el gr√°fico para ver el primero de los valores, el que est√° en el primer nodo: dice 2751. Ahora saquemos el promedio de los precios de los inmuebles round(mean(avisosInmuebles$USDm2),0) ## [1] 2751 ¬°Coincide! Lo que nos muestra este √°rbol es, para cada nodo, el promedio de los precios de los inmuebles y la cantidad de casos cubiertos desde ah√≠ en adelante. Sin embargo, las ‚Äúreglas‚Äù por las que va a clasificar se encuentran solo en los nodos ra√≠z de m√°s bajo nivel, las que dicen 1747 (13%), 2230 (21%), 2659 (29%), 3363 (36%) y 6137 (1%). Para ver mejor cu√°les son las reglas ejecutemos la funci√≥n rpart.rules() View(rpart.rules(arbolRegresion)) La variable que m√°s us√≥ fue barrios, y solo usa la variable property_type para algunos subconjuntos de barrios. Este √°rbol dir√° que el precio en d√≥lares por metro cuadrado para Puerto Madero es de USD 6.137, por ejemplo. Ahora bien ¬øEs el promedio observado? round(mean(avisosInmuebles$USDm2[avisosInmuebles$BARRIO==&quot;PUERTO MADERO&quot;]),0) ## [1] 6137 S√≠, coincide. Y eso es exactamente lo que hace un √°rbol de regresi√≥n: elige c√≥mo segmentar a las variables y a cada nodo le asigna como valor el promedio. Ahora bien, antes introdujimos la idea de entrop√≠a como gu√≠a para ir particionando nuestro espacio de varaibles, pero ¬øQu√© us√≥ ahora?. La respuesta es el RMSE (Root Mean Squared Error), es decir el promedio de la raiz cuadrada de los errores de predicci√≥n. Para cada nodo de nuestro √°rbol, √©l se va a preguntar: ¬øqu√© variables y qu√© valores de esas variables maximizan la ca√≠da en el RMSE? Y con ese principio en mente termina de cubrir todos los casos. Veamos cu√°l es la ca√≠da en el RMSE entre asignar para cada uno de los inmuebles el valor del promedio de los inmuebles y cu√°nto cambia con la primera apertura, en la que usa la variable BARRIOS prediccionInicial &lt;- mean(avisosInmuebles$USDm2) rmseInicial &lt;- sqrt(mean((prediccionInicial-avisosInmuebles$USDm2)^2)) rmseInicial ## [1] 1446.731 Ahora veamos qu√© pasa con este error al abrir por la primera variable. No se ve del todo claro, pero en el gr√°fico y en las reglas podemos entender que el √°rbol pregunta de que barrio es y genera tres bifucarciones: 1) PUERTO MADERO, 2) BELGRANO, COUGHLAN, COLEGIALES, NU√ëEZ, PALERMO, RECOLETA Y RETIRO, 3) Otros barrios. Veamos el RMSE de esta clasificacion prediccionBarrios &lt;- ifelse(avisosInmuebles$BARRIO == &quot;PUERTO MADERO&quot;, 6137, ifelse(avisosInmuebles$BARRIO %in% c(&quot;BELGRANO, COUGHLAN&quot;,&quot;COLEGIALES&quot;,&quot;NU√ëEZ&quot;,&quot;PALERMO&quot;,&quot;RECOLETA&quot;,&quot;RETIRO&quot;),3363, 2332)) rmseBarrios &lt;- sqrt(mean((prediccionBarrios-avisosInmuebles$USDm2)^2)) rmseBarrios ## [1] 1342.169 rmseBarrios / rmseInicial - 1 ## [1] -0.07227452 Esa apertura gener√≥ una ca√≠da de aproximadamente 8% en el RMSE de las predicciones y, dado el algoritmo de generaci√≥n del √°rbol y los p√°rametros elegidos, es la apertura que m√°s mejora este indicador. 7.6 Ejercicio En base a lo aprendido en este cap√≠tulo, entrenar un √°rbol de decisi√≥n con el dataset de Titanic, pero esta vez separando entre training (70% del dataset) y testing (30%) del dataset. Adem√°s, prueben dos par√°metros distintos (mincases 5 y mincases 100) y estimen la accuracy (o tasa de acierto) tanto en el dataset de training como testing ¬øCon cu√°l de los dos modelos se quedar√≠an para predecir qui√©n sobrevivi√≥ o no en el Titanic? ¬øPor qu√©? 7.7 Lecturas recomendadas Para profundizar y/o reforzar algunos de los puntos de este cap√≠tulo recomiendo la lectura del Cap√≠tulo 8 de Introduction to Statistical Learning de James, Witten, Hastie y Tibsharani Para un tratamiento de divulgaci√≥n, did√°ctico y estimulante recomiendo nuevamente la lectura del libro de Walter Sosa Escudero linkeado al final del cap√≠tulo 4. Para una aproximaci√≥n m√°s te√≥rica de Machine Learning recomiendo la lectura del libro de Thomas Mitchell: Machine Learning. "]
]
