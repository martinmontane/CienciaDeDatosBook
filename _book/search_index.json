[
["index.html", "Ciencia de datos para curiosos ¡Sólo curiosos de acá en adelante! ¿Qué necesitamos para arrancar?", " Ciencia de datos para curiosos Martin Montane 2020-04-14 ¡Sólo curiosos de acá en adelante! La ciencia de datos ha estado presente casi en cualquier contexto que se pueda pensar: en los medios masivos, en nuestra experiencia diaria cuando usamos Netflix o nos tomamos el subte y en la charla con colegas o incluso familiares y amigos. Este libro tiene como objetivo principal dar una idea sobre qué es la ciencia de datos, para qué sirve y cómo podemos usarla. Para esto, se necesita solo una cosa: curiosidad. Con estas ganas de conocer lo que hoy no conocemos, pero que nos llama la atención, el resto de las herramientas pueden ir aprendiéndose. ¿Qué necesitamos para arrancar? Este libro utiliza el lenguaje de programación R y la interfaz de desarrollo RStudio para comprender para qué sirve la Ciencia de Datos y para comenzar a explorar sus aplicaciones. Esto implica que necesitamos una computadora (en lo posible, que tenga 4gb de memoria RAM o superior) y descargar e instalar tanto R como RStudio en nuestras computadoras. Descargar instalar estos dos softwares es muy simple ya que son gratuitos. R de hecho es un lenguaje de programación open source, o de código abierto, lo que significa que cualquiera puede colaborar. Haciendo click aquí van a poder descargar la última versión de R para Windows, Mac o Linux. Una vez que lo hayan descargado solo tienen que instalarlo. Ahora descarguen RStudio, también van a poder elegir la versión que corresponde según su sistema operativo. RStudio va a identificar automáticamente la versión de R que ya tienen instalada, por lo que es importante que instalen RStudio luego de haber instalado R. Una vez que tienen todo esto instalado pueden pasar al primer capítulo de este libro "],
["introduccion-practica-a-la-ciencia-de-datos.html", "1 Introduccion practica a la Ciencia de Datos 1.1 Nuestra primera investigación: el precio de las propiedades en CABA 1.2 Conociendo RStudio 1.3 Importando datos a R 1.4 ¿Cómo R organiza los datos? 1.5 Inspeccionando nuestros datos 1.6 Retomando nuestro ejercicio: ¿Cuánto aumentaron las viviendas? 1.7 Conclusiones 1.8 Ejercicios 1.9 Extensión: cargando y guardando datos de otros formatos", " 1 Introduccion practica a la Ciencia de Datos Al terminar este capítulo ustedes van a poder: - Crear un proyecto en RStudio - Cargar datos a una sesión de R - Identificar y comprender las distintas estructuras con las que R maneja los datos 1.1 Nuestra primera investigación: el precio de las propiedades en CABA En este primer capítulo vamos a estudiar el mercado inmobiliario de la Ciudad de Buenos Aires. Más específicamente, analizaremos los movimientos de los precios en los años 2013-2017 a través de un muy interesante (y masivo) dataset que públicamente ofrece Properati1. Más allá de que analizar los datos de las propiedades es de por sí interesante y pedagógico para introducir varias de las herramientas que vamos a utilizar, el mercado inmobiliario fue procesando varios cambios en parámetros claves como el precio del dólar y la oferta de créditos hipotecarios. Por esta razón, los precios durante los últimos años de nuestra muestra han experimentado un alza, o eso dicen las fuentes del sector. Figure 1.1: Nota de TELAM sobre el precio de los inmuebles en CABA durante 2017 En este primer capítulo vamos a hacer uso de un dataset que tiene el precio promedio del metro cuadrado en USD para el período 2013-2017 desagregado a nivel de barrios. Este dataset se descargó del portal de datos de Properati y se manipuló para que sea apto para nuestro primer análisis. Vamos a intentar responder dos preguntas: ¿Aumentaron los precios de los inmuebles (medido como USD en m2) en el 2017? ¿Todos los barrios aumentaron en la misma proporción o existen heterogeneidades? Pero antes de adentrarnos en nuestra pregunta de investigación tenemos que detenernos en algunos puntos importantes. 1.2 Conociendo RStudio RStudio es el Entorno Ingregrado de Desarrollo (IDE) que vamos a usar a lo largo de este libro, pero ¿Qué hace exactamente un IDE? ¿Para que lo necesitamos? Un IDE es un software que nos ayuda programar de una manera más simple y eficiente. Entre algunas de sus múltiples funciones, nos sugiere qué deberíamos continuar escribiendo , nos marca dónde nos hemos confundido y administra nuestros archivos en proyectos. Pero no existe mejor manera de aprender qué es un IDE que usándolo. Abran RStudio. Por default, deberían encontrarse con algo similar a lo que aparece en la Figura 1. RStudio divide la pantalla en cuatro paneles. Ariba a la izquierda vemos el Panel de edición de archivos, donde vamos a modificar los archivos donde guardamos nuestro código, a la derecha vemos el Panel de estructuras de datos, donde aparecen las “estructuras de datos” que están creadas (más sobre esto abajo en el documento). Abajo a la izquierda tenemos el panel donde se visualiza la consola o terminal, que es donde vamos a ver ejecutado nuestro código y a su derecha un Panel multiuso, que será de utilidad para explorar archivos, ver la salida gráfica de nuestro código, encontrar ayuda sobre nuestros comandos, entre otras funciones. Figure 1.2: Impresión de pantalla de RStudio Vamos a ejecutar nuestro primer código en R. En el panel de abajo a la izquierda (consola o terminal), ejecuten el siguiente código: 5 * 10 # No deberíamos necesitar a R para esto... ## [1] 50 Debería devolver, también en el panel de la consola, el número 50. Ya ejecutamos nuestro primer código en R, pero antes de seguir ejecutemos lo mismo de una manera un poco distinta. Escriban exactamente el mismo código, pero ahora en el Panel de edición de archivos. Una vez que lo hayan escrito, seleccionen el código completo con el mouse y presionen Control + Enter. En la consola deberían obtener exactamente el mismo resultado. ¿Qué fue lo que pasó? Nuestro código está en un archivo (que todavía no guardamos). Al seleccionar el código y apretar Control + Enter RStudio pasa este código a la consola y se ejecuta, tal como antes. Para guardar el archivo que acabamos de editar, presionen Control + S, pongan el nombre que quieran al nuevo archivo, que tendrá la terminación .R, que es como vamos a identificar a los archivos que contienen código de R. 1.2.1 Proyectos en RStudio Hasta ahora creamos un archivo .R que tiene una sola línea de código, pero en los proyectos en los que solemos trabajar la cantidad de archivos con código y con datos se multiplica. Si no pensamos en la organización para un proyecto de ciencia de datos, el resultado puede ser un conjunto de archivos con nombres poco intuitivos en las ubicaciones más variadas dentro de nuestra computadora. Para ayudar a corregir al menos parcialmente este problema, RStudio ofrece la posibilidad de guardar todos los archivos vinculados a un proyecto en una misma dirección de nuestra computadora. De esta manera, evitamos tener que recordar dónde guardamos los archivos y podemos movernos rápidamente de proyecto en proyecto. Crear un nuevo proyecto en RStudio es realmente simple. Solo tienen que ir a File -&gt; New Project... -&gt; New Directory -&gt; New Project. Allí deben elegir un nombre para el proyecto y hacer click en Create Project ¡Listo! Ya creamos nuestro primer proyecto. Nuestra pantalla ahora debería estar dividida en cuatro paneles. ¿Recuerdan la carpeta donde crearon el proyecto? Una vez que lo hayan hecho, ejecuten el siguiente código desde el panel de abajo a la izquierda: getwd() ¿Qué sucedió? Les devolvió la dirección de la carpeta donde está este proyecto. De ahora en más, todo lo que guarden o a lo que intenten acceder desde el proyecto de RStudio se hará dentro de este directorio. Esto trae muchos beneficios que iremos viendo de aquí en adelante. 1.3 Importando datos a R Cargar los datos a la sesión de R - es decir cuando abrimos RStudio - es una de las tareas más importantes ¡no hay ciencia de datos sin datos! En esta sección vamos a ver distintas formas de cargar los datos y veremos dónde aparecen visualmente en RStudio. 1.3.1 Comma Separated Values Uno de los formatos más conocidos para guardar datos son los archivos separados por comas (CSV). El formato consiste simplemente en elementos separados por algún delimitador, en general una coma, que terminan formando una matriz. Veamos un ejemplo con el precio de los inmuebles por año y barrio: precioAvisos &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/precioBarrios.csv&#39;, sep=&#39;;&#39;, stringsAsFactors = FALSE) En este código hicimos muchas cosas ¿no? Pero antes de empezar a explicar lo que acaban de ejecutar, fijense qué pasó en el panel de arriba a la derecha: deberían tener una fila que dice “precioAvisos” con 48 observaciones 6 variables. Si descargan el csv que acabamos de cargar directamente a R y lo abren, por ejemplo, con excel van a ver también que tiene 48 observaciones y 6 variables ¡Hicimos nuestra primera carga de datos a R! ¿Cómo lo hicimos? Con la ayuda de la función read.csv. ¿Qué es una función? Una función no es otra cosa que un conjunto de código que no vemos, pero que podemos usar de la siguiente manera: le damos algo y nos devuelve otra cosa. Una función en los lenguajes de programación es una “caja” que procesa algo que le damos (input) y nos devuelve algo (output). ¿Qué le damos a una función? Le pasamos valores. En este caso, file, sep y stringsAsFactors. ¿Qué nos devuelve? Lo que hay en ese archivo (file) separado por el caracter que nosotros le pasamos (sep). Por el momento, dejaremos stringsAsFactors de un lado. Pero ¿Cómo aparecieron esos datos en un objeto con un nombre precioAvisos? Eso lo hicimos con la ayuda de &lt;-, nuestra forma de asignar resultados de funciones a objetos. El operador de asignación &lt;- “pasa” o asigna lo que sea que está a su derecha a un objeto que está a su izquierda. En nuestro ejemplo. a la izquierda esta “precioAvisos”, que es como se llamará nuestro objeto, y a la derecha está nuestra primera función “read.csv()” 1.4 ¿Cómo R organiza los datos? Hagan click en “precioAvisos”, en el panel de arriba a la derecha ¿Qué ven? Este es tan solo uno de los objetos que tiene R para ir guardando datos. La función class() nos ayuda para saber qué usó R para guardar estos datos: class(precioAvisos) ## [1] &quot;data.frame&quot; En este caso, vemos que es un data.frame, un caso específico - y muy importante - de las estructuras de datos u objetos de R. Se trata de un caso un poco complejo, vayamos un poco para atrás y veamos las estructuras de datos más simples que maneja R. Luego veremos que se relacionan con los data.frame. 1.4.1 Vectores R organiza a los datos en diferentes estructuras según el dominio de los datos. El dominio no es otra cosa que los valores que una variable puede llegar a tomar. Por ejemplo, si una variable solo puede tomar números reales y enteros, R pondrá esa variable en un vector númerico o integer, para ser más preciso. Un vector es la estructura más básica con la que vamos a lidiar. Un vector no es otra cosa que una colección numerada de valores, es decir una estructura que puede contener uno o más valores, y puede accederse a través de índices que denotan el orden de cada número dentro del vector ¿Simple, no? Creemos nuestro primer vector primerVector &lt;- c(20,40,60,80,100) Ya está: el objeto primerVector es un vector númerico que tiene cinco elementos: 20, 40, 60, 80 y 100. Como les prometí, el vector es una colección numerada de valores, por lo que podemos acceder a cualquiera de los valores llamando al objeto por su nombre y escribiendo la posición entre corchetes [] (la numeración arranca desde 1 en R) primerVector[3] # Devuelve el valor del elemento en la tercera posición de nuestro vector ## [1] 60 La mayor parte de nuestros datos no consiste solo en números, sino en datos mixtos: números, texto, variables categóricas, variables lógicas. Todos estos tipos de datos pueden ser representados en nuestros versátiles vectores: vectorTexto &lt;- c(&quot;Croacia&quot;,&quot;Argentina&quot;,&quot;Nigeria&quot;,&quot;Islandia&quot;) vectorLogico &lt;- c(FALSE, TRUE, TRUE, FALSE) Los vectores de texto no requieren demasiada discusión: son vectores cuyos elementos son texto. Por su parte, los vectores lógicos quizás sí requieran algo de explicación. Los elementos de estos vectores solo pueden tomar los valores TRUE o FALSE, y resultan de mucha utilidad para hacer preguntas del estilo ¿Es este vector un vector de texto? is.character(vectorTexto) ## [1] TRUE Volvamos a nuestros datos sobre los precios de los inmuebles. Ya comentamos que había seis variables, pero no dijimos que a su vez son ¡6 vectores!. Uno es CHARACTER, como nuestro vectorTexto anterior, cinco son INTEGER, o numeric, como nuestro primer_vector y el último es sfc_MULTIPOLYGON (vamos a ver un poco más en detalle este último tipo de vector en próximas clases) ¿Cómo podemos acceder a ellos? La llave es el operador $ precios2017 &lt;- precioAvisos$USDm2_2017 str(precios2017) # ¿Los 100 barrios porteños en realidad son 48? ## num [1:48] 1946 2299 1989 1920 3199 ... ¿Qué fue lo que hicimos? Creamos el objeto precios2017 al que le asignamos (&lt;-) el vector/variable USDm2_2017 que está en el Data Frame precioAvisos (vamos a ver más sobre los Data Frame más adelante). Al usar la función str() (son las primeras tres letras de la palabra structure en inglés) vemos que se trata de un vector integer (o númerico) con 48 elementos ¡La misma cantidad de observaciones que tenemos en precioAvisos! Ahora es donde las cosas se ponen un poco más interesantes. Podemos hacer múltiples operaciones aritméticas sobre este vector de una manera muy simple: str(precios2017*2) # Multiplicación ## num [1:48] 3891 4598 3978 3839 6398 ... str(precios2017/4) # División ## num [1:48] 486 575 497 480 800 ... str(precios2017 - 20) # Resta por solo un número ## num [1:48] 1926 2279 1969 1900 3179 ... str(precios2017 - precios2017) # Resta con otro vector de igual tamaño ## num [1:48] 0 0 0 0 0 0 0 0 0 0 ... Una de las ventajas de R es su vectorización. En las operaciones anteriores vimos como múltiplicar, dividir, restar o cualquier otra operación aritmética se aplica de manera individual sobre cada uno de los elementos de los vectores, con lo cual la operación se hace elemento por elemento. Hagamos algo que sea útil para describir la distribución de los precios de los inmuebles ¿Cuál es el valor mínimo del metro cuadrado? ¿Cuál es el máximo? ¿Cuál es el precio promedio? Las funciones min(), max() y mean() hacen el trabajo por nosotros. resumen_2017 &lt;- c(min(precios2017), max(precios2017), mean(precios2017)) resumen_2017 ## [1] 872.7541 6070.3288 2203.1119 1.4.2 Listas y Data Frames Otras estructuras de datos importantes en R son las listas y Data Frames: Las listas son objetos que contienen a su vez un conjunto ordenados de objetos. Los Data Frames son un caso específico de listas. 1.4.2.1 Listas Las listas tienen la posibilidad de almacenar objetos con distinta clase. Es decir, es posible crear una lista en la cual se almacenan otras estructuras de datos con clases distintas: lista1 &lt;- list(Nombres = c(&quot;Fernando&quot;,&quot;Martín&quot;), Apellido=&quot;Montané&quot;, tienehijos = FALSE, edad = 26) lista1 ## $Nombres ## [1] &quot;Fernando&quot; &quot;Martín&quot; ## ## $Apellido ## [1] &quot;Montané&quot; ## ## $tienehijos ## [1] FALSE ## ## $edad ## [1] 26 En este caso creamos una lista que se llama lista1 y almacena 4 vectores. El primero, es un vector Nombres que contiene dos elementos de clase character. El segundo, un vector de un solo elemento, Apellido de clase character. El tercero, un vector lógico de un elemento ( tienehijos ). Finalmente, el vector edad, que posee un elemento númerico. En las listas hay que acceder a los objetos, que puede ser cualquiera de las estructuras que vimos hasta ahora (incluyendo las listas). Para acceder a ellos existen al menos 3 formas: Usando el signo $ lista$Nombres Usando doble corchetes e indicando la posición del objeto buscado lista[[1]] Usando doble corchetes e indicando el nombre del objeto buscado lista[['Nombres']] lista1$Nombres ## [1] &quot;Fernando&quot; &quot;Martín&quot; lista1[[1]] ## [1] &quot;Fernando&quot; &quot;Martín&quot; lista1[[&#39;Nombres&#39;]] ## [1] &quot;Fernando&quot; &quot;Martín&quot; 1.4.2.2 Data Frames ¡Ahora estamos en condiciones de explicar qué es un Data Frame! Como se adelantó, un Data Frame es un caso específico de listas. En la gran mayoría de las aplicaciones se puede describir como una matriz en la cual variables (columnas) pueden ser de distintas clases. df &lt;- data.frame(Nombres = c(&#39;Juan&#39;,&#39;Pedro&#39;,&#39;Ana&#39;,&#39;Delfina&#39;), Edad = c(21,46,58,27), EstadoCivil = c(&#39;Soltero&#39;,&#39;Casado&#39;,&#39;Casado&#39;,&#39;Soltero&#39;), SecundarioCompleto = c(TRUE, FALSE, TRUE, TRUE)) df ## Nombres Edad EstadoCivil SecundarioCompleto ## 1 Juan 21 Soltero TRUE ## 2 Pedro 46 Casado FALSE ## 3 Ana 58 Casado TRUE ## 4 Delfina 27 Soltero TRUE str(df) # Resumen del objeto ## &#39;data.frame&#39;: 4 obs. of 4 variables: ## $ Nombres : Factor w/ 4 levels &quot;Ana&quot;,&quot;Delfina&quot;,..: 3 4 1 2 ## $ Edad : num 21 46 58 27 ## $ EstadoCivil : Factor w/ 2 levels &quot;Casado&quot;,&quot;Soltero&quot;: 2 1 1 2 ## $ SecundarioCompleto: logi TRUE FALSE TRUE TRUE El comando str nos devuelve un resumen de las variables del data frame, incluyendo sus clases. Si prestan atención, el vector numérico Edad y el lógico SecundarioCompleto tienen la clase esperada, pero los vectores de caracteres Edad y SecundarioCompleto dice Factor w/2 o w/4 levels ¿Qué es esto? La clase Factor es un caso específico de vectores. En esta clase de vectores, las variables solo pueden tomar un conjunto de valores predeterminados, es decir que tienen una categoría. Estas categorías se llaman levels en R. Internamente R transforma estas variables y les asigna un número entero, que son los valores que nos devolvió el comando str. Sin embargo, estos números hacen referencia a una categoría que tiene un nombre. En nuestro caso ¿Tiene sentido que los dos vectores con texto sean Factores?. Respuesta: no. El estado civil de una persona sí puede segmentarse en categorías que se repiten, pero no es el caso de los nombres. Para evitar que R transforme nuestros vectores character en Factor lo que hay que hacer cuando definimos un Data Frame es decírselo a R con el parámetro stringsAsFactors: df &lt;- data.frame(Nombres = c(&#39;Juan&#39;,&#39;Pedro&#39;,&#39;Ana&#39;,&#39;Delfina&#39;), Edad = c(21,46,58,27), EstadoCivil = c(&#39;Soltero&#39;,&#39;Casado&#39;,&#39;Casado&#39;,&#39;Soltero&#39;), SecundarioCompleto = c(TRUE, FALSE, TRUE, TRUE), stringsAsFactors = FALSE) df ## Nombres Edad EstadoCivil SecundarioCompleto ## 1 Juan 21 Soltero TRUE ## 2 Pedro 46 Casado FALSE ## 3 Ana 58 Casado TRUE ## 4 Delfina 27 Soltero TRUE str(df) # Resumen del objeto. Ahora ya no tenemos Factors. ## &#39;data.frame&#39;: 4 obs. of 4 variables: ## $ Nombres : chr &quot;Juan&quot; &quot;Pedro&quot; &quot;Ana&quot; &quot;Delfina&quot; ## $ Edad : num 21 46 58 27 ## $ EstadoCivil : chr &quot;Soltero&quot; &quot;Casado&quot; &quot;Casado&quot; &quot;Soltero&quot; ## $ SecundarioCompleto: logi TRUE FALSE TRUE TRUE Los vectores Factor son de mucha utilidad, especialmente cuando se trabaja con modelos estadísticos en R. Vamos a verlos en otras aplicaciones más adelante. Ahora ya tenemos los elementos suficientes como para definir más concretamente qué es un Data Frame: es un conjunto de listas, que en su versión más usual contienen un vector cada una de un mismo largo (mismas cantidad de observaciones), pero que pueden almacenar variables de distinto dominio. Esto es algo muy útil para la mayor parte de datasets con los que se trabaja. 1.5 Inspeccionando nuestros datos ¿Cómo sabemos qué datos tenemos cargados? Tenemos al menos dos funciones que sirven para inspeccionar rápidamente los datos. Por un lado, podemos usar la función View() (Notar la V mayúscula al principio de la función). Lo que hay que hacer es pasarle uno de nuestros objetos que están cargados, y nos devuelve la tabla entera en el panel de edición. View(precioAvisos) # Debería abrirse una tabla dentro del panel de edición Deberían ver una matriz con 6 columnas y unas 48 filas. Las filas representan observaciones (en este caso, barrios), mientras que las columnas hacen referencia a las variables, que tienen los siguientes nombres: BARRIOS, USDm2_2013, USDm2_2014, USDm2_2015, USDm2_2016, USDm2_2017 y geometry ¿Qué significan? BARRIOS: es una variable que indica cuál es el nombre del barrio al que pertenece cada observación USDm2_201x: estás 5 columnas hacen referencia al valor promedio de los inmuebles creados en cada año para cada barrio (período 2013-2017) Otra función que puede ser muy útil para describir nuestros objetos, que ya hemos usado anteriormente, es str(). Nos devuelve las primeras observaciones de cada una de las variables, también la clase de nuestro objeto, la cantidad de observaciones (o filas), que en este caso son 48, y la cantidad de variables (o columnas), que son 6. También nos indica la clase de cada una de las variables, que en este caso son int o num. Para entender bien las clases de datos y las diversas estructuras que los contienen tenemos que ir un casillero más atrás y explicar cómo es R organiza las cosas. str(precioAvisos) ## &#39;data.frame&#39;: 48 obs. of 6 variables: ## $ BARRIOS : chr &quot;AGRONOMIA&quot; &quot;ALMAGRO&quot; &quot;BALVANERA&quot; &quot;BARRACAS&quot; ... ## $ USDm2_2013: num 1749 2034 1893 2229 2620 ... ## $ USDm2_2014: num 1458 1930 1818 1901 2544 ... ## $ USDm2_2015: num 1689 2086 1858 2390 2695 ... ## $ USDm2_2016: num 2057 1950 1834 1751 3287 ... ## $ USDm2_2017: num 1946 2299 1989 1920 3199 ... 1.6 Retomando nuestro ejercicio: ¿Cuánto aumentaron las viviendas? Ya estamos en condiciones de hacer un par de cálculos más y ver si efectivamente, como en la noticia principal, las propiedades aumentaron de precio en 2017. Para esto, no tenemos que hacer nada nuevo, usando el operador $, que nos deja elegir el vector de nuestro data.frame, solo tenemos que hacer una división. mean(precioAvisos$USDm2_2017)/mean(precioAvisos$USDm2_2016) - 1 ## [1] 0.05299051 Según nuestros datos, aumentaron 5,3% en 2017. Coincidimos con la nota, pero nos da un poco menos que lo esperado. Veamos ahora cuál fue la variación de los barrios: recuerden que R puede vectorizar la operación. Vamos a agregarle una nueva columna a nuestro data frame, combinando lo que aprendimos hoy precioAvisos$variacion2017 &lt;- precioAvisos$USDm2_2017/precioAvisos$USDm2_2016 - 1 View(precioAvisos) Ordenen desde el visor a las variaciones por barrio ¿Qué pasó? Si miran con atención, el precio de los inmuebles en Villa Soldati, para nuestra muestra, cayó un 63%. Claro que esto tiene un impacto sobre el cálculo de la variación en 2017. No se preocupen, en el próximo capítulo, con la ayuda de tidyverse vamos a poder ver qué pasa con esto. ¿Qué pueden decir sobre la heterogeneidad en los barrios? ¿Cuáles crecieron más y cuáles menos? 1.7 Conclusiones Los datos públicos a los que accedimos nos permitieron responder las dos preguntas iniciales: En primer lugar, se observó un incremento en los precios de los inmuebles en USD durante el 2017 en comparación al 2016. Este aumento fue de aproximadamente 5,3%, según un promedio simple de las variaciones barriales. En segundo lugar, las variaciones en los precios exhiben una importante heterogeneidad regional, con algunos barrios ubicados en la parte noroeste (Villa Pueyrredón, Villa Ortúzar, Chacarita, Colegiales) y sudeste (Nueva Pompeya, Parque Chacabuco, Boedo) de la capital aumentando más que el resto. 1.8 Ejercicios ¿Cuál fue la variación de los precios de los inmuebles entre 2016 y 2015? ¿Cuál fue la variación de los precios de los inmuebles entre 2017 y 2013? ¿Cuál fue el barrio que más creció entre 2017 y 2013? ¿Cuál fue el barrio que menos creció entre 2017 y 2013? ¿Cual es el año en el que los precios de la ciudad fueron más altos? ¿Cuál fue ese valor? 1.9 Extensión: cargando y guardando datos de otros formatos 1.9.1 Microsoft Excel Para bien o para mal, las planillas de Excel están por todos lados. Es importante saber cómo cargarlas a R y poder trabjar con ellas desde aquí. Para esto, vamos a necesitar la ayuda de un paquete de datos de R (en el Capítulo 2 pueden aprender qué es un paquete de datos) y descargar un archivo excel. En el ejemplo suponemos que este archivo está disponible en la carpeta principal del proyecto y que ya tienen instalado el paquete readxl. library(readxl) datos &lt;- read_excel(path = &#39;child_mortality_0_5_year_olds_dying_per_1000_born.xlsx&#39;, sheet = 1) Si inspeccionan el archivo datos con str(), glimpse() o View() van a poder ver qué tipo de datos hay en lo que leímos desde R. Solo necesitamos pasarle dos argumentos: la dirección (path) y el número o nombre de hoja (en este caso, la primera). Esta función tiene muchos otros parámetros para adaptarnos al formato que tenga la hoja de excel. Pueden averiguar más preguntando de esta manera: ?read_excel() ¿Cómo guardamos los datos a excel? Existen muchas alternativas para hacerlo, entre ellas la función write_xlsx que brinda el paquete writexl ¿Por qué? Porque otras alternativas tienen diveresas dependencias, como rJava, que requiere tener instalado Java versión 64 bits en la computadora y muchas veces no es fácil darse cuenta de esto. La instalación de este paquete solo debe hacerse una vez, como se explicará en los capítulos que siguen. install.packages(&quot;writexl&quot;) Una vez que lo instalaron, ya podemeos usar la función para escribir archivos xlsx luego de usar library(writexl). library(writexl) write_xlsx(x = datos ,path = &#39;Mortalidad Infaitl.xlsx&#39;) Este video resume en unos pocos minutos gran parte de lo que vamos a ver acá y es recomendable↩ "],
["transformando-nuestros-datos-data-wrangling.html", "2 Transformando nuestros datos (data wrangling) 2.1 Instalando nuestro primer paquete en R: tidyverse 2.2 El dataset gapminder 2.3 Transformaciones de los datos 2.4 Transformando la presentación de los datos: pivot_wider y pivot_longer 2.5 Uniendo datos de distintas fuentes: left_join 2.6 La mise en place: preparando el dataset de inmuebles 2.7 Ejercicios 2.8 Extensiones", " 2 Transformando nuestros datos (data wrangling) Al terminar este capítulo ustedes van a poder: - Entender qué es un paquete de R, cómo se instala y cómo se carga - Comprender a qué se llama Ciencia de Datos - Realizar lastransformaciones mas comunes de datos - Realizar algunos gráficos elementales En el primer capítulo de este libro realizamos algunas de las etapas de un típico proyecto de Ciencia de Datos: leímos los datos y realizamos un breve análisis descriptivo. Sin embargo, en la práctica esto no suele suceder de esta manera e inmediatamente después de leer los datos tenemos que realizar una serie de transformaciones que, como regla general, ocupa la mayor parte del tiempo. Este segundo capítulo tiene el objetivo de cubrir las herramientas que tidyverse nos ofrece para realizar estas transformaciones, conocidas de manera coloquial como Data Wrangling. Vamos a intentar cumplir este objetivo en dos partes. En primer lugar, vamos a pasar lista de las funciones que los van a acompañar de ahora en adelante para hacer las transformaciones de datos necesarias. En segundo lugar, vamos a usarlas para llegar al mismo data frame de precio de los inmuebles que se usó en el primer capítulode los datos tal como son descargados desde Properati 2.1 Instalando nuestro primer paquete en R: tidyverse De ahora en adelante vamos a ir usando funciones que no vienen instaladas con la instalación de R base que ya hicieron. Este conjunto de funciones, que hacen a R realmente poderoso a partir de la colaboración de miles de personas, se guardan en paquetes (packages, en inglés) que podemos instalar de una manera muy simple: con la función install.packages() install.packages(&#39;tidyverse&#39;) ¡Perfecto! una vez que lo hayan instalado no lo tienen que hacer más en esa computadora. Ya van a poder usar las funciones que tiene, que nos serán muy importantes. Para usarlas, solo tenemos que llamar cada vez que abramos RStudio - en rigor, cada vez que tengamos una nueva sesión de R - a la función library() o require(), ambas hacen lo mismo. Pero ¿Qué es tidyverse? Es un conjunto de packages. Según RStudio, Tidyverse es “[…] a coherent system of packages for data manipulation, exploration and visualization that share a common design philosophy”. En la práctica, tidyverse nos va a permitir articular e implementar diversas facetas del proceso de análisis de datos de una manera unificada. Tiene una curva de aprendizaje, por lo cual no se preocupen si en esta primera sección hay algunas cosas que no terminan de entenderse. Ecosistema de paquetes que componen tidyverse y su vinculación con cada proceso de un proyecto de Ciencia de Datos 2.2 El dataset gapminder En 2007 Hans Rosling (1948-2017), un médico sueco, dio una de las charlas Ted más famosas2. Su presentación mostraba la evolución de tres variables en el tiempo: PIB per cápita, expectativa de vida al nacer y población. Uno de los principales mensajes de su charla es que, aunque no lo notemos, el mundo ha mejorado considerablemente - y continúa haciéndolo. Vamos a trabajar con estos optimistas datos en este capítulo. Podemos acceder a los datos con los que trabajó Rosling con el siguiente código: gapminder_df &lt;- read.table(file = &quot;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/gapminder.csv&quot;, sep=&#39;;&#39;, header = TRUE, stringsAsFactors = FALSE) Este data frame cuenta con 6 columnas, cuyo nombre podemos obtenerlo de la siguiente manera: colnames(gapminder_df) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; E identifican los siguientes datos: country: Nombre de país continent: Nombre del continente year: año de la observación lifeExp: expectativa de vida al nacer (en años) pop: cantidad de habitantes gdpPercap: Producto Interno Bruto (PIB) por habitante 2.3 Transformaciones de los datos Recordemos que precisamos de las funciones de tidyverse para transformar nuestros datos. Para esto, solo tenemos que aplicar el comando \\(`require()`\\) o \\(`library()`\\), los dos cumplen con nuestro objetivo: require(tidyverse) # Pueden usar library(tidyverse), el resultado debería ser el mismo. 2.3.1 Selección de columnas: select() El comando select() nos permite elegir columnas de nuestros data frames. Solo debemos pasarle los nombres de las variables que deseamos retener. Conservar solo algunas de las variables de un Data Frame es una operación que se realiza muy frecuentemente, así que practiquemos con dos ejemplos. Es importante remarcar el papel que cumple el pipe (%&gt;%): todo lo que está antes es pasado a lo que le sigue para ser procesado. gapminder_df %&gt;% select(country) # Seleccionamos solo la variable de país Revisen el panel de “Enviroment” ¿Ven algún objeto nuevo creado? ¿gapminder_df perdió alguna columna (pueden usar str() o colnames() para probarlo?) No, y esto es muy simple de explicar: no asignamos nada a un nuevo objeto. Simplemente le dijimos a R que queríamos solo esa columna, y fue lo que nos devolvió. Para asignar, nunca se olviden de usar &lt;- luego del nombre del objeto. Vayamos un poco más allá ¿En qué formato devolvió la columna que queríamos seleccionar? Para esto, ahora sí vamos a asignar lo que sea que devuelva select() a una nueva variable. Luego vamos a usar la función class() para ver qué es lo que devolvió. gapminderCol &lt;- gapminder_df %&gt;% select(country) class(gapminderCol) ## [1] &quot;data.frame&quot; Un data.frame… pero si es solo una columna? no debería ser un vector? La función select() siempre devuelve un data.frame, aun si se selecciona una sola columna. Esto podría no parecer un problema para sus análisis, pero muchas funciones que usarán más adelante necesitarán que estemos pasando un vector, no un vector dentro de una lista (recuerden del capítulo 1 que un data.frame era un conjunto de n listas que tenían un vector de igual tamaño en cada una de ellas) Si queremos seleccionar una columna, pero que quede guardado en un vector tenemos muchas opciones. Una ya la conocen: es simplemente usar el operador $, que devuelve un vector. Otra alternativa es usar la función unlist() luego de select() o pull(). Verifiquen ustedes mismos usando class() y las herramientas que ya conocen para ver si efectivamente son iguales gapminderVec1 &lt;- gapminder_df %&gt;% select(country) %&gt;% unlist() gapminderVec2 &lt;- gapminder_df %&gt;% pull(country) gapminderVec3 &lt;- gapminder_df$country Pero no siempre queremos seleccionar una sola columna, sea que nos devuelva un data.frame o un vector ¿Cómo podemos seleccionar dos columnas? Muy simple: escribimos otra columna separado por una coma. gapminder_subset &lt;- gapminder_df %&gt;% select(country, continent) ¿Cuántos países únicos hay en el dataset? ¿Cuantos continentes? En este contexto nos va a sere muy útil la función unique(). Esta función toma un objeto como argumento y devuleve un nuevo objeto que contiene los casos únicos. Usémosla para saber la cantidad de países y continentes: paises &lt;- unique(gapminder_subset$country) length(paises) # Length() nos devuelve la cantidad de elementos que tiene un vector ## [1] 142 length(unique(gapminder_subset$continent)) # Podemos combinar las funciones ## [1] 5 Existen 142 países y 5 continentes en nuestro dataset. 2.3.2 Selección de casos: filter() Cuando queramos analizar nuestros datos según ciertas características que tengan nuestras observaciones, es preciso poder seleccionar los casos según los valores que toman en una o más variables. tidyverse (en rigor, uno de sus paquetes: dplyr) nos ofrece el método filter() para realizar esta clase de transformaciones. Por ejemplo: ¿Cuáles son las observaciones que corresponden al año 2002? gapminder_df %&gt;% filter(year == 2002) # Año 2002 Como podemos ver, este comando nos devuelve un tibble (o data frame) con las observaciones correspondientes al año 2002. Recordamos: Las funciones que aplicamos a un objeto (en este caso gapminder) tienen que estar mediadas por lo que se conoce como pipe (%&gt;%). Detengámonos para analizar qué fue lo que hicimos en mayor detalle. La función filter toma operadores lógicos como argumentos. Estos operadores devuelven TRUE o FALSE dependiendo si una comparación se verifica o no. En nuestro caso particular, el operador lógico utilizado fue ==, que singifica exactamente igual a. Puede parecer raro que usemos doble igual en lugar de un solo igual, pero recuerden que en R el = se encuentra reservado para la asignación, al igual que &lt;-. Utilizando el mismo operador lógico podemos hacer todavía más cosas. Por ejemplo, podemos filtrar con respecto a otras variables o combinar condiciones de filtrado mediante el mismo método # Combinando filtros gapminder_df %&gt;% filter(country == &quot;Argentina&quot;, year == 2007) Como podemos ver en el último de los ejemplos, se pueden combinar más de un operador lógico separado por comas. Cada condición se concatena a la anterior como un AND lógico ¿Qué signfica esto? Que la función filter() toma cada uno de los operadores lógicos y busca que todos se cumplan de manera SIMULTÁNEA. En nuestro último caso, filtramos los datos que correspondían tanto a Argentina como al año 2007, lo que devolvió un objeto de una sola fila: los valores correspondientes para Argentina en el año 2007. La Figura 1 muestra todas las combinaciones booleanas (o lógicas) posibles dados dos conjuntos. La función filter() aplica el operador lógico &amp; a cada una de las condiciones que imponemos. No se preocupen: podemos generar el resto de las condiciones booleanas usando operadores booleanos tales como |, que representa OR, o ! que representa NOT, es decir que lo podemos usar para negar una condición. Si todo esto suena complejo es totalmente razonable. De cualquiera manera, algunos ejemplos (y mucha práctica) va a hacer que todo sea muy intuitivo. Veamos algunos ejemplos Representación gráfica de todas las operaciones lógicas posibles. Fuente: R for Data Science # Datos de argentina pero que NO incluyan al año 2007 gapminder_df %&gt;% filter(country == &quot;Argentina&quot;, !year == 2007) # ! representa la negación ## country continent year lifeExp pop gdpPercap ## 1 Argentina Americas 1952 62.485 17876956 5911.315 ## 2 Argentina Americas 1957 64.399 19610538 6856.856 ## 3 Argentina Americas 1962 65.142 21283783 7133.166 ## 4 Argentina Americas 1967 65.634 22934225 8052.953 ## 5 Argentina Americas 1972 67.065 24779799 9443.039 ## 6 Argentina Americas 1977 68.481 26983828 10079.027 ## 7 Argentina Americas 1982 69.942 29341374 8997.897 ## 8 Argentina Americas 1987 70.774 31620918 9139.671 ## 9 Argentina Americas 1992 71.868 33958947 9308.419 ## 10 Argentina Americas 1997 73.275 36203463 10967.282 ## 11 Argentina Americas 2002 74.340 38331121 8797.641 # Uno de los inconvenientes cuando queremos filtrar por más de un criterio de una misma variable es que tenemos que hacer lo siguiente gapminder_df %&gt;% filter(country == &quot;Argentina&quot;, year == 2002 | year == 2007) ## country continent year lifeExp pop gdpPercap ## 1 Argentina Americas 2002 74.34 38331121 8797.641 ## 2 Argentina Americas 2007 75.32 40301927 12779.380 Bastante más simple que en la explicación anterior ¿No? Vamos a complejizarlo levemente introduciendo al operador %in%. Este operador %in% es muy útil para matchear múltiples condiciones de manera simple. Lo que hace es devolver TRUE en todos los elementos de un vector que cumplen con alguno de los valores contenidos en el vector de la derecha. En el siguiente caso devuelve las observaciones que corresponden a los años 2002 o 2007. # Podemos solucionarlo mediante el siguiente método gapminder_df %&gt;% filter(year %in% c(2002,2007)) # Es una forma de concatenar condiciones de tipo | (OR) # gapminder_df %&gt;% filter(year == 2002 | year == 2007) # Da el mismo resultado Hagamos algo útil con lo que aprendimos hasta ahora, aun haciendo referencia a una librería que todavía no usamos como ggplot. Este paquete es uno de los más utilizados para realizar gráficos. gapminder_argentina &lt;- gapminder_df %&gt;% filter(country == &quot;Argentina&quot;) # Expectativa de vida al nacer en Argentina 1952-2007 ggplot(gapminder_argentina) + geom_line(aes(x = year, y = lifeExp)) # PIB per cápita en Argentina 1952-2007 ggplot(gapminder_argentina) + geom_line(aes(x = year, y = gdpPercap)) Creo que podemos sacar nuestras primeras conclusiones para Argentina: la expectativa de vida parece haber crecido bastante estable durante todo el período bajo análisis, pero la evolución del PIB per cápita bien puede parecerse a lo que registra un sismógrafo ! 2.3.3 Ordenando: la función arrange() La función arrange nos permite ordenar un dataset en orden ascendente o descendente en base a los valores de las variables. Está parte del proceso suele ser relevante para algunas transformaciones de datos y para la inspección visual de valores extremos. # En sentido ascendente (del valor más bajo al más alto) gapminder_df %&gt;% arrange(lifeExp) En sentido descendente (del valor más alto al más gapminder_df %&gt;% arrange(desc(lifeExp)) Veamos una de las principales funcionalidades de tidyverse al combinar algunos de los comandos que aprendimos hasta ahora. Vamos a filtrar el dataset para el continente de América y el año 2007 y luego (recordar: está a la derecha del último pipe) ordenar las observaciones según la expectativa de vida al nacer, de mayor a menor: gapminder_df %&gt;% filter(continent == &quot;Americas&quot;, year == 2007) %&gt;% arrange(desc(lifeExp)) ## country continent year lifeExp pop gdpPercap ## 1 Canada Americas 2007 80.653 33390141 36319.235 ## 2 Costa Rica Americas 2007 78.782 4133884 9645.061 ## 3 Puerto Rico Americas 2007 78.746 3942491 19328.709 ## 4 Chile Americas 2007 78.553 16284741 13171.639 ## 5 Cuba Americas 2007 78.273 11416987 8948.103 ## 6 United States Americas 2007 78.242 301139947 42951.653 ## 7 Uruguay Americas 2007 76.384 3447496 10611.463 ## 8 Mexico Americas 2007 76.195 108700891 11977.575 ## 9 Panama Americas 2007 75.537 3242173 9809.186 ## 10 Argentina Americas 2007 75.320 40301927 12779.380 ## 11 Ecuador Americas 2007 74.994 13755680 6873.262 ## 12 Venezuela Americas 2007 73.747 26084662 11415.806 ## 13 Nicaragua Americas 2007 72.899 5675356 2749.321 ## 14 Colombia Americas 2007 72.889 44227550 7006.580 ## 15 Jamaica Americas 2007 72.567 2780132 7320.880 ## 16 Brazil Americas 2007 72.390 190010647 9065.801 ## 17 Dominican Republic Americas 2007 72.235 9319622 6025.375 ## 18 El Salvador Americas 2007 71.878 6939688 5728.354 ## 19 Paraguay Americas 2007 71.752 6667147 4172.838 ## 20 Peru Americas 2007 71.421 28674757 7408.906 ## 21 Guatemala Americas 2007 70.259 12572928 5186.050 ## 22 Honduras Americas 2007 70.198 7483763 3548.331 ## 23 Trinidad and Tobago Americas 2007 69.819 1056608 18008.509 ## 24 Bolivia Americas 2007 65.554 9119152 3822.137 ## 25 Haiti Americas 2007 60.916 8502814 1201.637 2.3.4 Creando y modificando variables: mutate() Crear nuevas variables en base a los valores de otras variables que ya existen suele ser una parte necesaria para enriquecer el análisis. En el marco de tidyverse la forma de lograrlo es a través del verb (verb es tan solo otra forma de llamar a las funciones en el contexto de tidyverse) mutate(). Imagemos, por ejemplo, que queremos la población medida en millones de personas para hacer más fácil su lectura: new_gapminder &lt;- gapminder_df %&gt;% mutate(pop = pop / 1000000) head(new_gapminder, n = 3) # Head nos permite ver solo una determinada cantidad de filas ## country continent year lifeExp pop gdpPercap ## 1 Afghanistan Asia 1952 28.801 8.425333 779.4453 ## 2 Afghanistan Asia 1957 30.332 9.240934 820.8530 ## 3 Afghanistan Asia 1962 31.997 10.267083 853.1007 La sintaxis es simple: del lado izquierdo de la igualdad escribimos el nombre de la variable y del lado derecho definimos su valor (en este caso, el valor de pop dividido por un millón). Al utilizar el nombre de una variable que anteriormente ya existía no creamos una nueva, sino que reemplazamos a pop. Si escribimos el nombre de una variable que no existe R agrega esa variable al dataset: # Calcuando el PIB gapminder_df %&gt;% mutate(gdp = gdpPercap * pop) 2.3.5 Resumiendo y transformando datos en base a grupos Muchas veces es necesario resumir diversas variables de un dataset en base a grupos. En nuestro ejemplo, preguntas que precisarían de agrupar datos serían algunas como las siguientes: ¿Cuál es la expectativa de vida al nacer promedio por continente para cada uno de los años? ¿Cuál es el país más pobre y más rico, medido por PIB per cápita, de cada continente para cada uno de los años? ¿Cuál es la diferencia en la expectativa de vida al nacer para cada país con respecto a la media del continente para cada año? Para responder estas preguntas, tidyverse brinda dos funciones: group_by(), para agrupar observaciones según categorías de una variable, y summarise(), para aplicar alguna transformación sobre cada conjunto de datos. Veamos cómo podríamos resolver estas tres preguntas con estas funciones y otras que ya hemos aprendido. # Primera pregunta gapminder_df %&gt;% group_by(year, continent) %&gt;% summarise(mean_lifeExp = mean(lifeExp)) ## # A tibble: 60 x 3 ## # Groups: year [12] ## year continent mean_lifeExp ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1952 Africa 39.1 ## 2 1952 Americas 53.3 ## 3 1952 Asia 46.3 ## 4 1952 Europe 64.4 ## 5 1952 Oceania 69.3 ## 6 1957 Africa 41.3 ## 7 1957 Americas 56.0 ## 8 1957 Asia 49.3 ## 9 1957 Europe 66.7 ## 10 1957 Oceania 70.3 ## # ... with 50 more rows # Segunda pregunta gapminder_df %&gt;% group_by(year, continent) %&gt;% summarise(poor_country = min(gdpPercap), rich_country = max(gdpPercap), poor_country_nom = country[gdpPercap == poor_country], rich_country_nom = country[gdpPercap == rich_country]) ## # A tibble: 60 x 6 ## # Groups: year [12] ## year continent poor_country rich_country poor_country_nom rich_country_nom ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1952 Africa 299. 4725. Lesotho South Africa ## 2 1952 Americas 1398. 13990. Dominican Republic United States ## 3 1952 Asia 331 108382. Myanmar Kuwait ## 4 1952 Europe 974. 14734. Bosnia and Herzegovina Switzerland ## 5 1952 Oceania 10040. 10557. Australia New Zealand ## 6 1957 Africa 336. 5487. Lesotho South Africa ## 7 1957 Americas 1544. 14847. Dominican Republic United States ## 8 1957 Asia 350 113523. Myanmar Kuwait ## 9 1957 Europe 1354. 17909. Bosnia and Herzegovina Switzerland ## 10 1957 Oceania 10950. 12247. Australia New Zealand ## # ... with 50 more rows # Tercera pregunta gapminder_df %&gt;% group_by(year, continent) %&gt;% mutate(dif_lifeExp = lifeExp - mean(lifeExp)) ## # A tibble: 1,704 x 7 ## # Groups: year, continent [60] ## country continent year lifeExp pop gdpPercap dif_lifeExp ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. -17.5 ## 2 Afghanistan Asia 1957 30.3 9240934 821. -19.0 ## 3 Afghanistan Asia 1962 32.0 10267083 853. -19.6 ## 4 Afghanistan Asia 1967 34.0 11537966 836. -20.6 ## 5 Afghanistan Asia 1972 36.1 13079460 740. -21.2 ## 6 Afghanistan Asia 1977 38.4 14880372 786. -21.2 ## 7 Afghanistan Asia 1982 39.9 12881816 978. -22.8 ## 8 Afghanistan Asia 1987 40.8 13867957 852. -24.0 ## 9 Afghanistan Asia 1992 41.7 16317921 649. -24.9 ## 10 Afghanistan Asia 1997 41.8 22227415 635. -26.3 ## # ... with 1,694 more rows Las tres respuestas comienzan de la misma manera, es decir con la función group_by(). Entre los paréntesis hay que colocar los nombres de las variables por las cuáles se quiere agrupar, en nuestro caso las variables year y continent. De esta manera todo lo que siga después de la próxima pipe se aplicará sobre cada uno de los grupos generados por las combinaciones año y continente. En el caso de la primera respuesta, usamos la función summarise(), que devuelve un Data Frame (en rigor, tibble) con resúmenes para cada uno de los grupos. Debido a que existen 12 “fotos” de las variables (una cada cinco años) y se definen 5 continentes (África, América, Asia, Europa y Oceanía), el data frame que devuelve tiene 60 filas (12 países multiplicado por 5 continentes da 60 grupos). Dentro de los paréntesis de summarise() podemos crear tantas variables como queramos. En el primer caso, creamos una variable que se llama mean_lifeExp que recibe el resultado de aplicar mean() a la variable lifeExp. Esta función es aplicada a cada uno de los grupos que definimos anteriormente. En el segundo caso hacemos algo similar a lo anterior, pero definimos cuatro variables. poor_country busca el valor mínimo del PIB per cápita para cada grupo a través de min(), rich_country hace lo opuesto a través de la función max(), poor_country_nom busca el nombre que corresponde al PIB per cápita más bajo y rich_country_nom busca el nombre que corresponde al PIB per cápita más alto. Estas últimas dos variables se generan a través de filtrar el vector country y buscar el valor que corresponde al PIB per cápita más bajo o alto, según corresponda, mediante el código country[gdpPercap == poor_country]. Finalmente, la tercera parte del código combina las funciones group_by() y mutate(), que ya vimos anteriormente. La novedad es que ahora podemos utilizarla para crear nuevas variables en el dataset basadas en agregaciones de otras variables. En este ejemplo, creamos la variable dif_lifeExp, que toma la diferencia entre la expectativa de vida al nacer para cada observación y la media de cada grupo (en nuestro caso, año y continente). El siguiente gráfico muestra una visualización basada en este dataset que acabamos de generar, usando ggplot 2.4 Transformando la presentación de los datos: pivot_wider y pivot_longer Los datos pueden venir presentados en dos formatos: largo o ancho. Los datasets con formato largo tienen pocas columnas y muchas filas, mientras los de formato ancho poseen muchas columnas y pocas filas. Sin embargo, en ambas representaciones los datos son exactamente los mismos. En diversas situaciones es preferible tener una de las dos representaciones. Por ejemplo, para graficar con la librería ggplot2 muchas veces es conveniente contar con representaciones en formato largo de los datos. Tidyverse ofrece dos métodos que sirven para este propósito: pivot_wider() y pivot_longer() (estas funciones reemplazaron a gather() y a spread(), respectivamente). pivot_wider(): toma un conjunto de variables (vectores/columnas) y las colapsa en una sola columna con valores que resumen los datos de ese conjunto de variables. Hace que el data frame sea más largo pivot_longer(): toma dos variables y las descompone entre múltiples variables (hace que el data frame sea más ancho) Como pueden ver, no existe una medida absoluta de largo o ancho. Simplemente un dataset puede tener una representación más ancha o más larga. Vamos a exportar una tabla desde R para que lo puedan usar en otro software. En general, cuando los datos ya han sido procesados, las salidas se muestran en tablas más bien anchas. Con los datos que contamos vamos a crear un archivo .csv que tenga en las filas a los paises y en las columnas a los años (como valor, a la pboblación en millones). # Nos quedamos con tres coolumnas: country, year y pop. # Además, con mutate hacemos que la población esté representada por millones gapminder_sub &lt;- gapminder_df %&gt;% select(country,year,pop) %&gt;% mutate(pop = round(pop / 1000000,1)) head(gapminder_sub,n = 5) # Muestra las primeras cinco filas ## country year pop ## 1 Afghanistan 1952 8.4 ## 2 Afghanistan 1957 9.2 ## 3 Afghanistan 1962 10.3 ## 4 Afghanistan 1967 11.5 ## 5 Afghanistan 1972 13.1 Ya estamos en condiciones de usar pivot_wider masAncho &lt;- gapminder_sub %&gt;% pivot_wider(names_from = year,values_from = pop) head(masAncho,6) ## # A tibble: 6 x 13 ## country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997` `2002` `2007` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 8.4 9.2 10.3 11.5 13.1 14.9 12.9 13.9 16.3 22.2 25.3 31.9 ## 2 Albania 1.3 1.5 1.7 2 2.3 2.5 2.8 3.1 3.3 3.4 3.5 3.6 ## 3 Algeria 9.3 10.3 11 12.8 14.8 17.2 20 23.3 26.3 29.1 31.3 33.3 ## 4 Angola 4.2 4.6 4.8 5.2 5.9 6.2 7 7.9 8.7 9.9 10.9 12.4 ## 5 Argentina 17.9 19.6 21.3 22.9 24.8 27 29.3 31.6 34 36.2 38.3 40.3 ## 6 Australia 8.7 9.7 10.8 11.9 13.2 14.1 15.2 16.3 17.5 18.6 19.5 20.4 La función pivot_wider() necesita solo dos parámetros. En primer lugar, debemos decirle de cuál columna hay que tomar los nuevos nombres de columnas en el parámetro names_from. Sn segundo lugar, solo tenemos que decirle de qué columna tiene que tomar los valores, en values_from. ¿Qué tenemos que hacer para guardar esta salida? usamos la función write.table(). Vamos a usar cuatro parámetros. En x solo tenemos que pasarle el objeto a escribir, en file un nombre de archivo (vean que es .csv), en sep usamos un caracter que queremos que use para separar a las columnas (recomiendo usar ;). Finalmente, en el parámetro row.names usamos FALSE con el objetivo que no incluya el número de filas en la salida write.table(x = masAncho, file = &#39;PaisPob.csv&#39;, sep = &#39;;&#39;, row.names=FALSE) Ahora que exportamos nuestro primer data frame, podemos ponernos del otro lado del mostrador. Imaginemos que nos pasan un .csv con estos datos, pero nosotros queremos procesarlos todavía un poco más. En ese caso, muchas de las herramientas que conocemos nos piden que los datos estén un poco más “largos”. Acá entra en juego pivot_longer(), pero antes tenemos que leer los datos que acabamos de exportar # El parámetro header sirve para avisar que los datos tienen una primera # fila que es el nombre de las columnas y check.names es otro parámetro # que es neceario cuando los nombres de las columnas son números masAncho &lt;- read.table(file = &#39;PaisPob.csv&#39;, sep = &#39;;&#39;, header=TRUE, check.names = FALSE, stringsAsFactors = FALSE) head(masAncho) ## country 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 ## 1 Afghanistan 8.4 9.2 10.3 11.5 13.1 14.9 12.9 13.9 16.3 22.2 25.3 31.9 ## 2 Albania 1.3 1.5 1.7 2.0 2.3 2.5 2.8 3.1 3.3 3.4 3.5 3.6 ## 3 Algeria 9.3 10.3 11.0 12.8 14.8 17.2 20.0 23.3 26.3 29.1 31.3 33.3 ## 4 Angola 4.2 4.6 4.8 5.2 5.9 6.2 7.0 7.9 8.7 9.9 10.9 12.4 ## 5 Argentina 17.9 19.6 21.3 22.9 24.8 27.0 29.3 31.6 34.0 36.2 38.3 40.3 ## 6 Australia 8.7 9.7 10.8 11.9 13.2 14.1 15.2 16.3 17.5 18.6 19.5 20.4 Ahora ya podemos recrear nuestra versión anterior, más larga reconstruccion &lt;- masAncho %&gt;% pivot_longer(cols = 2:13, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% arrange(country,year) head(reconstruccion, n = 5) # Muestra las primeras cinco filas ## # A tibble: 5 x 3 ## country year pop ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 8.4 ## 2 Afghanistan 1957 9.2 ## 3 Afghanistan 1962 10.3 ## 4 Afghanistan 1967 11.5 ## 5 Afghanistan 1972 13.1 La función pivot_longer() requiere un poco más de atención. A diferencia de pivot_wider, tenemos que 1) definir el nombre de la nueva variable que tendrá como categoría los nombres de otras columnas (parámetro names_to), 2) el nombre de la variable que tendrá los valores que estaban en variables que ahora se resumirán en la nueva columna (parámetro values_to y 3) las columnas que quieren colapsarse dentro de las dos nuevas variables (van en cols). Los primeros dos parámetros no tienen gran complejidad, pero la selección de las variables puede realizarse de varias maneras. En el caso anterior, lo hicimos indicando la posición de las columnas en el Data Frame. En el siguiente código exhibimos tres métodos que logran lo mismo. reconstruccion2 &lt;- masAncho %&gt;% pivot_longer(cols = -country, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% arrange(country,year) # Todas las columnas menos country reconstruccion3 &lt;- masAncho %&gt;% pivot_longer(cols = -1, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% arrange(country,year) # Todas las columnas menos la primera reconstruccion4 &lt;- masAncho %&gt;% pivot_longer(cols = c(&#39;1952&#39;,&#39;1957&#39;,&#39;1962&#39;,&#39;1967&#39;,&#39;1972&#39;,&#39;1977&#39;, &#39;1982&#39;, &#39;1987&#39;,&#39;1992&#39;,&#39;1997&#39;,&#39;2002&#39;,&#39;2007&#39;), names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% arrange(country,year) # Todas las columnas con esos nombres # Esta línea nos devuelve TRUE si los 4 objetos sin iguales entre sí o FALSE si hay # al menos uno que no lo es all(sapply(list(reconstruccion2, reconstruccion3, reconstruccion4), FUN = identical, reconstruccion)) ## [1] TRUE Algunas leves sutilezas ¿Es igual nuestro dataset reconstruido al original? Veamoslo identical(gapminder_sub,reconstruccion) ## [1] FALSE R nos dice que no, que no son iguales ¿Cómo puede ser? tienen exactamente los mismos valores, mismas filas y mismas columnas… Indagemos un poco más sobre el tipo de datos de las columnas class(gapminder_sub$year) ## [1] &quot;integer&quot; class(reconstruccion$year) ## [1] &quot;character&quot; El problema es que la reconstrucción tiene a la variable de años guardada como tipo character, mientras que en el original era númerica ¿Qué fue lo que pasó?. Lo que sucede es normal: siempre que usen pivot_longer() y los nombres de las variables sean números, va a asumir que es texto, ya que eso es lo que se espera de los nombres de las columnas. No se preocupen, podemos usar as.integer() para realizar la conversión reconstruccion &lt;- reconstruccion %&gt;% mutate(year=as.integer(year)) # Además hay otra razón (menor) por la cual no son iguales, y es que # gapminder_sub es solo un data frame y reconstruccion es un # tibble, un data.frame modificado para tidyverse. gapminder_sub &lt;- gapminder_sub %&gt;% as_tibble(gapminder_sub) # Son iguales ! identical(reconstruccion, gapminder_sub) ## [1] TRUE 2.5 Uniendo datos de distintas fuentes: left_join Otra de las transformaciones más comunes que hacemos sobre los datos es agregar nueva información en base a una (o más) columnas en las que coinciden. Por ejemplo, sabiendo que un dato corresponde a Argentina, podríamos agregar variables específicas para ese país sobre otra dimensión, como podría ser la tasa de mortalidad infantil. Haremos exactamente esto, con la ayuda de otro dataset: mortalidadInfantil &lt;- read.table(file=&quot;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/MortalidadInfantilLong.csv&quot;, sep = &quot;,&quot;, header = TRUE, stringsAsFactors = FALSE) gapminder_argentina &lt;- gapminder_df %&gt;% filter(country == &quot;Argentina&quot;) gapminder_argentina1952 &lt;- gapminder_argentina %&gt;% filter(year==1952) mortalidadInfantilArgentina1952 &lt;- mortalidadInfantil %&gt;% filter(Year==1952 &amp; country==&quot;Argentina&quot;) gapminder_argentina1952 &lt;- left_join(gapminder_argentina1952, mortalidadInfantilArgentina1952, by=&quot;country&quot;) gapminder_argentina1952 ## country continent year lifeExp pop gdpPercap Year Mortalidad ## 1 Argentina Americas 1952 62.485 17876956 5911.315 1952 88.8 La función clave es left_join(). Lo que hace es tomar dos data frames y los une por una o más variables que tienen en común, que en este caso es la variable country. Por cada fila del primer dataset que le pasamos, busca el valor que corresponde en la variable que tiene een comun en el segundo data frame que le pasamos, y agrega como columna la información que existe en el segundo data frame. De esta manera, agregamos la mortalidad infantil para Argentina en 1952, que es de 88.8 cada 1000 niños de entre 0 y 5 años. En rigor, con este dataset podemos agregar más que la información de la mortalidad infantil para Argentina en 1952, sino que lo podemos hacer para todas las observaciones de la siguiente manera: gapminder_full &lt;- left_join(gapminder_df, mortalidadInfantil, by=c(&quot;country&quot;,&quot;year&quot;=&quot;Year&quot;)) gapminder_full Si prestaron atención, hicimos dos cosas distintas esta vez: elegimos dos variables para hacer el join: country e year ¿Por qué? Porque de esa manera podemos completar la información sobre la mortalidad infantil para cada combinación de país y año. Pero por otro lado, al nombre de la segunda columna le aclaramos que “year” en el primer data frame en realidad se llama “Year” en el segundo. Prueben que pasa si solo dejan “year” en esa segunda parte. 2.6 La mise en place: preparando el dataset de inmuebles A esta altura vimos una gran cantidad de comandos para transformar datos con un interesante dataset como es gapminder. Sin embargo, probablemente hayan sido demasiados para procesarlos de una sola vez. Lo que vamos a hacer ahora es aplicarlo en nuestro objetivo inicial: transformar los datasetas tal como se descargan desde Properati hasta el dataset que trabajamos en el capítulo anterior. Vamos a usar varios de los comandos que ya vimos. Vamos a trabajar con una versión levemente modificada de los datos descargados de Properati porque vamos a procesar una muestra estratificada por barrio y año. Esto quiere decir que tomamos aleatoriamente 30 observaciones (siempre que las hayan) de cada combinación barrio y año. Esto lo hacemos para reducir la cantidad de filas, que eran más de 400mil en el dataset original y logramos reducirlas sensiblemente, sin perder generalidad en nuestra explicación. barriosOriginal &lt;- read.table(file=&quot;https://github.com/datalab-UTDT/datasets/raw/master/barriosSample.csv&quot;, sep = &quot;;&quot;, header = TRUE, stringsAsFactors = FALSE) dim(barriosOriginal) # Número de filas y de columnas ## [1] 4780 31 colnames(barriosOriginal) # Nombres de las columnas ## [1] &quot;BARRIOS&quot; &quot;COMUNA&quot; &quot;NUM_DE_BAR&quot; ## [4] &quot;created_on&quot; &quot;operation&quot; &quot;property_type&quot; ## [7] &quot;place_name&quot; &quot;place_with_parent_names&quot; &quot;geonames_id&quot; ## [10] &quot;lat.lon&quot; &quot;price&quot; &quot;currency&quot; ## [13] &quot;price_aprox_local_currency&quot; &quot;price_aprox_usd&quot; &quot;surface_in_m2&quot; ## [16] &quot;price_usd_per_m2&quot; &quot;floor&quot; &quot;rooms&quot; ## [19] &quot;expenses&quot; &quot;properati_url&quot; &quot;image_thumbnail&quot; ## [22] &quot;description&quot; &quot;title&quot; &quot;extra&quot; ## [25] &quot;surface_total_in_m2&quot; &quot;surface_covered_in_m2&quot; &quot;price_per_m2&quot; ## [28] &quot;id&quot; &quot;country_name&quot; &quot;state_name&quot; ## [31] &quot;year&quot; Recuerden que el dataset con el que trabajamos en el capítulo anterior tenia tan solo 6 variables, que contenían 1) los nombres de los barrios y 2) el valor de los precios en dólares para cada año (2013-2017). Para empezar, vamos a quedarnos solo con las variables que son relevantes: barrios, precios en dolares y años barriosOriginal &lt;- barriosOriginal %&gt;% select(BARRIOS, price_usd_per_m2, year) str(barriosOriginal) ## &#39;data.frame&#39;: 4780 obs. of 3 variables: ## $ BARRIOS : chr &quot;AGRONOMIA&quot; &quot;AGRONOMIA&quot; &quot;AGRONOMIA&quot; &quot;AGRONOMIA&quot; ... ## $ price_usd_per_m2: num 1304 1642 1333 2800 1467 ... ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... Ahora, necesitamos calcular el precio PROMEDIO por barrio y año. Por suerte, ya conocemos las funciones group_by() y summarise(). Es todo lo que necesitamos para esta transformación: barriosOriginal &lt;- barriosOriginal %&gt;% group_by(BARRIOS, year) %&gt;% summarise(precioPromedio = mean(price_usd_per_m2)) Ya estamos bastante cerca de nuestro objetivo: contamos con un Data Frame donde se registra el precio promedio en USD del metro cuadrado de los inmuebles por barrio para el período 2013-2017. La diferencia con el dataset de la clase pasada es que este se presenta en formato largo y el anterior en formato ancho. La función pivot_wider, que ya vimos anteriormente, va a hacer lo que necesitamos barriosOriginal &lt;- barriosOriginal %&gt;% pivot_wider(names_from = year, values_from = precioPromedio) head(barriosOriginal, n = 3) ## # A tibble: 3 x 6 ## # Groups: BARRIOS [3] ## BARRIOS `2013` `2014` `2015` `2016` `2017` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AGRONOMIA 1682. 1254. 1588. 2051. 1781. ## 2 ALMAGRO 2024. 1922. 1989. 1891. 2270. ## 3 BALVANERA 1857. 1791. 1908. 1642. 1668. Ya casi estamos ! Lo único que nos falta es cambiar el nombre de las columnas. Para eso vamos a utilizar la función paste(): colnames(barriosOriginal)[2:6] &lt;- paste(&#39;USDm2_&#39;,colnames(barriosOriginal)[2:6], sep=&quot;&quot;) head(barriosOriginal, n =3) ## # A tibble: 3 x 6 ## # Groups: BARRIOS [3] ## BARRIOS USDm2_2013 USDm2_2014 USDm2_2015 USDm2_2016 USDm2_2017 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AGRONOMIA 1682. 1254. 1588. 2051. 1781. ## 2 ALMAGRO 2024. 1922. 1989. 1891. 2270. ## 3 BALVANERA 1857. 1791. 1908. 1642. 1668. Lo que hace esta función es tomar vectores y concatenarlos entre sí. En este caso le pasamos dos vectores: uno de tipo character con un solo elemento (“USDm2_”) y otro que contiene la posición 2 a 6 del vector character colnames(barriosOriginal), es decir los nombres de las variables 2 a 6 del dataset barriosOriginal. En estos simples pasos ya construimos el dataset con el que trabajamos la clase anterior desde el formato que tenían cuando se descargaron desde el portal de datos. 2.7 Ejercicios Vuelvan a cargar el data frame de gapminder y respondan las siguientes preguntas: ¿Cuál es la observación con mayor expectativa al necer de todo el dataset? ¿A qué país corresponde y en qué año? ¿Cuál es la expectativa de vida a nacer promedio por continente en 1952? ¿Y en 2007? ¿Cuánto aumento la expectativa de vida al nacer por continente entre 2007 y 1952? ¿Cuál fue el país, por continente, que más aumentó su expectativa de vida al nacer en términos absolutos? Entre 1952 y 2007 ¿Cuál fue el país que más aumento su PIB per cápita? ¿Y por continente? ¿Cuánto aumento el PIB per cápita de Argentina entre 1952 y2007? ¿Y entre 1977 y 2002? 2.8 Extensiones 2.8.1 R Cheatsheets RStudio elabora y publica de manera periódica distintas “cheatsheets” donde tienen resumidas todas las funciones de algunos paquetes, para qué sirven y ejemplo sencillos. Suelen ser muy útiles como para tener presente cuando trabajan con sus datasets y para que conozcan más funciones que las introducidas en el libro. Además, los paquetes van cambiand con el tiempo y algunas funciones pueden quedar desactualizadas. En https://rstudio.com/resources/cheatsheets/ van a encontrar todas las disponibles. La relevante para este capítulo es la del paquete dyplr https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf Este video resume en unos pocos minutos gran parte de lo que vamos a ver acá y es recomendable↩ "],
["visualizaciones-de-datos-en-r.html", "3 Visualizaciones de datos en R 3.1 La importancia de la visualización de los datos 3.2 GGPLOT: Grammar of Graphics 3.3 ¿Cuál es la relación entre el ingreso de un país y la expectativa de vida al nacer? Scatterplot 3.4 ¿Cuál fue la evolución de la expectativa de vida al nacer? Gráfico de líneas 3.5 Reproduciendo el gráfico de Hans Rosling 3.6 Mapas 3.7 Ejercicios 3.8 Extensión: animando el gráfico de Hans Rosling 3.9 Material de lectura", " 3 Visualizaciones de datos en R Al terminar este capítulo ustedes van a poder: - Entender la sintáxis usada por ggplot - Hacer gráficos de dispersión de puntos y de líneas - Modificar los valores predeterminados de las paletas de colores, etiquetas, títulos, entre otros elementos de los gráficos - Exportar los gráficos con calidad de publicación - Animar y exportar animaciones de gráficos 3.1 La importancia de la visualización de los datos La visualización de datos ha ganado espacio en diversas publicaciones y en las últimas décadas ya constituye una disciplina en sí misma. Su relevancia podría justificarse de muchas maneras, pero hay un punto que es especialmente relevante para el análisis de datos: los parámetros y coeficientes con los que solemos trabajar no siempre son tan simples de interpretar como pensamos. Por ejemplo, algunos autores recomiendan fuertemente graficar las predicciones del modelo ante distintos valores, antes que los coeficientes del modelo (McElreath, 2016). Este punto dista de ser uno moderno. En 1973 el estadístico Francis Anscombe aportó la siguiente evidencia sobre cómo muchas de las variables que solemos tomar como de resumen, o incluso de relación entre ellas, puede engañarnos si no visualizamos correctamente los datos: Si prestan atención, van a poder ver que la nube de puntos de cada uno de los datasets es bien distinta. El primero de arriba a la izquierda parece un scatter plot alrededor de una media, que podría modelarse linealmente. El segundo, el de arriba a la derecha, parecería tener una relación más cuadrática. El tercero, el de abajo a la izquierda, muestra una relación linea aparentemente con baja volatilidad, pero un punto bien alejado de esa recta, mientras que el último muestra casi ninguna relación entre x e y, con la excepción de un punto muy alejado del resto. ¿Pero que sucede cuando trazamos una línea que minimiza la distancia cuadrática entre los puntos? La recta (azul) es ¡la misma! ¿Y qué pasa con los promedios de las variables y la correlación entre ellas? También son iguales: ## # A tibble: 4 x 4 ## dataset promedioX promedioY corXY ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Dataset 1 9 7.50 0.816 ## 2 Dataset 2 9 7.50 0.816 ## 3 Dataset 3 9 7.5 0.816 ## 4 Dataset 4 9 7.50 0.817 De esta manera queda en claro que, en algunas ocasiones, trabajar con resumenes de nuestros datos puede llevarnos a extraer conclusiones erróneas sobre cómo deberíamos analizarlos. Durante los años se fue mejorando este punto que hizo Anscombe y… ahora sabemos que una nube de puntos que se parece a un dinosauro: si nos guiamos solo por las clásicas medidas de resumen, son indistinguibles 3.2 GGPLOT: Grammar of Graphics Cuando se trata de hacer visualizaciones en R, ggplot es, por lejos, nuestro principal aliado y la librería más ampliamente difundida. Este paquete viene incluido dentro de tidyverse, por lo que cargando dicho paquete ya van a contar con todas sus funciones. Con ggplot podemos crear decenas de distintos tipos de gráficos mediante una sintáxis unificada, lo que hace que se ganen importante economías de escala, una vez superada la curva de aprendizaje. library(tidyverse) 3.3 ¿Cuál es la relación entre el ingreso de un país y la expectativa de vida al nacer? Scatterplot Una nube de puntos o scatter plot es un tipo de gráfico que, por lo general, muestra la relación entre dos variables. Digo por lo general, porque en rigor es posible agregar otra información en estos gráficos mediante colores o tamaños de los puntos. Sigamos con el ejemplo del capitulo dos: los datos de gapminder. Vamos a filtrar los datos para el año 2007, el último en este dataset. gapminder_df &lt;- read.table(file = &quot;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/gapminder.csv&quot;, sep=&#39;;&#39;, header = TRUE, stringsAsFactors = FALSE) gapminderLastCut &lt;- gapminder_df %&gt;% filter(year==2007) Ya podemos comenzar con nuestro primer ejemplo. ggplot comienza siempre dela misma manera: usando la función homónima y comentando que datos queremos graficar. Luego, le decimos qué tipo de gráfico queremos hacer, lo que en GG se hace con geom_, seguido por un sufijo que hace referencia específica al tipo de gráfico. Además, GGPLOT no hace magia: en este caso, necesita saber qué variable poner en el eje x y cuál otra poner en el eje y. En ggplot, estas últimas indicaciones van dentro de una función que se llama aes(), que es la abreviación de aesthetics. ggplot(data = gapminderLastCut, mapping = aes(x=gdpPercap, y = lifeExp)) + geom_point() Nuestro gráfico scatter, que en ggplot se hace con geom_point(), muestra en el eje horizontal el PIB per cápita de los países y en el eje vertical la expectativa de vida al nacer. La grilla de fondo de color gris y los títulos de los ejes son defaults de ggplot, así como la - no tan recomendada - decisión de cortar al eje vertical en un valor levemente superior a 40 años. Una vez que tenemos los datos que queremos en nuestro gráfico, podemos empezar a cambiar estos detalles que pueden no gustarnos. Para empezar, los títulos de los ejes tienen por definición el nombre de las variables, vamos a ponerles nombres más acordes a lo que estamos mostrando. Como irán aprendiendo, estos cambios van en distintas partes de nuestro código de ggplot mediante nuevas funciones agregadas por medio de +: ggplot(data = gapminderLastCut, mapping = aes(x=gdpPercap, y = lifeExp)) + geom_point() + labs(x = &quot;PIB per cápita&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;) En este caso usamos la función labs(), con sus respectivos parámetros x e y, a los que les pasamos directamente el nombre que queremos que tenga. La función labs permite más cosas que esto, como agregar título, subtítulos e incluso información sobre la fuente de nuestro gráfico: ggplot(data = gapminderLastCut, mapping = aes(x=gdpPercap, y = lifeExp)) + geom_point() + labs(x = &quot;PIB per cápita&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;A más ingresos mayor tiempo de vida?&quot;, subtitle=&quot;Expectativa de vida al nacer según nivel de ingreso&quot;, caption=&quot;Fuente: Gapminder&quot;) 3.3.1 Agregando colores según otras variables En nuestro data.frame de gapminder contamos con otra variable que sería de interés mostrar: el continente del país que hablamos. Esta variable es de tipo categórica y es muy común agregar esta clase de variables en nuestros gráficos de dispersión o scatter plots Agregar esta clase de información es realmente fácil. Dentro de la función aes(), además de determinar cuáles son los valores del eje vertical (y) y del eje horizontal (x), podemos indicar cuál es la variable según la cual queremos que ponga los colores: mediante el argumento color ggplot(data = gapminderLastCut, mapping = aes(x=gdpPercap, y = lifeExp,color=continent)) + geom_point() + labs(x = &quot;PIB per cápita&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;A más ingresos mayor tiempo de vida?&quot;, subtitle=&quot;Expectativa de vida al nacer según nivel de ingreso&quot;, caption=&quot;Fuente: Gapminder&quot;) Podemos observar cómo los colores no se distribuyen aleatoriamente entre niveles de ingreso, sino que se ubican más o menos en los mismos rangos, con algunas excepciones. Más adelante vamos a ver un gráfico que nos va a ser útil para detectar estás diferencias. 3.4 ¿Cuál fue la evolución de la expectativa de vida al nacer? Gráfico de líneas Otro de los gráficos más simples consiste en analizar la evolución de una determinada variable en el tiempo mediante una línea de tiempo. En ggplot estos gráficos pueden crearse usando geom_line. Digamos que queremos observar la evolución de la expectativa de vida al nacer promedio por continente. Usando las herramientas de tidyverse podemos generar este promedio de la siguiente manera: promedioContinente &lt;- gapminder_df %&gt;% group_by(continent,year) %&gt;% summarise(promedio=mean(lifeExp)) Luego, ya estamos en condiciones de hacer el gráfico: ggplot(data = promedioContinente, mapping = aes(x=year, y = promedio,color=continent)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;Expectativa de vida al nacer según continente&quot;, caption=&quot;Fuente: Gapminder&quot;) Ahora que hicimos nuestro gráfico de líneas, presten atención a dos puntos. Por un lado, cuando usamos el parámetro color en combinación con geom_line() ggplot entiende que esa variable debe ser la que corta a los datos, y los valores de cada una de ellas debe ser graficada por separada. Por otro lado, y en relación al anterior punto, ggplot genera de manera automática una leyenda que nos permite unir el color de la variable por la que abrimos a los datos y su categoría. Veamos cómo cambiar la apariencia de la leyenda, lo que nos llevará a conocer otras funciones importantes de ggplot. Si al intentar hacer este gráfico se decepcionaron al ver que no ven ninguna línea, no se preocupen. Miren la consola y lean si no está el siguiente mensaje:geom_path: Each group consists of only one observation. Do you need to adjust the group aesthetic? En caso de ser así, lo que sucede es que ggplot agrupa de manera predeterminada a aquellas variables que no son numéricas. Como dos de las tres variables que están usando son de tipo charactere, entonces ggplot las agrupa y hay un problema al hacer líneas: necesitamos al menos dos puntos para hacer una línea. Tenemos dos soluciones. La primera consiste en convertir a alguna de las variables que están como caracteres (por ejemplo, es posible que tengan la variable year como character). La segunda solución es usar el argumento group dentro de la función aes(), este argumento explicita por cual variable queremos agrupar, por lo que desactivamos la acción que tiene ggplot de manera determinada. Hay un ejercicio al final de este capítulo que repasa este punto. 3.4.1 Cambiando la apariencia de las leyendas Para modificar la presentación de la leyenda podemos usar distintas funciones. Imaginen que deseamos cambiar el título de la leyenda. Esto se hace simplemente con una leve modificación dentro de la función labs(), agregando un parámetro que haga referencia al aesthetic que mapearon. En este caso, corresponde color: ggplot(data = promedioContinente, mapping = aes(x=year, y = promedio,color=continent)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;Expectativa de vida al nacer según continente&quot;, caption=&quot;Fuente: Gapminder&quot;, color =&#39;Continente&#39;) Muchas veces también queremos cambiar el lugar en el que está la leyenda, o quizás su dirección (puede estar de manera vertical, como en el ejemplo, u horizontal). Pero antes de mostrar cómo hacer esto, vamos a ver que podemos guardar gráficos de ggplot como objetos: graficoLinea &lt;- ggplot(data = promedioContinente, mapping = aes(x=year, y = promedio,color=continent)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;Expectativa de vida al nacer según continente&quot;, caption=&quot;Fuente: Gapminder&quot;, color =&#39;Continente&#39;) # Si ejecutan la siguiente línea va a devolver el gráfico # graficoLinea Esta funcionalidad nos va a servir tanto como para ir agregando “capas” a nuestro gráfico como para exportar nuestros gráficos, como veremos posteriormente. Ahora, modifiquemos de lugar a la leyenda usando la función theme() graficoLinea + theme(legend.position = &quot;bottom&quot;) # graficoLinea + theme(legend.position = &quot;top&quot;) Con la función theme() podemos cambiar todos aquellos aspectos que no tengan que ver con el contenido, sino con las formas, tamaños y disposiciones de nuestros objetos, veremos más opciones de esto cuando queramos guardar nuestros gráficos. Muchas veces no queremos agregar las leyendas, que por default ggplot las muestra ¿Cómo evitamos que se muestren? con legend.position=“none”. Otra clásica situación es querer cambiar el orden de la leyenda. Por default ggplot usa el orden que tenga la variable factor (categórica) por la que estamos abriendo a los datos. Si bien podemos cambiar este orden en las variables originales, suele ser una mejor idea modificarlas específicamente para el gráfico. Esto lo podemos hacer con scale_color_manual. Esta función en rigor define información relevante para la paleta y su relación con los colores. Los breaks los valores que la leyenda puede tomar, mientras que en values directamente debemos indicar los colores para cada uno de estos valores en el sistema hexadecimal. Vean el siguiente ejemplo: graficoLinea + scale_color_manual(breaks=c(&quot;Europe&quot;,&quot;Asia&quot;,&quot;Oceania&quot;,&quot;Africa&quot;,&quot;Americas&quot;), values = c(&quot;#E41A1C&quot;,&quot;#377EB8&quot;,&quot;#4DAF4A&quot; ,&quot;#984EA3&quot;,&quot;#FF7F00&quot;)) Vean cómo ahora el orden de las leyendas se desplegan según la secuencia que nosotros queríamos y los colores han cambiado de acauerdo a nuestro vector values ¿De dónde elegí esos colores? De las muy buenas paletas de colores que ofrece el paquete RColorBrewer y que pueden explorar acá o mediante el siguiente código, una vez que lo hayan instalado: library(RColorBrewer) display.brewer.all() Como pueden ver el paquete cuenta con un conjunto de paletas que son útiles en diversas situaciones. En este caso, los cinco colores que usé en el gráfico anterior pertenecen a la paleta set1. Generar estos vectores es muy simple, solo tenemos que usar la función brewer.pal() con la cantidad de colores a generar y la paleta desde donde obtenerlos brewer.pal(n = 5,name = &quot;Set1&quot;) ## [1] &quot;#E41A1C&quot; &quot;#377EB8&quot; &quot;#4DAF4A&quot; &quot;#984EA3&quot; &quot;#FF7F00&quot; 3.5 Reproduciendo el gráfico de Hans Rosling Ahora que ya introdujimos algunos de los gráficos más usados estamos en condiciones de reproducir uno de los gráficos de Rosling para el año 2007. En el camino, vamos a introducir algunos puntos básicos y relevantes de ggplot. Recuerden que en gapminderLastCut tenemos los datos pertenecientes al último año de nuestro dataset, 2007. Trabajaremos con estos datos, agregando capas de a una a la vez y aclarando, en los comentarios, para que sirven: # Definición de los datos a graficar y de qué mostrar en # cada aesthetic gapminder2007 &lt;- ggplot(data = gapminderLastCut, mapping = aes(x = gdpPercap, y = lifeExp, color=continent, size=pop)) # Tipo de gráfico a hacer gapminder2007 &lt;- gapminder2007 + geom_point() # ¿Qué leyendas mostrar? No mostrar aquella relacionada con el tamaño. Prueben qué pasa si no agregan esto al gráfico gapminder2007 &lt;- gapminder2007 + guides(size=FALSE) # Elegimos un tema preestablecido, en este caso *minimal* gapminder2007 &lt;- gapminder2007 + theme_minimal() # El eje x, que tiene una variable continua, queremos que muestre # los datos en logaritmos para evitar que los puntos se &quot;junten&quot; # muy cerca del inicio y no nos permitan ver los cambios gapminder2007 &lt;- gapminder2007 + scale_x_continuous(trans = &#39;log10&#39;) gapminder2007 Hemos agregado un par de funciones más que vale la pena aclarar. En primer lugar, el uso de la función guides(), que entre otras cosas es útil para elegir qué leyendas queremos que aparezcan en nuestro gráfico y qué no. Por definición, cada aesthetic que agregamos (color, tamaño, símbolo) tiene un correlato en la leyenda. Pero en nuestro caso no queremos que muestre los tamaños con un determinado tamaño de población: basta con saber que mientras más grande es el tamaño de la bola, más alta es la población del país. Para eso agregamos guides(size=FALSE) En segundo lugar, incluimos theme_minimal(). Esta función, así como todas las que comienzan con theme_ son un conjunto de código que iría dentro de theme() y que nos permiten generar estilos preestablecidos de una manera muy simple. Minimal, por ejemplo, elimina el fondo gris con rayas blancas que es lo predeterminado en ggplot. Escriban “theme” y luego presionen tab en RStudio y verán más opciones como theme_bw, theme_dark. Pruebenlos. Finalmente, con scale_x_continuous() podemos transformar la presentación de los datos sin crear inncesariamente variables en nuestro data frame. En este caso queremos transformar los valores a logaritmo para evitar que las importantes diferencias en PIB per cápita entre los países no nos permitan ver variaciones relativas importantes entre los países de ingresos bajos y medios-bajos. Ahora vamos a usar prácticamente todo lo aprendido en este capítulo para armar un gráfico que podremos exportar. A nuestro último gráfico le agregamos algunos detalles como la mejora de la leyenda, otros colores y distintos títulos. gapminder2007 &lt;- gapminder2007 + scale_color_manual(breaks=c(&quot;Europe&quot;,&quot;Asia&quot;,&quot;Oceania&quot;,&quot;Africa&quot;,&quot;Americas&quot;), values = c(&quot;#E41A1C&quot;,&quot;#377EB8&quot;,&quot;#4DAF4A&quot; ,&quot;#984EA3&quot;,&quot;#FF7F00&quot;)) + labs(x = &quot;PIB per cápita&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;A más ingresos mayor tiempo de vida?&quot;, subtitle=&quot;Expectativa de vida al nacer según nivel de ingreso&quot;, color=&quot;Continente&quot;, caption=&quot;Fuente: Gapminder&quot;) Si quieren ejecuten el objeto gapminder2007 para ver cómo quedó. Ahora iremos con la última parte de este capítulo: cómo exportar los gráficos que hacemos. 3.5.1 Exportando gráficos de ggplot Exportar los gráficos es una tarea clave, y es realmente fácil con ggplot. Solo debemos usar la función ggsave: ggsave(filename = &quot;hansRosling2007.png&quot;, plot = gapminder2007, dpi = 300) ggsave nos pide un nombre de archivo, y un gráfico a exportar. Luego, nosotros podemos incluir información sobre la calidad del gráfico o ciertos parámetros adicionales. DPI es la cantidad de Dot Per Inch (puntos por pulgadas) que queremos que tenga. Mientras mayor sea este número, mayor será la “calidad” del gráfico (y tendrá mayores píxeles y peserá más. Ejecuten el código, deberían tener algo similar a esto. Figure 3.1: Primer gráfico exportado. Gapminder para el año 2007 3.6 Mapas En el capítulo 4 de este mismo libro introducimos a los datos espaciales. Allí vamos a analizar por qué requieren de un tratamiento especial tanto en el formato en el que están almacenados y su manera de representarse. En esta sección vamos a mostrar las capacidades que tiene el paquete ggplot para hacer mapas en R. Vamos a trabajar con dos data frames, uno que contiene el precio de los inmuebles de la Ciudad de Buenos Aires como puntos, y otro que tiene el precio de los inmuebles promeido por barrio. Ambos data frame son, en realidad, objetos sf, que se ven en detalle en el capítulo 4. library(sf) preciosCABA &lt;- read_sf(&#39;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/PreciosCABASample.geojson&#39;) preciosCABABarrios &lt;- read_sf(&#39;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/PreciosCABABarrioYear.geojson&#39;) proyeccionCenso2010 &lt;- &#39;+proj=tmerc +lat_0=-34.629717 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +a=6378388 +b=6378386.996621622 +towgs84=-148,136,90,0,0,0,0 +units=m +no_defs &#39; preciosCABA &lt;- st_transform(preciosCABA,crs = proyeccionCenso2010) preciosCABABarrios &lt;- st_transform(preciosCABABarrios,crs = proyeccionCenso2010) Una vez que ejecuten todo el código van a tener dos objetos nuevos: preciosCABA y preciosCABABarrios. El primero de ellos es una muestra estratificada por año y barrios (es decir que toma una muestra para cada uno de estos grupos) de los anuncios de ventas de inmuebles en la Ciudad de Buenos Aires, mientras que el segundo muestra la evolución por año del precio promedio en USD de los inmuebles ofertados. Hacer un gráfico con objetos de clase sf es realmente fácil con ggplot, solo debemos agregar geom_sf. Imaginen que queremos graficar los avisos de todos los años: ggplot() + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2)) ¿Fácil, no? Y también muy poco estético, pero ya vamos a tener tiempo de corregirlo. Presten atención a la escala, aparecen todos los puntos en negro, excepto uno que aparece en azul claro, indicador de valor muy alto. Evidentemente existe algún valor extremo, y esto suele ser algo normal. Como nuestro objetivo es simplemente hacer gráficos, vamos a eliminar estos datos, pero no deberían hacerlo con tanta facilidad en sus investigaciones: preciosCABA &lt;- preciosCABA %&gt;% filter(PrecioM2&lt;10000 &amp; PrecioM2&gt;500) ggplot() + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2)) Pareciera haber mejorado en algo, pero por ahora sigamos con nuestro ejercicio. Lo primero que querríamos hacer es eliminar ese fondo y, quizás, las cuadrículas y también la latitud y longitud que nos señalan los ejes. Lo primero lo podemos hacer usando algunos de los temas que vienen con ggplot, por ejemplo theme_minimal() ggplot() + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2)) + theme_minimal() Para eliminar las lineas de fondo y las referencias a la longitud y latitud solo debemos agregar coord_sf(datum = NA): ggplot() + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2)) + theme_minimal() + coord_sf(datum = NA) Mucho mejor. Sin embargo, si queremos mostrar el precio de las propiedades en el mapa según su valor debemos poner un contexto y también mejorar la escala. El Contexto serán los datos de polígonos que tenemos en el otro dataset, mientras que la escala intentaremos solucionarla usando la famosa paleta de viridis: ggplot() + geom_sf(data=preciosCABABarrios, fill=NA) + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2)) + scale_color_viridis_c() + theme_minimal() + coord_sf(datum = NA) Mucho mejor. Ahora bien, son muchos datos, qué tal si mostramos los distintos valores por año? Podemos hacer esto mediante la función facet_wrap(): ggplot() + geom_sf(data=preciosCABABarrios, fill=NA) + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2), size=0.5) + facet_wrap(facets = ~ Year) + scale_color_viridis_c() + theme_minimal() + coord_sf(datum = NA) Por lo visto, los valores de los precios se encuentran sesgados hacia la derecha (valores altos), lo que no nos permite visualizar correctamente los valores en los mapas. Creemos una variable categórica de los precios y veamos si mejora. preciosCABA&lt;-preciosCABA %&gt;% ungroup %&gt;% mutate(PrecioM2Cat = cut(PrecioM2,quantile(PrecioM2), include.lowest = TRUE)) ggplot() + geom_sf(data=preciosCABABarrios, fill=NA) + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2Cat,fill=PrecioM2Cat), size=0.5) + facet_wrap(facets = ~ Year) + scale_color_viridis_d() + scale_fill_viridis_d() + theme_minimal() + coord_sf(datum = NA) + labs(fill=&quot;Precios&quot;,color=&quot;Precios&quot;) En la primera parte creamos la categoría de la variable con la función cut(), que recibe 1) un conjunto de valores, 2) un vector con puntos de quiebre y nos devuelve un factor que dice en cada cual de esos segmentos caen los valores. Aunque lo hace muy bien, casi nunca nos gusta los labels que asigna, es decir las etiquetas que corresponden a cada nivel. Existen muchas formas de cambiar esto, una deellas es usando la función level(). Lo único que hay que hacer es pasarle los nuevos valores que deben reemplazar a los viejos. Veamos, primero, que devuelve la función: levels(preciosCABA$PrecioM2Cat) ## [1] &quot;[503,1.51e+03]&quot; &quot;(1.51e+03,2e+03]&quot; &quot;(2e+03,2.55e+03]&quot; &quot;(2.55e+03,9.71e+03]&quot; Ahora ya podemos reemplazar estos valores por algunos que nos parezcan más razonables. Veamos los valores de quiebre y creemos una mejor leyenda: round(quantile(preciosCABA$PrecioM2),0) ## 0% 25% 50% 75% 100% ## 503 1513 2000 2550 9706 nuevosLabels &lt;- c(&#39;De 503 a 1500&#39;, &#39;De 1501 a 1987&#39;, &#39;De 1988 a 2500&#39;,&#39;Más de 2500&#39;) levels(preciosCABA$PrecioM2Cat) &lt;- nuevosLabels Además de esto, vamos a ponerle un título y un subtítulo para agregar más información a nuestro mapa: ggplot() + geom_sf(data=preciosCABABarrios, fill=NA) + geom_sf(data = preciosCABA, mapping = aes(color=PrecioM2Cat,fill=PrecioM2Cat), size=0.5) + facet_wrap(facets = ~ Year) + scale_color_viridis_d() + scale_fill_viridis_d() + theme_minimal() + coord_sf(datum = NA) + labs(fill=&quot;Precios&quot;,color=&quot;Precios&quot;,title=&quot;Evolución de precios de los inmuebles en CABA&quot;,subtitle=&#39;2015-2019&#39;) 3.6.1 Mapas animados A veces, mostrar muchos mapas estáticos no comunica tan bien como hacerlo de una manera animada. El paquete gganimate nos permite usar prácticamente la misma sintáxis de ggplot para hacer animados a nuestros mapas. Hagamos uno que muestre los precios de los inmuebles por año para cada barrio de la Ciudad de Buenos Aires: library(gganimate) preciosCABABarrios&lt;-preciosCABABarrios %&gt;% mutate(PrecioM2Cat = cut(PrecioM2,quantile(PrecioM2), include.lowest = TRUE)) levels(preciosCABABarrios$PrecioM2Cat) &lt;- c(&#39;De 761 a 1661&#39;, &#39;De 1662 a 1973&#39;, &#39;De 1974 a 2319&#39;, &#39;Más de 2320&#39;) ggplot(data = preciosCABABarrios, mapping = aes(fill=PrecioM2Cat)) + geom_sf(color=&#39;white&#39;) + guides(size=FALSE) + theme_minimal() + coord_sf(datum = NA) + scale_fill_viridis_d() + labs(title = &quot;Mapa de precios de inmuebles en CABA&quot;, y = &quot;&quot;, subtitle=&quot;Año: {current_frame}&quot;, fill=&quot;Categoría de precios&quot;, caption=&quot;Fuente: Properati Data&quot;) + transition_manual(Year) ## nframes and fps adjusted to match transition ## Rendering [============&gt;------------------------------------------------------] at 4.4 fps ~ eta: 1s Rendering [==========================&gt;----------------------------------------] at 3.9 fps ~ eta: 1s Rendering [=======================================&gt;---------------------------] at 3.8 fps ~ eta: 1s Rendering [=====================================================&gt;-------------] at 3.6 fps ~ eta: 0s Rendering [===================================================================] at 3.6 fps ~ eta: 0s ## Frame 1 (20%) Frame 2 (40%) Frame 3 (60%) Frame 4 (80%) Frame 5 (100%) ## Finalizing encoding... done! 3.7 Ejercicios Descarguen el dataset de precios de los inmuebles que pueden obtener desde el siguiente link, que lo pasaremos a formato largo y eliminaremos el sufijo USDm2_ para los precios de cada año. precioAvisos &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/precioBarrios.csv&#39;, sep=&#39;;&#39;, stringsAsFactors = FALSE) precioAvisos&lt;-precioAvisos %&gt;% pivot_longer(cols = -BARRIOS, names_to= &quot;Year&quot;, values_to=&#39;Valor&#39; ) %&gt;% mutate(Year=gsub(pattern = &quot;USDm2_*&quot;, replacement = &quot;&quot;, x = Year)) Ahora hagan los siguientes gráficos. En cada uno de ellos prueben con distintos colores y themes. Prueben instalar el paquete ggthemes y usar algunos de sus themes, como por ejemplo theme_fiverthirtyeight(). Un gráfico de linea de tiempo que muestre la evolución de los precios entre 2013 y 2017 para los barrios de Agronomia, Almagro, Caballito y Coghlan ¿Pueden hacer este gráfico sin usar el argumento group? Revisen las variables de preciosAvisos y piensen la respuesta y las posibles soluciones. Un scatter plot para el precio (eje vertical) por año (eje horizontal) ¿Qué pasa si en el punto 2 usan geom_boxplot en lugar de geom_point? Recuerden que para hacer un boxplot es preciso tener más de un punto por grupo que se muestra, ya que resume la distribución de un conjunto de puntos. 3.8 Extensión: animando el gráfico de Hans Rosling El gráfico original de Hans Rosling muestra la evolución en el tiempo de las dos variables a lo largo del tiempo. Con el paquete gganimate solo debemos agregar dos líneas de código - y modificar levemente otras dos - para tener el gráfico que buscamos. En primer lugar, si queremos mostrar la evolución en el tiempo de estas variables tenemos que usar el data frame completo, no solo la última foto. En segundo lugar, debemos indicar cuáles son los quiebres en nuestro eje horizontal, porque en caso de no hacerlo R va a expresar algunos cortes con notación científica. Esto lo hacemos dentro de scale_x_continuous, donde podemos incluir, dentro de breaks, los valores que queremos que muestre. En tercer lugar, tenemos que indicarle a ggplot que se trata de una animación. Esto lo hacemos con transition_time(year), en donde le indicamos cual es la variable por la cual tiene que hacer la transición de las fotos. Además le decimos que, cómo hay baches entre los años, los complete con una interpolación lineal con el siguiente código ease_aes(‘linear’) Finalmente modificamos el título del gráfico con Año: {frame_time} para que nos vaya indicando el año que estamos viendo en la animación. library(gganimate) gapminderAnim &lt;- ggplot(data = gapminder_df, mapping = aes(x = gdpPercap, y = lifeExp, color=continent, size=pop)) + geom_point() + guides(size=FALSE) + theme_minimal() + scale_x_continuous(trans = &#39;log10&#39;,breaks = c(1000,10000,70000)) + scale_color_manual(breaks=c(&quot;Europe&quot;,&quot;Asia&quot;,&quot;Oceania&quot;,&quot;Africa&quot;,&quot;Americas&quot;), values = c(&quot;#E41A1C&quot;,&quot;#377EB8&quot;,&quot;#4DAF4A&quot; ,&quot;#984EA3&quot;,&quot;#FF7F00&quot;)) + labs(x = &quot;PIB per cápita&quot;, y = &quot;Expectativa de vida al nacer (en años)&quot;, title=&quot;Año: {frame_time}&quot;, subtitle=&quot;Expectativa de vida al nacer según nivel de ingreso&quot;, color=&quot;Continente&quot;, caption=&quot;Fuente: Gapminder&quot;) + transition_time(year) + ease_aes(&#39;linear&#39;) Si lo ejecutan, deberían tener algo similar a lo que se ve abajo: Aunque para generar esta animación use anim_save(), el equivalente a ggsave(), pero para animaciones: anim_save(filename=&quot;animacion.gif&quot;, animation = gapminderAnim, width=2100, height=1500, res=300) 3.9 Material de lectura ggplot2: Elegant Graphics for Data Analysis, Hadley Wickham "],
["datos-espaciales-en-r.html", "4 Datos espaciales en R 4.1 ¿Qué es un dato espacial? 4.2 ¿Dónde estamos en la Tierra? 4.3 Coordinate Reference Systems 4.4 Un ejemplo: datos públicos de GCBA y Properati 4.5 Otras operaciones espaciales 4.6 Incorporando información a nuestro dataset: los subtes 4.7 Ejercicio", " 4 Datos espaciales en R Al terminar este capítulo ustedes van a poder: - Comprender por qué los datos espaciales son distintos al resto de los datos - Las dificultades de la representación de esos datos y los estándares utilizados - Trabajar con datos espaciales en R: su importación, manipulación e introducción a los gráficos - Identificar los principales tipos de archivos donde suelen compartirse estos datos 4.1 ¿Qué es un dato espacial? Un dato espacial o georreferenciado tiene una característica que lo hace único: posee información sobre su ubicación en la Tierra. No es el único tipo de dato que tiene particularidades, por ejemplo las series de tiempo tienen información sobre un específico período de tiempo donde se registró la información. Esto trae importantes consideraciones al momento de realizar el análisis estadístico, lo que generó el desarrollo de toda una rama de la estadística. No obstante, los datos espaciales no presentan un desafío solo al momento de su análisis, sino que presentan específicidades en la forma de representar su información geográfica y realizar transformaciones en los datos. 4.2 ¿Dónde estamos en la Tierra? La respuesta a esta pregunta puede ser un poco más compleja de lo que uno piensa, al menos si desea realizar un análisis con esta información. La respuesta más fácil en este momento sería decir: Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina. Bien, es un primer paso. Ahora: ¿Cómo calculamos la distancia con respecto a Abbey Road 2, Londres, Inglaterra, donde se encuentra el famoso cruce peatonal de la tapa del disco de los Beatles, Abbey Road? Imposible saberlo solo con esa información. Si nosotros introdujeramos esos datos en un GPS (o Google Maps), lo que haría es traducir las direcciones que les pasamos a un sistema de grillas que divide al globo en celdas en base a líneas imaginarias en sentido paralelo a los polos (paralelos) y perpendicular a ellos (meridianos). Nuestra dirección quedaría transformada directamente en un vector con dos posiciones: latitud y longitud. Ahora “Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina” se convirtió en (-34.714656,-58.785999) y “Abbey Road 2, Londres, Inglaterra” en (51.532068, -0.177305). Las latitudes y longitudes se expresan en grados, así que ya podemos establecer una diferencia cuantitativa entre nuestras dos posiciones ¡Incluso podemos expresarlo en una medida de distancia como metros o kilómetros! Para esta clase vamos a necesitar varios paquetes, así que los cargamos. Recordemos que si no están instalados hay que usar la función install.packages() library(tidyverse) library(sf) # Paquete clave para manipular datos espaciales library(tmap) # Uno de los paquetes para hacer mapas Una vez que los cargamos, vamos a crear nuestro dataframe con datos espaciales en base a las coordenadas latitud y longitud que definimos anteriormente: # Creamos un Data Frame con los datos necesarios datos &lt;- data.frame(lat = c(-34.714656, 51.532068), long = c(-58.785999, -0.177305), ubicacion = c(&quot;UTDT&quot;, &quot;Abbey Road&quot;)) # Lo convertimos a un objeto sf puntosEspaciales &lt;- st_as_sf(datos, coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) st_distance(puntosEspaciales) # En metros ## Units: [m] ## [,1] [,2] ## [1,] 0 11131513 ## [2,] 11131513 0 st_distance(puntosEspaciales)/1000 # En kilómetros ## Units: [m] ## [,1] [,2] ## [1,] 0.00 11131.51 ## [2,] 11131.51 0.00 Según estos cálculos, nos separan aproximadamente 11.131 kms de Abbey Road. Perfecto, pudimos definir nuestra ubicación en la tierra e incluso medir la distancia con otro punto. Hay un parámetro que todavía no introdujimos y que resulta clave cuando lidiamos con datos espaciales: CRS, las siglas de Coordinate Reference System. En la próxima sección vamos a explicar qué son y para que sirven. 4.3 Coordinate Reference Systems 4.3.1 Elipsoides, sistemas de coordenadas y datums Representar una ubicación en la superficie de la tierra implica superar diversos obstáculos. Para empezar, la tierra no es una esfera: tiene una forma que suele modelarse como geoide, pero incluso eso es una aproximación. La tierra tiene una forma particular, con diversos accidentes geográficos que la hacen única (y difícil de manipular matemáticamente). Sin embargo, nosotros - y a fines prácticos, todas las personas que trabajan con datos georreferenciados - trabajamos en su versión como geoide, y es en relación a esta modelización de la tierra que se montan los CRS. Definido el geoide, ese modelo de la forma de la tierra, introducimos el primer componente de los CRS: el elipsoide. El elipsoide es una aproximación al geoide con mejores propiedades matemáticas. Para definir un elipsoide necesitamos un par de parámetros que definen su forma. Una vez que contamos con un elipsoide podemos establecer un sistema de grillas tridimensional, como el de latitud y longitud, lo segmenta según los ángulos que se forman entre la línea imaginaria paralela a los polos (paralelos) y la línea imaginaria perpendicular a los polos (meridiano) en un determinado punto, en relación al paralelo y meridiano de origen. Pero ¿cómo relacionamos al elipsoide con el geoide? Si bien el primero es una aproximación del segundo, para establecer un CRS necesitamos saber como se relacionan entre ellos: tenemos que “fijar” el elipsoide al geoide. Esto es lo que hace el datum: define el origen y la orientación de los ejes de coordenadas. Piensen en el datum como la información necesaria para “dibujar” el sistema de coordenadas en el elipsoide Entonces ya tenemos tres elementos que poseen los CRS: Un elipsoide (un modelo de la tierra, en rigor de un geoide) Un sistema de coordenadas, que nos permite determinar la posición de un punto en relación a otro en base a líneas imaginarias Un datum, que nos permite dibujar ese sistema de coordenadas en el elipsoide de tal manera que represente al ubicaciones específicas en el geoide Si no quedó del todo claro no se preocupen: es un tema complejo que, en la mayoría de los casos, solo basta con saber que estos conceptos existen y qué significan. El objetivo de esta subsección es dar la definición básica de cada elemento porque probablemente se encuentren con esta información en diversos lugares, pero a fines prácticos suele utilizarse siempre el mismo elipsoide, datum y sistema de coordenadas, o variaciones que no tienen grandes efectos a los fines prácticos de nuestros trabajos. El World Geodetic System (WGS84) es un standard en la industria a nivel mundial, y existen algunas variaciones locales (la más famosa, el North American Datum (NAD83)) que no nos traerán mayores problemas al momento de las transformaciones. Piensen en el CRS como las unidades de peso o de distancia: cada observación que veamos de datos espaciales corresponde a un determinado CRS y no corresponde hacer operaciones entre dos observaciones pertenecientes a distintos CRS. 4.3.2 Proyecciones Hasta ahora hemos trabajado en la representación de la Tierra en tres dimensiones. Sin embargo, todos los mapas con los que hemos trabajado desde chicos tienen dos dimensiones ¿Cómo transformamos un objeto de tres dimensiones a uno de dos dimensiones? Debemos realizar proyecciones de ese objeto tridimensional que, como veremos en breve, involucra diversos tradeoffs3. Piensen en la proyección como una tarea de traducción: algo se pierde en el proceso. La proyección hoy en día más famosa es MERCATOR, la proyección que usa, entre otros servicios, Google Maps. Diseñada hace ya varios siglos para la navegación marítima, esta transformación es relativamente buena en lo relativo preservar formas y útil para navegar. En lo que realmente falla este tipo de proyección es en definir el tamaño de las unidades geógraficas: los países que están cerca de los polos aparentan tener un tamaño mucho más grande del que realmente tienen, mientras que lo inverso sucede con los que están cerca de la línea del ecuador. Tal es así que existe una página web (https://thetruesize.com/) que permite experimentar de manera interactiva con los tamaños de los países en diversas partes de la proyección. En la Figura 1 muestro un ejemplo: Groenlandia, Islandia, Noruega, Suecia, Finlandia y Reino Unido combinadas ocupan aproximadamente el 50% de Brasil (Figura 1). Figure 4.1: La proyección MERCATOR distorsiona nuestra percepción de los tamaños La oferta de proyecciones es prácticamente ilimitada. El paquete mapproj en R nos permite transformar el mundo en base a diversas proyecciones, incluyendo algunas que preservan el tamaño de los países. La Figura 2 muestra el mundo desde otra perspectiva: los países del norte son más chicos de lo que parecen en la proyección mercator. Figure 4.2: La proyección MOLLWEIDE mantiene la representación de los tamaños Las proyecciones también forman parte de los CRS, que pueden o no tener una proyección. Sea como sea, lo importante de esta sección es haberlos convencido de que importa conocer en que CRS están expresados los datos espaciales. Las transformaciones entre CRS no hace falta conocerlas, sino que el paquete sf lo hará por nosotros. Insisto: lo importante es saber que los datos espaciales SIEMPRE tienen un CRS, aun si no está definido explícitamente en nuestro archivo. Volvamos al ejemplo de los inmuebles de las propiedades de la introducción de este libro para ver un qué formato de archivos tienen los datos espaciales y un ejemplo sobre transformación de CRS. 4.4 Un ejemplo: datos públicos de GCBA y Properati 4.4.1 CABA El Gobierno de la Ciudad de Buenos Aires (GCBA) brinda acceso a muchos recursos digitales muy interesantes en su repositorio https://data.buenosaires.gob.ar. De hecho, allí están disponibles los shapefiles donde se encuentran los polígonos que definen a los barrios y a las comunas que utilizamos en las anteriores clases. Todavía hay una parte que no había introducido sobre las etapas de preprocesamiento de los datos: ¿cómo trajimos los dos datasets y conseguimos agrupar a los inmuebles por barrio? El primer paso es descargar los datos (haciendo click aquí)[http://cdn.buenosaires.gob.ar/datosabiertos/datasets/barrios/barrios-zip.zip] Deberían descargar un .zip que deberían extraer en la misma carpeta donde esta su proyecto de R. Ahora deberían tener cinco archivos nuevos, con los nombres barrios_badata y siete terminaciones diferentes. Esta es la presentación de los shapefiles, un tipo de archivo creado para almacenar datos espaciales, diseñado por la empresa ESRI. En rigor, trabajar con shapefiles tiene ciertos problemas, uno de ellos es el formato multiarchivo que tiene. En este caso, el shapefile se encuentra compuesto por cinco archivos, pero pueden ser más y nunca menos que tres. Sea como sea, este tipo de formato se encuentra ampliamente difundido y mejor amigarnos con el. Veamos cómo leer estos datos espaciales en R. Para esto, vamos a usar la función st_read del paquete sf. ## Reading layer `barrios_badata&#39; from data source `D:\\OneDrive\\Documents\\UTDT\\Cursos\\CienciaDeDatosParaCuriosos\\Figuras\\Capitulo 4 - Datos Espaciales\\barrios_badata&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 48 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 93743.42 ymin: 91566.42 xmax: 111751.4 ymax: 111401.7 ## proj4string: +proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ## Coordinate Reference System: ## No user input ## wkt: ## PROJCS[&quot;Argentina_GKBsAs&quot;, ## GEOGCS[&quot;GCS_Campo_Inchauspe&quot;, ## DATUM[&quot;Campo_Inchauspe&quot;, ## SPHEROID[&quot;International_1924&quot;,6378388.0,297.0]], ## PRIMEM[&quot;Greenwich&quot;,0.0], ## UNIT[&quot;Degree&quot;,0.0174532925199433]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;False_Easting&quot;,100000.0], ## PARAMETER[&quot;False_Northing&quot;,100000.0], ## PARAMETER[&quot;Central_Meridian&quot;,-58.4627], ## PARAMETER[&quot;Scale_Factor&quot;,0.999998], ## PARAMETER[&quot;Latitude_Of_Origin&quot;,-34.6297166], ## UNIT[&quot;Meter&quot;,1.0]] barrios &lt;- st_read(&quot;barrios_badata&quot;) st_crs(barrios) La función st_crs() nos devuelve información sobre el sistema de referencia de coordenadas que tiene nuestro objeto. Todos los datos espaciales tienen algún tipo de sistema de referencia de coordenadas, solo que a veces no contamos información sobre cuál es. Uno de los formatos que existen para almacenar información sobre el CRS de los datos se conoce como proj4string, que lo podemos ver en la última linea de lo que nos devolvió la función st_crs(). Se trata de un conjunto de parámetros que definen a un CRS, siempre antecedidos por un +. Por ejemplo, podemos ver que los datos tienen una proyeccion MERCATOR (+proj=tmerc), con latitud de origen -34.6297166 y longitud de origen -58.4627. La unidad de la proyección está en metros (quiere decir que al movernos en el eje cartesiano xy que genera la proyección cada unidad es un metro). En el caso de los shapefiles, la información sobre el CRS pueden consultarla en el archivo .prj, uno de los cinco que conforman al shapefile en este caso. 4.4.2 Properati Dejemos por un momento el dataset del GCBA y enfoquémonos en los datos sobre precios de las propiedades que conseguimos desde Properati. Debido a que el dataset que les presenté la primera vez contaba con aproximadamente 500.000 observaciones, y eso va a llevarnos mucho tiempo de procesamiento en la clase, vamos a trabajar sólo con una muestra de las observaciones iniciales. Para esto, vamos a descargar directamente las propiedades listadas durante los últimos seis meses desde febrero de 2018. Para esto, tienen que ejecutar el siguiente código: datosPrecios &lt;- read.csv(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/DatosAdicionales/datosPrecios.csv&quot;) Deberían tener un objeto que se llame datosPrecios con las variables lat-lon, lat, lon y price_usd_per_m2 Ahora que tenemos nuestro dataset de precios, vamos a ver que, aunque tiene información sobre la latitud y longitud, R no lo reconoce como un dataset de datos espaciales. Para eso tenemos que convertirlo a un objeto sf. Pero antes de hacer esto, vamos a lidiar con los datos faltantes. Los datos faltantes son la norma en la mayoría de los datasets, y pueden generarnos más de un problema, especialmente si la razón por la que no aparecen en nuestros datos no es aleatoria. Sea como sea, en R los datos faltantes se registran como NA (Not Available) y es importante saber como identificarlos. Para eso, debemos utilizar la función is.na() que nos devuelve TRUE cuando el elemento del vector es NA y FALSE caso contrario. precioNA &lt;- is.na(datosPrecios$price_usd_per_m2) str(precioNA) ## logi [1:106854] FALSE TRUE TRUE FALSE FALSE FALSE ... Como vemos, la función devuelve un vector lógico con valores FALSE y TRUE, correspondiente a cuando hay datos disponibles en price_usd_per_m2 o el dato está faltante, respectivamente. Podemos sumar la cantidad de datos faltantes mediante la función sum() sum(precioNA) ## [1] 37058 sum(precioNA)/nrow(datosPrecios) * 100 # Calculamos como proporción de los datos totales ## [1] 34.68097 Aproximadamente un tercio del dataset no tiene datos sobre la variable que necesitamos. Vamos a eliminar estos datos ya que no nos sirven para nuestro análisis y nos ocupa espacio en la memoria de nuestra computadora: datosPrecios &lt;- datosPrecios %&gt;% filter(!is.na(price_usd_per_m2)) ¿Hay datos faltantes en otras variables de nuestro dataset? Podemos usar la función anyNA, que nos devuelve TRUE o FALSE dependiendo si algún elemento de un vector es un dato faltante o no. anyNA(datosPrecios$price_usd_per_m2) # Devuelve FALSE, tal como esperábamos. ## [1] FALSE anyNA(datosPrecios$lat.lon) # Pero todavía tenemos datos faltantes en esta variable: ## [1] TRUE latlonNA &lt;- is.na(datosPrecios$lat.lon) sum(latlonNA) ## [1] 7725 sum(latlonNA)/nrow(datosPrecios) * 100 # Calculamos como proporción de los datos totales ## [1] 11.06797 datosPrecios &lt;- datosPrecios %&gt;% filter(!is.na(datosPrecios$lat - lon)) anyNA(datosPrecios) # No tenemos ningun dato faltante en nuestro dataset ## [1] FALSE Bien, nuestro dataset posee finalmente 62.071 observaciones que podemos usar, aproximadamente un 58% de las observaciones originales. Si bien no tenemos datos sobre el CRS al que se asocian las coordenadas lat-lon del dataset que acabamos de procesar, lo cierto es que uno asume que corresponde al EPSG 4326, es decir al CRS que utiliza el elipsoide WSG84 y no se encuentra proyectado en dos dimensiones. EPSG no es otra cosa que un listado de códigos con CRS clasificados, lo que hace más simple consultar a qué CRS corresponde un determinado número (mucho más simple que la defición de proj4string). Como sf realiza bien su trabajo, nos deja crear un objeto sf con los datos que tiene nuestro data frame, y cuando consultamos la información sobre el CRS nos devuelve tanto su código EPSG como su definición proj4string. Pero antes de convertirlo, vamos a asegurarnos de que las variables lat y lon sean númericas. datosPrecios$lat &lt;- as.numeric(datosPrecios$lat) datosPrecios$lon &lt;- as.numeric(datosPrecios$lon) datosPrecios &lt;- st_as_sf(datosPrecios, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) st_crs(datosPrecios) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCS[&quot;WGS 84&quot;, ## DATUM[&quot;WGS_1984&quot;, ## SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, ## AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] 4.4.3 Asignando los inmuebles a los barrios Tenemos dos datasets espaciales: barrios, donde tenemos polygonos con los límites de cada barrio, y datosPrecios, donde tenemos puntos de los inmuebles. Para eso, debemos usar nuestra primera operación espacial: la función st_join, que nos devuelve un nuevo objeto sf con los atributos que corresponden a la intersección espacial entre los puntos y los polígonos. Piensen en esta función como el join que vimos la clase pasada. datosPreciosBarrios &lt;- st_join(datosPrecios, barrios) ## Error in st_geos_binop(&quot;intersects&quot;, x, y, sparse = sparse, prepared = prepared): st_crs(x) == st_crs(y) is not TRUE Tenemos un error! ¿Qué significa st_crs(x) == st_crs(y) is not TRUE? Nos dice algo muy importante: estamos intentando hacer una operación espacial con datos que están en distinta unidad, es decir en CRS distintos. Veamoslo: st_crs(datosPrecios) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCS[&quot;WGS 84&quot;, ## DATUM[&quot;WGS_1984&quot;, ## SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, ## AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] st_crs(barrios) ## Coordinate Reference System: ## No user input ## wkt: ## PROJCS[&quot;Argentina_GKBsAs&quot;, ## GEOGCS[&quot;GCS_Campo_Inchauspe&quot;, ## DATUM[&quot;Campo_Inchauspe&quot;, ## SPHEROID[&quot;International_1924&quot;,6378388.0,297.0]], ## PRIMEM[&quot;Greenwich&quot;,0.0], ## UNIT[&quot;Degree&quot;,0.0174532925199433]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;False_Easting&quot;,100000.0], ## PARAMETER[&quot;False_Northing&quot;,100000.0], ## PARAMETER[&quot;Central_Meridian&quot;,-58.4627], ## PARAMETER[&quot;Scale_Factor&quot;,0.999998], ## PARAMETER[&quot;Latitude_Of_Origin&quot;,-34.6297166], ## UNIT[&quot;Meter&quot;,1.0]] Efectivamente, los proj4string de ambos datos espaciales no coinciden. Por suerte, sf no nos va a dejar hacer operaciones espaciales como la de st_join si los CRS de ambos objetos no coinciden. Además, sf nos permite realizar la transformación de una forma muy simple con st_transform datosPrecios &lt;- datosPrecios %&gt;% st_transform(st_crs(barrios)) datosPreciosBarrios &lt;- st_join(x = datosPrecios, y = barrios) colnames(datosPreciosBarrios) ## [1] &quot;lat.lon&quot; &quot;price_usd_per_m2&quot; &quot;BARRIO&quot; &quot;COMUNA&quot; &quot;PERIMETRO&quot; ## [6] &quot;AREA&quot; &quot;OBJETO&quot; &quot;geometry&quot; Ya tenemos los datos de los barrios para cada uno de los anuncios de los inmuebles con los que estamos trabajando, solo usando la información sobre su ubicación en el espacio ¿A cuántos inmuebles pudo ubicar dentro de los barrios de la Ciudad de Buenos Aires? sum(!is.na(datosPreciosBarrios$BARRIO)) ## [1] 18610 18.610 observaciones de los datos totales (30%) corresponden a barrios de la CABA, según nuestra operacion espacial. Veamos si todo funcionó de acuerdo a lo que queríamos, para eso vamos a inspeccionar los puntos que fueron asignados algún barrio y los que no. Lo que hace un spatial join es agregar las columnas del segundo data frame al primero (entre otras en nuestro ejemplo aquella que contiene la información sobre los barrios). Lo que vamos a hacer es simplemente crear una variable que diga TRUE si hay algún dato en esa variable, es decir que el matcheo fue exitoso, y FALSE cuando no lo fue, usando la funcion ifelse(). Esta función se fija si algo se cumple (en este caso, si la variable BARRIO tiene un valor NA), y devuelve algo si se cumple y otra cosa si no se cumple: datosPreciosBarrios &lt;- datosPreciosBarrios %&gt;% mutate(CABA = ifelse(is.na(BARRIO), TRUE, FALSE)) # Chequeo para ver si lo hicimos bien table(datosPreciosBarrios$CABA) ## ## FALSE TRUE ## 18610 43461 Listo, ahora ya podemos hacer nuestros gráficos usando tmaps o ggplot. Recuerden usar library() o require() para poder trabajar con sus funciones. Además, vamos a usar RColorBrewer, una libreria para usar diferentes paletas de colores. Todo esto está explicado en el capítulo 3 de este libro. library(tmap) library(ggplot2) library(RColorBrewer) # Gráfico usando tmaps tm_shape(barrios) + tm_polygons(col = &quot;white&quot;, border.col = &quot;black&quot;) + tm_shape(datosPreciosBarrios) + tm_dots(col = &quot;CABA&quot;, palette = brewer.pal(n = 2, name = &quot;Set1&quot;)) # Gráfico usando ggplot ggplot() + geom_sf(data = barrios, fill = NA) + geom_sf(data = datosPreciosBarrios, mapping = aes(color = CABA), size = 0.1, show.legend = FALSE) + coord_sf(xlim = c(90000, 110000), ylim = c(92000, 113000), datum = NA) + scale_color_brewer(palette = &quot;Set1&quot;) + theme_minimal() Todo parece haber funcionado bien. Con el dataset datosPreciosBarrios (e incorporando datos sobre otros años) podemos ejecutar los códigos que vimos en la clase pasada y lograr al dataset que analizamos durante nuestra primera clase: ya están en condiciones de descargar datos espaciales de distintas fuentes y combinarlos ! 4.5 Otras operaciones espaciales st_join() o st_union() son solo algunas de las operaciones espaciales que podemos realizar sobre nuestros datos. Lo que hace es identificar aquellos datos espaciales que tocan a otros (en nuestro caso particular, puntos y polígonos), sin importar la forma en que lo hacen. Por ejemplo, el punto podría coincidir con el limite de un polígono, o podría estar dentro de él: st_join nos devuelve cualquiera de estos dos casos. Veamos todas las operaciones espaciales mediante el didáctico ejemplo del libro de Geocomputation with R # creamos un polígono que en realidad es un triángulo (la # primera y la cuarta coordenada coinciden) a = st_sfc(st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1))))) # Creamos una línea l = st_sfc(st_linestring(x = matrix(c(-1, -1, -0.5, 1), ncol = 2))) # Creamos puntos p = st_cast(st_sfc(st_multipoint(x = matrix(c(0.5, 1, -1, 0, 0, 1, 0.5, 1), ncol = 2))), &quot;POINT&quot;) ggplot() + geom_sf(data = a) + geom_sf(data = l) + geom_sf(data = p) + theme_minimal() + annotate(&quot;text&quot;, x = c(0.5, 1, -1, 0) + 0.05, y = c(0, 1, 0.5, 1), label = c(1:4)) ¿Cuáles puntos de nuestro objeto p se intersectan con nuestro polígono a? En el gráfico se ve fácilmente: los puntos 1 y 2. También sabemos cómo hacerlo en base a nuestro ejemplo anterior. st_intersects(p, a) ## Sparse geometry binary predicate list of length 4, where the predicate was `intersects&#39; ## 1: 1 ## 2: 1 ## 3: (empty) ## 4: (empty) # Si usamos SPARSE = FALSE nos devuelve algo mucho más # amigable st_intersects(p, a, sparse = FALSE) ## [,1] ## [1,] TRUE ## [2,] TRUE ## [3,] FALSE ## [4,] FALSE Supongamos ahora que queremos encontrar aquellos puntos que NO se intersectan con nuestro polígono. La función que deberíamos usar se llama st_disjoint st_disjoint(p, a, sparse = FALSE) ## [,1] ## [1,] FALSE ## [2,] FALSE ## [3,] TRUE ## [4,] TRUE Refinemos un poco nuestra búsqueda. Supongamos que queremos encontrar qué objetos espaciales se encuentran dentro de los límites de otro objeto. Pueden ser dos polígonos, un punto y un polígono o una línea y un polígono, por ejemplo. Sigamos con nuestro ejemplo ¿Cuáles puntos se encuentran dentro de nuestro polígono? st_within(p, a, sparse = FALSE) ## [,1] ## [1,] TRUE ## [2,] FALSE ## [3,] FALSE ## [4,] FALSE ¿Y cuáles son los puntos de que tocan los límites de otro polígono? st_touches(p, a, sparse = FALSE) ## [,1] ## [1,] FALSE ## [2,] TRUE ## [3,] FALSE ## [4,] FALSE ¿Cuáles son los puntos que están a una distancia menor a 0.5 de algún límite del polígono? ¿Y de 1? st_is_within_distance(p, a, dist = 0.5, sparse = FALSE) ## [,1] ## [1,] TRUE ## [2,] TRUE ## [3,] FALSE ## [4,] FALSE st_is_within_distance(p, a, dist = 1, sparse = FALSE) ## [,1] ## [1,] TRUE ## [2,] TRUE ## [3,] FALSE ## [4,] TRUE 4.6 Incorporando información a nuestro dataset: los subtes Agreguemos información a nuestros inmuebles a través de la relación espacial que tiene con otros objetos. Esta vez, descarguen dos shapefiles y extraigánlos y copiénlos a nuestro proyecto. Para que funcionen las dos primeras lineas del siguiente código, que cargan los archivos, hay que guardar ambos shapefiles dentro de una carpeta que se llame Subte. Una vez que hayamos hecho eso, este código debería leer ambos shapefiles a nuestra sesión de R: ## Reading layer `estaciones_de_subte&#39; from data source `D:\\OneDrive\\Documents\\UTDT\\Cursos\\CienciaDeDatosParaCuriosos\\Figuras\\Capitulo 4 - Datos Espaciales\\Subte&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 87 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 97881.68 ymin: 98442.25 xmax: 108564.5 ymax: 108167.8 ## proj4string: +proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ## Reading layer `red_de_subte&#39; from data source `D:\\OneDrive\\Documents\\UTDT\\Cursos\\CienciaDeDatosParaCuriosos\\Figuras\\Capitulo 4 - Datos Espaciales\\Subte&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 81 features and 2 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 97881.68 ymin: 98442.25 xmax: 108564.5 ymax: 108167.8 ## proj4string: +proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs estacionesSubte &lt;- st_read(dsn = &quot;Subte&quot;, layer = &quot;estaciones_de_subte&quot;) recorridoSubte &lt;- st_read(dsn = &quot;Subte&quot;, layer = &quot;red_de_subte&quot;) tm_shape(barrios) + tm_polygons(col = &quot;white&quot;) + tm_shape(estacionesSubte) + tm_bubbles(size = 0.2, col = &quot;white&quot;) + tm_shape(recorridoSubte) + tm_lines() Incoporemos a nuestro dataset una información que podría ser relevante: la distancia entre la casa y la estación más cercana de subte. Para esto, será clave la función st_distance. Veamos qué hace distancias &lt;- st_distance(datosPreciosBarrios, estacionesSubte) str(distancias) ## Units: [m] num [1:62071, 1:87] 5987 39789 44069 43435 39987 ... dim(distancias) ## [1] 62071 87 La función nos devuelve la distancia en una determinada unidad, en este caso en metros, entre los objetos que le pasamos. En esta ocasión es los anuncios de los inmuebles contra las estaciones de subte, que como podemos ver son 87. Nos interesa la distancia mínima, así que vamos a correr el siguiente código: datosPreciosBarrios &lt;- datosPreciosBarrios %&gt;% mutate(distanciaSubte = apply(distancias, 1, function(x) min(x))) Vamos a explicar paso por paso lo que hicimos. En primer lugar, aplicamos el verb mutate() que, como ya explicamos en el capítulo 2, crea una variable en nuestro dataset. En nuestro caso, la nueva variable se llama distanciaSubte. Luego aplicamos la función apply. Esta función es muy versátil y útil en R ya que nos permite crear nuestras propias funciones. Lo que hay que pasarle 1) es una matriz (como nuestro objeto distancias), 2) una orientación para aplicar nuestra función, 1 representa filas y 2 columnas, y 3) una función. En nuestro caso, la función es muy simple y lo único que hace es tomar el mínimo. Como escribimos 1, toma el mínimo de cada fila. En español le dijimos: “tomá la matriz distancias, y para cada fila elegí el mínimo y devolvelo” ¿Qué podemos hacer con esta información? Muchas cosas, como por ejemplo ver si el precio de los inmuebles de alguna manera se encuentra afectado por la cercanía al subte. Con todo, vamos a terminar esta sección con un gráfico indicando las lineas de subte y con color diferencial si los inmuebles se encuentran a 500 metros o menos de una estación de subte. datosPreciosBarrios &lt;- datosPreciosBarrios %&gt;% filter(!is.na(BARRIO)) %&gt;% mutate(cerca = ifelse(distanciaSubte&lt;=500,TRUE,FALSE)) ggplot() + geom_sf(data = barrios) + geom_sf(data = datosPreciosBarrios, size=0.5, aes(color=cerca)) 4.6.1 Una alternativa más simple: usando otro método de join espacial Anteriormente en este capítulo vimos cómo podemos establecer relaciones espaciales entre nuestros objetos con st_intersects() st_contains(), st_touches() o st_is_within_distance(). Utilicemos este conocimiento para modificar el st_intersects() que usa por definición st_join() para establecer las relaciones entre los polígonos/puntos de nuestros datos espaciales. # Eleminamos las columnas que creeamos antes datosPreciosBarrios &lt;- datosPreciosBarrios %&gt;% select(-distanciaSubte,-cerca) datosPreciosBarrios &lt;- st_join(datosPreciosBarrios,estacionesSubte, join=st_is_within_distance, dist=500) La lógica del st_join() es igual a la que usamos anteriormente para agregar los datos de los barrios a los anuncios de los inmuebles. La principal diferencia es que ahora en el parámetro join le decimos que, en lugar de st_intersects() use st_is_within_distance() para tener en cuenta si dos entidades espaciales están relacionados o no. el parámetro dist simplemente define cual es el umbral de distancia para determinar si está o no cerca. Recuerden que los CRS tienen asociados una unidad de medidas, en este caso es metros. Esperamos el mismo comportamiento que antes: a aquellos inmuebles que se los haya detectado a una distancia menor a 500 metros de alguna estación de subte se le va asignar las columnas correspondientes con el valor a esa estación de subte. Cuando no haya coincidencia con ninguna de las estaciones, esos datos van a a aparecer como NAs datosPreciosBarrios &lt;- datosPreciosBarrios %&gt;% mutate(CERCA=ifelse(is.na(ESTACION),FALSE,TRUE)) ggplot() + geom_sf(data = barrios) + geom_sf(data = datosPreciosBarrios, size=0.5, aes(color=CERCA)) El gráfico es el mismo que en la subsección anterior. 4.7 Ejercicio Hemos leído datos en formato shapefile, una forma de guardar y compartir datos espaciales ha sido explicado en detalle anteriormente. Adicionalmente, hemos cargado datos desde archivos separados con coma (csv) que tenían latitud y longitud en dos columnas. Otro de los archivos que continenen datos espaciales más difundidos es el geojson, la versión espacial (geo) de los json. Se trata de un conjunto de listas, pero lo importante es que en los últimos años se ha tomado como estandard que el CRS sea el WGS84 (epsg:4326). De esta manera, todo lo que carguemos desde estos archivos estará en ese sistema de coordenadas, muy útil para evitar confusiones. sf permite levantar esta clase de archivos, al igual que el resto que vimos. Ejecuten el siguiente código para levantar los datos de los precios de los inmuebles para la Ciudad de Buenos Aires y los polígonos de los barrios avisosCABA &lt;- read_sf(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/datosPreciosBarrios.geojson&quot;) barrios &lt;- read_sf(&#39;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/barriosCABA.geojson&#39;) ¿Cuál es el valor promedio por metro cuadrado de los inmuebles ofertados en la Ciudad de Buenos Aires? ¿Cuál es el valor promedio por metro cuadrado de los inmuebles ofertados en la Ciudad de Buenos Aires en cada uno de sus barrios? Hacer un mapa con el valor promedio del metro cuadrado por barrio de la Ciudad de Buenos Aires. Este video resume en unos pocos minutos gran parte de lo que vamos a ver acá y es recomendable↩ "],
["data-wrangling-de-datos-espaciales.html", "5 Data wrangling de datos espaciales 5.1 Introduccion 5.2 Areal weighted interpolation 5.3 Haciendo mapas de nuestros nuevos datos 5.4 Ejercicio", " 5 Data wrangling de datos espaciales Al terminar este capítulo ustedes van a poder: - Reutilizar información entre polígonos solapados que no coinciden en su totalidad - Georeferenciar direcciones - Estimar tiempos de transporte entre distintos puntos espaciales 5.1 Introduccion Los datos espaciales tienen muchas características particulares que hacen que su almacenamiento, transformación de datos, herramientas de análisis y de visualización varien con respecto a otros tipos de datos. En el capítulo de introducción a los datos espaciales vimos como leer distintos tipos de archivos, cómo “mezclar” información entre datos solo por la relación espacial entre las entidades (con st_join()) y también hemos realizado algunas visualizaciones con tmap() y ggplot() El objetivo de este capítulo es introducir a un conjunto de herramientas adicionales que pueden ser de gran importancia para trabajar con cualquier tipo de datos espaciales. En particular, una de ellas busca resolver el clásico problema de tener información sobre áreas que no están perfectamente superpuestas, mientras que otras dos introducen a la posibilidad de georreeferenciar direcciones y de estimar distancia de desplazamientos entre distintos puntos. Estas herramientas no son exhaustivas, pero sí suelen ser relevantes para poder hacer un mejor uso de los datos espaciales y poder responder preguntas relevantes. 5.2 Areal weighted interpolation El INDEC realiza censos con un intervalo de aproximadamente 10 años en Argentina. Las últimas dos ediciones al escribir estas lineas fueron en los años 2001 y 2010. Los censos poblacionales son una de las formas más clásicas de conseguir información homogenéa, de alcance nacional sobre las condiciones de vida de toda la población Argentina, tanto urbana y rural, con un nivel de granuralidad bastante aceptable. Todavía más, al hacerse cada cirto período de tiempo, es posible analizar variaciones entre décadas. La unidad espacial mínima de los censos de Argentina son los radios censales. Comparar la evolución de diversas variables entre censos a ese nivel de desagregación es muy útil para responder muchas interesantes. En este ejercicio vamos a poder analizar cómo cambió la edad promedio de los habitantes de estos radios censales entre 2001 y 2010 en la Ciudad de Buenos Aires. Para hacerlo, vamos a encontrarnos con un problema entre los polígonos de ambos censos. Miren la figura que aparece abajo ¿Son iguales? Figure 5.1: La proyección MERCATOR distorsiona nuestra percepción de los tamaños Claramente no: los radios censales cambiaron de forma entre 2001 y 2010. Esto es atendible, ya que estos radios suelen crearse respetando ciertos criterios metodológicos que no priorizan la comparación entre censos. Con todo, sería una pena no poder trabajar con estos datos: vamos a aprender una alternativa para resolver este problema, pero a un costo. Para hacer comparables los datos entre ambos polígonos, debemos hacer determinados supuestos de cómo se distribuyen los datos hacia adentro de ellos Uno de los supuestos más usuales es asumir que los datos se distribuyen de manera homogénea hacia dento de los polígonos. Esto hace muy fácil poder trabajar con los polígonos mediante lo que se conoce como Interpolación ponderada por peso, o areal weighted interpolation. 5.2.1 Carga de los datos Lo primero que habría que hacer es cargar los datos. Sin embargo, sabemos que debemos cargar las librerias en nuestra sesión de R para que muchas de las funciones que usamos se ejecuten. library(tidyverse) # Para manipular datos y graficarlos library(sf) # Para manipular datos espaciales library(tmap) # Para graficar datos espaciales Una vez que tengamos cargadas las librerías vamos a descargar los datos desde el repositorio. Podría haber enviado todo en un RData, pero quiero introducirlos a una nueva forma de guardar datos espaciales: geojson. La ventaja más obvia de este tipo de archivos espaciales es que consiste de un solo archivo que contiene todos los datos en listas. No vamos a preocuparnos estrictamente por su forma, pero lo que sí vamos a hacer es leer un geojson porque probablemente sea uno de los archivos con los que eventualmente se encuentren. datosCenso2001 &lt;- read_sf(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/AWI/radiosCensalesCABA2001.geojson&quot;) datosCenso2010 &lt;- read_sf(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/AWI/radiosCensalesCABA2010.geojson&quot;) Los geojson se leen con la misma función read_sf. Un punto importante a tener en cuenta cuando trabajamos con este formato de datos espaciales es que obliga a que el CRS sea el 4326, es decir el WSG84 sin proyectar. Aunque podríamos seguir trabajando con esta proyección, vamos a transformarlos a la proyección inicial en la cual vinieron. No conocemos el código EPSG, pero sí el proj4string correspondiente: +proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs. Con el siguiente código van a tener a los dos objetos en la proyección con la que arrancaron. datosCenso2001 &lt;- datosCenso2001 %&gt;% st_transform(&#39;+proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs&quot;&#39;) datosCenso2010 &lt;- datosCenso2010 %&gt;% st_transform(&#39;+proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs&quot;&#39;) Si quieren investiguen que variables hay en cada uno de los datos censales con las funciones que ya vimos en clase, tales como str o colnames. Pueden probar una función adicional de tidyverse: glimpse() glimpse(datosCenso2001) ## Rows: 3,407 ## Columns: 23 ## $ COD_2001 &lt;chr&gt; &quot;020100402&quot;, &quot;020100305&quot;, &quot;020100301&quot;, &quot;020100303&quot;, &quot;020100307&quot;,... ## $ PROV_ &lt;chr&gt; &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;... ## $ DEPTO_ &lt;chr&gt; &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;... ## $ pop &lt;int&gt; 110, 1405, 1162, 762, 591, 738, 752, 481, 1129, 555, 1047, 803, ... ## $ pop014 &lt;int&gt; 14, 180, 177, 74, 109, 124, 133, 81, 147, 99, 155, 144, 150, 108... ## $ pop1564 &lt;int&gt; 96, 1049, 757, 528, 366, 532, 473, 297, 768, 336, 681, 503, 650,... ## $ pop64plus &lt;int&gt; 0, 176, 228, 160, 116, 82, 146, 103, 214, 120, 211, 156, 106, 11... ## $ highSchoolAboveALLPOP &lt;int&gt; 25, 851, 619, 396, 286, 329, 352, 256, 554, 307, 528, 300, 435, ... ## $ terciarioAboveALLPOP &lt;int&gt; 6, 600, 427, 217, 193, 221, 191, 148, 365, 211, 299, 165, 285, 1... ## $ universitaryAboveALLPOP &lt;int&gt; 2, 354, 248, 118, 123, 135, 81, 77, 190, 108, 150, 82, 159, 46, ... ## $ highSchoolAbove2564 &lt;int&gt; 10, 704, 477, 310, 204, 284, 274, 194, 451, 224, 419, 250, 383, ... ## $ terciarioAbove2564 &lt;int&gt; 3, 530, 357, 187, 142, 200, 165, 121, 325, 165, 259, 150, 262, 9... ## $ universitaryAbove2564 &lt;int&gt; 2, 324, 209, 110, 97, 126, 75, 64, 177, 95, 142, 77, 153, 46, 12... ## $ pop2564 &lt;int&gt; 60, 891, 593, 407, 277, 422, 384, 248, 642, 280, 586, 387, 536, ... ## $ avgAge &lt;dbl&gt; 28.72727, 36.58434, 40.85714, 43.49213, 41.24196, 36.57182, 40.0... ## $ descuentoJubil &lt;dbl&gt; 0.6333333, 0.8321256, 0.8084291, 0.7699387, 0.7460938, 0.8010899... ## $ descuentoJubilAsalariado &lt;dbl&gt; 0.8181818, 0.8921933, 0.8626198, 0.8403756, 0.7751938, 0.8604651... ## $ asalariado &lt;dbl&gt; 0.7333333, 0.6497585, 0.5996169, 0.6533742, 0.5039062, 0.7029973... ## $ cuentapropia &lt;dbl&gt; 0.2000000, 0.3393720, 0.3888889, 0.3404908, 0.4765625, 0.2833787... ## $ servDomestico &lt;dbl&gt; 0.1000000, 0.0205314, 0.0287356, 0.0276074, 0.0546875, 0.0299728... ## $ asalariadoPrivado &lt;dbl&gt; 0.1333333, 0.1086957, 0.1800766, 0.1932515, 0.1757812, 0.1771117... ## $ ocupadosUniversitariaComp &lt;dbl&gt; 0.0000000, 0.3683575, 0.3639847, 0.3036810, 0.3945312, 0.3024523... ## $ geometry &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((4192913 615..., MULTIPOLYGON (((419... 5.2.2 Diferentes radios censales Como les comenté anteriormente, los radios censales para la Ciudad de Buenos Aires sufrieron importantes cambios en su forma. Pero no hace falta que me crean a mí o a la imagen que puse anteriormente: con R pueden verlo por ustedes mismos. De paso, vamos a introducir una función nueva de sf y algo nuevo sobre cómo hacer múltiples mapas. Vamos a observar cómo son los radios censales en Palermo para 2001 y para 2010. Para esto lo primero que debemos hacer es hacer “zoom” en Palermo para ambos censos y luego hacer un mapa que los muestre lado a lado. La función st_crop será muy útil para esto. Lo que hace recortar los polígonos en base a las coordenadas más “externas” de un determinado polígono. De esta manera, siempre queda un cuadrado o un rectángulo en general “centrado” con respecto a los datos espaciales que nosotros queremos. # Palermo tiene el código de departamento número 014 radiosCensalesPalermo2010 &lt;- datosCenso2010[datosCenso2010$DEPTO_ %in% &#39;014&#39;,] %&gt;% st_union() # st_union() junta a todos los polígonos en uno solo palermo2010 &lt;- st_crop(x = datosCenso2010, y = radiosCensalesPalermo2010) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries palermo2001 &lt;- st_crop(x = datosCenso2001, y = radiosCensalesPalermo2010) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries ¿Qué significa ese warning attribute variables are assumed to be spatially constant throughout all geometries? Lo que nos avisa es que st_crop cortó algunos polígonos y les asignó el valor de los atributos que tenían cuando los polígonos estaban completos ¿Esto es correcto? Depende de la variable con la que estamos trabajando. Imaginen que la variable nos describe el uso de la tierra. Si el polígono más grande era una zona industrial, un polígono recortado de él también tiene que ser una zona industrial, por lo cual no hay problema. Ahora imaginen que es el la cantidad de personas que viven allí. Es altamente problable que al recortar el polígono no sea correcto asignar la misma cantidad de personas. En este ejemplo particular esto nos tiene sin cuidado, ya que solo queremos quedarnos con la zona de Palermo y no usar sus variables. Ahora hagamos unos gráficos simples de Palermo en ambos censos con tmaps. Presten atención a la nueva función tm_sf(). Lo que hace es graficar la columna geometry que se encuentra en los datos que le pasamos con tm_shape. De esta manera, imita el tipo de datos que hay y no hace falta aclararle si son puntos o polígonos, por ejemplo. Luego usamos la función tm_layout que nos ayuda para modificar algunas cuestiones relacionadas con el diseño, como el título. Finalmente usamos tmap_arrange al que le podemos pasar gráficos y los acomoda en un mismo panel para mostrarlos de un solo golpe. mapa2001 &lt;- tm_shape(palermo2001) + tm_sf(border.col = &#39;black&#39;,col = &#39;grey90&#39;) + tm_layout(title=&#39;Radios censales 2001 - Palermo&#39;) mapa2010 &lt;- tm_shape(palermo2010) + tm_sf(border.col = &#39;black&#39;,col = &#39;grey90&#39;) + tm_layout(title=&#39;Radios censales 2010 - Palermo&#39;) tmap_arrange(mapa2001, mapa2010) Antes de analizar los gráficos, un par de puntos a tener en cuenta. Noten que guardé los mapas individualmente en objetos. Esto es realmente útil en algunas cirscunstancias, y no solo para luego ponerlos en un panel. Nos sirve, por ejemplo, para ir agregando capas de a poco a nuestro gráfico e ir guardando las versiones intermedias. En segundo lugar, es importante saber cómo exportar estos gráficos. Es realmente simple con tmap y también con ggplot. En este caso, al estar trabajando con tmap ejecuten lo siguiente: graficoSalida &lt;- tmap_arrange(mapa2001, mapa2010) tmap_save(graficoSalida,dpi = 300, filename = &#39;comparacionradioscensales.png&#39;) tmap_save solo requiere del objeto a exportar (en nuestro caso, graficoSalida) y la dirección y nombre donde se guardará (en este caso, lo guardamos en la misma carpeta del proyecto con el nombre ‘comparacionradioscensales.png’). DPI es un parámetro adicional que indica la cantidad de Dots Per Inch (DPI) que queremos que tenga nuestra imagen. 300 DPI es una cantidad elevada para gráficos en la computadora. Volvamos a los gráficos de los radios censales. Comparen distintos puntos de Palermo y verán que las diferencias son elevadas entre ambos censos. Aun si este cambio en los polígonos no es del todo reparable, podemos comparar la evolución de diferentes variables si estamos dispuestos a tolerar un error basado en la asunción de que las variables se distribuyen uniformemente en el espacio. 5.2.2.1 Breve desvío: haciendo lo mismo con ggplot() La librería ggplot2 es la más utilizada para hacer gráficos en R. Veamos cómo hacer lo mismo que hicimos en la subsección anterior, pero esta vez usando esta librería. La principal diferencia es que ggplot, para graficar más de un gráfico en un panel, nos pide que los datos estén todos juntos en un mismo data.frame. Esto es lo que se muestra en el capítulo de visualizaciones de este mismo libro. Sin embargo, no siempre esta es la mejor elección. Muchas veces queremos hacer gráficos con datos que están representados en distintos objetos y esto no debería ser un impedimento para poder armar nuestro panele de graficos. Por suerte, el paquete gridExtra tiene exactamente lo que necesitamos para solucionar este problema. No se olviden de instalar el paquete gridExtra para poder ejecutar lo siguiente: library(gridExtra) mapa2001 &lt;- ggplot(palermo2001) + geom_sf(col = &#39;black&#39;,fill = &#39;grey90&#39;) + labs(title=&#39;Radios censales 2001 - Palermo&#39;) + coord_sf(datum = NA) + theme_minimal() mapa2010 &lt;- ggplot(palermo2010) + geom_sf(col = &#39;black&#39;,fill = &#39;grey90&#39;) + coord_sf(datum = NA) + labs(title=&#39;Radios censales 2010 - Palermo&#39;) + theme_minimal() grid.arrange(mapa2001, mapa2010,ncol=2) Muy similar a lo que hicimos con tmap, no? El código incluso se parece bastente, con algunos cambios. coord_sf(datum = NA) lo usamos para que ggplot no nos muestre información sobre el sistemas de coordenadas de referencia (puede ser muy molesto para nuestro gráfico). themee_minimal() saca mucho de los elementos que los gráficos en ggplot tienen por default. Ahora sí, sigamos con lo importante. 5.2.3 Make polygons comparable again Estamos en condiciones de hacer lo que buscábamos: hacer comparables a los polígonos ¿Qué hace nuestro algoritmo de Areal Weighted Interpolation? Resumiendo y simplificando un poco, lo que hace es realmente simple: estima el porcentaje de un polígono que se solapa con otro y le asigna de manera proporcional el valor de las variables. Pero hay que tener en cuenta que las variables pueden ser conteos (como población) o porcentajes (como por ejemplo el porcentaje de población con estudios secundarios completos). En el primer caso, el algoritmo debe sumar las distintas partes que forman al polígono, mientras que en el segundo debe ponderar el porcentaje de acuerdo al solapamiento. En nuestros objetos tenemos ejemplos de ambas variables. Por ejemplo, en ambos casos tenemos el promedio de edad (avgAge), pero también la población de personas menores a 14 años (pop014). Veamos cómo podemos transformar los polígonos de 2001 a los del 2010 para poder observar la distribución de estas dos variables para los dos censos. El paquete que nos ayudará para lograr este objetivo es areal. Instalenlo si es la primera vez que están trabajando con este paquete. Ya deberían saber como hacerlo: install.packages(‘areal’). Una vez que lo tengan instalado, usen require o library para poder usar la función aw_interpolate. La función pide algunos parámetros. En primer lugar, source pide el objeto cuya información espacial queremos cambiar, en este caso los datos del censo 2001. Luego, nos pide el sid, que no es otra cosa que un conjunto de identificadores únicos para cada uno de los datos de nuestra source. Luego, en .data nos pide el target, es decir la forma que van a tomar nuestros datos espaciales, en nuestro ejemplo los radios censales de 2010, y también nos pide los ids (identificadores) correspondientes en tid. Luego, nos pide como quiere que ponderemos los nuevos polígonos con el parámetro weight, en nuestro caso queremos que sume (sum) proporcionalmente cada uno de los polígonos que contribuirán con el polígono de 2010. El parámetro output nos indica si queremos que nos devuelva un objeto sf con la columna de geometry incluida, o simplemente un data frame, elegimos ‘sf’. Finalmente, debemos indicar las variables intensivas o extensivas de los datos de origen. Las primeras hacen referencia a las variables que son porcentajes, mientras que las segundas hacen referencia a variables que son simplemente conteos. Nosotros tenemos una variable de cada una: pop014 es conteo mientras que avgAge es una variable extensiva. library(areal) nuevosDatos &lt;- aw_interpolate(source=datosCenso2001, sid = COD_2001, .data=datosCenso2010, tid = COD_2010, weight=&quot;sum&quot;, output = &quot;sf&quot;, extensive = &#39;pop014&#39;, intensive = &#39;avgAge&#39;) ## Error in aw_interpolate(source = datosCenso2001, sid = COD_2001, .data = datosCenso2010, : Data validation failed. Use ar_validate with verbose = TRUE to identify concerns. ¡No funcionó! Veamos por qué no funcionó usando la función ar_validate() que hace un chequeo de si la interpolación puede funcionar o no entre dos objetos ar_validate(source = datosCenso2001, target = datosCenso2010, varList = c(&#39;pop014&#39;,&#39;avgAge&#39;), verbose = TRUE) ## # A tibble: 6 x 2 ## test result ## &lt;chr&gt; &lt;lgl&gt; ## 1 sf Objects TRUE ## 2 CRS Match TRUE ## 3 CRS is Planar TRUE ## 4 Variables Exist in Source TRUE ## 5 No Variable Conflicts in Target FALSE ## 6 Overall Evaluation FALSE La función chequea 5 condiciones necesarias para que la interpolación funcione y nos indica si se cumple o no. En la primera se pregunta si ambos objetos son sf, la segunda si los sistema de coordenadas de referencia son iguales, la tercera si se encuentran proyectados, la cuarta nos indica si las variables que queremos convertir existen en el objeto de origen (source) y la quinta si no existen conflictos con el nombre de las variables en el objeto de destino. La sexta, overall evaluation, solo nos dice si todas se cumplen o no. En nuestro caso nos indica que hay algún conflicto en el objeto de destino. Lo que sucede es que las variables se llaman igual en el objeto de origen y destino, por lo que debemos cambiar los nombres de las columnas para que funcione aw_interpolate. Para esto, debemos usar la función colnames y paste0: colnames(datosCenso2001)[colnames(datosCenso2001) %in% c(&#39;pop014&#39;,&#39;avgAge&#39;)] &lt;- paste0(c(&#39;pop014&#39;,&#39;avgAge&#39;),&#39;_2001&#39;) colnames(datosCenso2001) ## [1] &quot;COD_2001&quot; &quot;PROV_&quot; &quot;DEPTO_&quot; ## [4] &quot;pop&quot; &quot;pop014_2001&quot; &quot;pop1564&quot; ## [7] &quot;pop64plus&quot; &quot;highSchoolAboveALLPOP&quot; &quot;terciarioAboveALLPOP&quot; ## [10] &quot;universitaryAboveALLPOP&quot; &quot;highSchoolAbove2564&quot; &quot;terciarioAbove2564&quot; ## [13] &quot;universitaryAbove2564&quot; &quot;pop2564&quot; &quot;avgAge_2001&quot; ## [16] &quot;descuentoJubil&quot; &quot;descuentoJubilAsalariado&quot; &quot;asalariado&quot; ## [19] &quot;cuentapropia&quot; &quot;servDomestico&quot; &quot;asalariadoPrivado&quot; ## [22] &quot;ocupadosUniversitariaComp&quot; &quot;geometry&quot; Ya sabemos que la función colnames() nos devuelve los nombres de las columnas de un data frame. Luego, elegimos aquellas columnas que tienen nombre de ‘pop014’ y ‘avgAge’. Finalmente, le asignamos lo que devuelve paste0(). Lo que hace es concatenar cadenas de texto, en este caso pop014 con _2001 y avgAge con _2001. Chequeen nuevamente los nombres de las columnas y verán que están cambiados. Ahora sí podemos correr nuestro código nuevosDatos &lt;- aw_interpolate(source=datosCenso2001, sid = COD_2001, .data=datosCenso2010, tid = COD_2010, weight=&quot;sum&quot;, output = &quot;sf&quot;, extensive = &#39;pop014_2001&#39;, intensive = &#39;avgAge_2001&#39;) colnames(nuevosDatos) ## [1] &quot;COD_2010&quot; &quot;PROV_&quot; &quot;DEPTO_&quot; ## [4] &quot;pop&quot; &quot;pop014&quot; &quot;pop1564&quot; ## [7] &quot;pop64plus&quot; &quot;highSchoolAboveALLPOP&quot; &quot;terciarioAboveALLPOP&quot; ## [10] &quot;universitaryAboveALLPOP&quot; &quot;highSchoolAbove2564&quot; &quot;terciarioAbove2564&quot; ## [13] &quot;universitaryAbove2564&quot; &quot;pop2564&quot; &quot;avgAge&quot; ## [16] &quot;ocupadosUniversitariaComp&quot; &quot;geometry&quot; &quot;pop014_2001&quot; ## [19] &quot;avgAge_2001&quot; 5.3 Haciendo mapas de nuestros nuevos datos Aprovechemos la nueva información que tenemos para poder hacer mapas. Usando ggplot2, grafiquemos la edad promedio en 2001, 2010 y la variación entre ambos censos. Lo primero que tenemos que hacer en ggplot, para que la leyenda represente correctamente los colores de nuestro mapa, es juntar todo en un mismo dataset que sea largo. Recuerden que esto es lo necesario para trabajar con paneles y facet_wrap(). Seleccionamos las dos y usamos pivot_longer() nuevosDatosLonger &lt;- nuevosDatos %&gt;% select(avgAge, avgAge_2001) %&gt;% pivot_longer(names_to = &quot;Year&quot;, values_to = &quot;AvgAge&quot;, cols = c(&quot;avgAge&quot;, &quot;avgAge_2001&quot;)) %&gt;% st_as_sf() ## Warning in val_cols[col_id] &lt;- unname(as.list(data[cols])): number of items to replace is not a ## multiple of replacement length select() y pivot_longer() no deberían traer demasiados problemas. Ahora bien, fijense que luego de hacer esto, usamos st_as_sf(), que como vimos en el capítulo de datos espaciales define a un data.frame como un objeto espacial sf. Esto no debería pasar, pero cuando usan pivot_longer, el data.frame deja de ser sf, pero como no pierde la columna de geometry, rápidamente podemos volver a convertirlo en un objeto sf. El mensaje que reciben es un error de cuando se usa pivot_longer. Queda resuelto cuando usamos st_as_sf() luego. Si quisieran, ya podrían hacer el gráfico, pero no sería el que ustedes quisieran guardar para compartir. Vamos a hacer algunos cambios, y de paso aprender un poco más sobre los factores en R. Para que facet_wrap() ponga en el lado izquierdo a 2001 y en el derecho a 2010, debemos convertir la variable Year a factor en el orden que queremos. nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% mutate(Year=factor(Year,levels = c(&quot;avgAge_2001&quot;,&quot;avgAge&quot;),labels = c(2001,2010))) Lo que hicimos es decirle a R que 1) queremos que la columna R se convierta a factor, 2) que los valores que tiene esa variable son “avgAge_2001” y “avgAge” y 3) que queremos los represente como “2001” y 2010, respectivamente. Internamente, R va a hacer que Year sea un vector numérico, con 1 represenetando a “2001” y 2 a “2010”, aunque todo el tiempo podemos trabajarlo como si fuera un vector de character. Esto es lo que logran los factores. nuevosDatosLonger$Year[1:6] # Lo que nos muestra ## [1] 2010 2001 2010 2001 2010 2001 ## Levels: 2001 2010 as.numeric(nuevosDatosLonger$Year)[1:6] # Como lo tiene en memoria ## [1] 2 1 2 1 2 1 Muy bien, ahora vamos a mejorar nuestro gráfico y, en lugar de hacerlo continuo, vamos a hacerlo discreto, haciendo que cada color represente el 25% de nuestros datos, lo que se conoce como cuartiles. Para discretizar un vector continuo, lo que tenemos que usar es cut(). Solo nos pide el vector y los puntos de quiebre. Estos puntos de quiebre no son otra cosa que un vector con estos valores. Veamos qué hace la funcion quantile() quantile(nuevosDatosLonger$AvgAge,na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 20.17079 38.02628 39.80790 41.38003 52.92446 De manera predeterminada, nos devuelvee los puntos de corte de los cuartiles, nada mal. El na.rm=TRUE es tan solo para que no tome en cuenta un valor NA que hay ene AvgAge, con el que lidiamos más adelante. Con estos datos podemos discretizar nuestra variable. nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% mutate(AvgAge=cut(AvgAge,breaks = quantile(AvgAge,na.rm = TRUE))) Perfecto, veamos qué es lo que hizo usando table() table(nuevosDatosLonger$AvgAge) ## ## (20.2,38] (38,39.8] (39.8,41.4] (41.4,52.9] ## 1776 1776 1776 1776 Lo hizo muy bien, pero esos intervalos los entendería solo una matemática. Podemos cambiarlos por algo mucho más interpretable: levels(nuevosDatosLonger$AvgAge) &lt;- c(&quot;De 20 a 38 años&quot;,&quot;entre 38 y 39.8 años&quot;,&quot;entre 39.8 y 41.4 años&quot;,&quot;más de 41.4 años y menos de 52.9&quot;) Por último, eliminamos los NAs que había en nuestra variable de edad nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% filter(!is.na(AvgAge)) Finalmente, podemos hacer nuestro gráfico. Si no comprenden totalmente el código, revisen el capítulo de visualizaciones. ggplot(nuevosDatosLonger) + geom_sf(col = NA, aes(fill=AvgAge)) + scale_fill_viridis_d() + labs(title=&#39;Edad promedio por radio censal en años&#39;, fill=&quot;&quot;) + coord_sf(datum = NA) + theme_minimal() + facet_wrap(~Year) + theme(legend.position = &quot;bottom&quot;) 5.4 Ejercicio La variable ocupadosUniversitariaComp indica la proporción de ocupados con estudios universitarios completos como el total de ocupados para cada uno de los radios censales en 2001 y 2010. Hagan un mapa usando ggplot que muestre a los datos de 2001 y 2010 en formato de polígonos 2010 en la Ciudad de Buenos Aire ¿Qué patrones encuentran? "],
["el-automovil-de-la-estadistica.html", "6 El automovil de la estadistica 6.1 ¿Cuál es la relación entre la altura y el peso de las personas? 6.2 El “objetivo” de la regresión lineal 6.3 Agregando variables explicativas 6.4 Regresión lineal múltiple: controlando por otros factores 6.5 Una razón para ser cuidadoso al interpretar los coeficientes: la multicolinealidad 6.6 Ejercicios 6.7 Lecturas recomendadas", " 6 El automovil de la estadistica Al terminar este capítulos ustedes van a poder: - Entender la mecánica de la Regresión Lineal - Poder estimar Regresiones Lineales simples y múltiples en R - Interpretar los coeficientes de los modelos y su incertidumbre - Comprender algunos de los límites de la Regresión Lineal Múltiple El objetivo de este capítulo es introducir a uno de los métodos más clásicos del análisis de datos: la regresión. Tal es así que es popularmente conocido como “el automóvil de la estadística” o, como dice Walter Sosa Escudero, “Los Rolling Stones” del análisis de datos. Veremos para qué puede servirnos y cómo funciona 6.1 ¿Cuál es la relación entre la altura y el peso de las personas? Imaginen que tienen la siguiente pregunta: ¿Cuál es la relación entre la altura de una persona y su peso? Esta es solo una de las muchas preguntas que nos podemos hacer, pero es representativa de lo que la regresión puede hacer por nosotros (o de lo que nosotros podemos hacer con su ayuda). Descarguen desde aquí los datos con los que trabajaremos la primera parte de la clase: load(url(&#39;https://github.com/datalab-UTDT/GIS2/raw/master/Data/HowellData.RData&#39;)) str(Howell1) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... Como pueden ver luego de ejecutar el comando “str”, Howell1 es un data.frame que contiene 544 observaciones en cuatro variables: height (altura), weight (peso), age (edad) y male (genero). Todas son variables numéricas, incluyendo male, que toma valor 1 cuando la observación pertenece a un hombre y 0 cuando es a una mujer. Recordemos nuestra primera pregunta ¿Cuál es la relación entre la altura y y el peso? Creo que todos haríamos lo mismo de una manera intuitiva: grafiquemos la relación entre ambas variables. Haremos este gráfico con la ayuda de ggplot2, el paquete principal dentro de tidyverse para hacer todo tipo de gráficos. También usaremos el paquete ggthemes, que nos permite rápidamente hacer un gráfico más estéticamente exitoso (ya introdujimos este paquete en el Capítulo 3). Para usar ggplot2() y algunos temas que nos da ggthemes() debemoso primero cargar las librerías, que es lo primero que hace el siguiente código: # Hay que cargar las librerias library(tidyverse) library(ggthemes) ggplot(Howell1) + geom_point(aes(x=weight, y=height)) + theme_fivethirtyeight() + labs(x=&#39;Peso (kg)&#39;, y = &#39;Altura (kg)&#39;) + theme(axis.title = element_text(size=14)) SOBRE EL GRÁFICO Veamos un poquito el código del gráfico. Por un lado, llamo a **ggplot**, que es mi manera de decirle que quiero hacer un gráfico, y le paso el data.frame desde donde usará los datos. Luego, usando un **+**, le decimos qué tipo de gráfico queremos que haga: un scatterplot (dispersión de puntos), **geom_point**. Pero ahí mismo tenemos que indicarle cual columna irá en la el eje x (horizontal) y cuáles en el eje y (vertical). Con estos elementos ya puede hacer un gráfico, si quieren ejecutenlo. Pero además quería emprolijar un poco el gráfico: con **theme_fivethirtyeight()**, del paquete **ggthemes**, podemos formatearlo como el estilo de la famosa página FiveThirtyEight. Además queremos agregarle nombres a los ejes, lo que hacemos con **labs()**, simplemente asignándole el nombre que queremos para cada uno de ellos. La última parte, ** theme(axis.title = element_text(size=14))**, es un poco más complejo pero básicamente lo que hacemos es decirle que queremos que los títulos de los ejes tengan un tamaño de 14 puntos. ¿Qué ven? Definitivamente parece haber una relación positiva entre altura y peso (es decir, a mayor peso de una persona observamos una mayor altura). Sin embargo, en la primera parte pareciera que los puntos son un poco más “empinados” y hacia el final se hacen un poco más planos ¿verdad? Veamos si podemos usar a la regresión para tener una mejor idea de esta relación. 6.2 El “objetivo” de la regresión lineal Lo que hace la regresión lineal es muy simple. Imaginen que queremos modelar la relación entre estas dos variables como una linea. Sí, sabemos que una linea no puede pasar perfectamente por todos los puntos: definitivamente va a tener que estar más cerca de algunos que de otros. Ahora bien ¿Pueden ustedes encontrar ustedes la recta que minimice la distancia entre todos los puntos? Si son honestos seguro respondieron que no, pero por suerte la regresión lo puede hacer por ustedes. La regresión lineal puede pensarse como aquella máquina que consigue la recta que mejor ajusta a los puntos ¿Qué significa que mejor ajusta? Ya que podríamos tener distintas medidas de cuál es la distancia a minimizar. La regresión lineal usa la sumatoria de la distancia al cuadrado entre cada observación y la recta. Si todo esto parece un poco difícil, no se preocupen. Vamos a intentar dejarlo lo más claro posible. La función que nos permite estimar regresiones lineales en R es la función lm(). Supongamos, primero, que no existe ninguna relación entre la altura y el peso. De hecho, asumamos que la altura de una persona solo puede explicarse por un único valor, que es para todos los casos el mismo. Pero antes de hacer eso, saquemos todas aquellas observaciones menores a 18 años. Howell1Adults &lt;- Howell1 %&gt;% filter(age&gt;=18) regresion1 &lt;- lm(data = Howell1Adults, formula = height ~ 1) Como pueden ver, la función lm necesita de dos parámetros para hacer su trabajo: los datos y una fórmula. La fórmula consiste en el lugar donde establecemos la relación entre nuestra variable dependiente (la que queremos explicar) y las independientes (las que nos explican a la variable dependiente). A la izquierda de ~ escribimos a la variable dependiente, y luego de ese símbolo ponemos las variables independientes. En nuestro caso queremos que solo nos explique la altura con una constante. Veamos que devolvió summary(regresion1) ## ## Call: ## lm(formula = height ~ 1, data = Howell1Adults) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.0721 -6.0071 -0.2921 6.0579 24.4729 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 154.5971 0.4127 374.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.742 on 351 degrees of freedom La función summary nos devuelve un resumen de nuestro objeto lm. Vamos a ir viendo algunas de las líneas, pero por el momento solo nos importa la parte de “Coefficients”. Los coeficientes en una relación lineal nos dicen cual es “el cambio esperado” en la variable dependiente (la que queremos explicar) ante una variación de una unidad en nuestra variable independiente (la que usamos para explicar a la dependiente). En nuestro caso, solo le pedimos que nos calculara lo que se conoce como intercepto, que está presente para cada una de las observaciones y no cambia de valor. Por lo que, en este simple modelo, nuestro único coeficiente, que tiene un valor de 154.5971, nos dice que la recta que minimiza la distancia al cuadrado hacia cada punto de la altura tiene ese valor. Ahora hagan lo siguiente: mean(Howell1Adults$height) ## [1] 154.5971 ¿Qué hicimos? Calculamos el promedio de la altura, y coincidió con el coeficiente que estimó la regresión lineal ¿Por qué? Porque la regresión puede pensarse como una máquina de hacer promedios. De hecho, dado un conjunto de puntos, el promedio es la medida que minimiza las distancias al cuadrado con respecto a todos los puntos. Vayan probando si no me creen. vector &lt;- c(0,5,2,4,6,8,12) promedio &lt;- mean(vector) sum((vector-promedio)^2) ## [1] 93.42857 sum((vector-(promedio+1))^2) ## [1] 100.4286 sum((vector-(promedio-1))^2) ## [1] 100.4286 sum((vector-(promedio+0.5))^2) ## [1] 95.17857 sum((vector-(promedio-0.5))^2) ## [1] 95.17857 6.3 Agregando variables explicativas Aun si el promedio es la mejor estimación de una altura cualquiera si suponemos que no hay relación entre la altura y el peso, ya sabemos que parece haber una relación positiva entre altura y peso. Vamos a modelar esto. regresion2 &lt;- lm(data = Howell1Adults, formula = height ~ weight) summary(regresion2) ## ## Call: ## lm(formula = height ~ weight, data = Howell1Adults) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.7464 -2.8835 0.0222 3.1424 14.7744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 113.87939 1.91107 59.59 &lt;2e-16 *** ## weight 0.90503 0.04205 21.52 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.086 on 350 degrees of freedom ## Multiple R-squared: 0.5696, Adjusted R-squared: 0.5684 ## F-statistic: 463.3 on 1 and 350 DF, p-value: &lt; 2.2e-16 Ahora tenemos un nuevo parámetro: weight, de valor 0.91 (para redondear). También estimó el intercepto: siempre lo hará, a menos que lo indiquemos con un - 1 en alguna parte de la fórmula. Pero ahora el intercepto es un poco más bajo, no? Y qué significa weight 0.91? Significa que para cada peso adicional, nuestro modelo estima que, en promedio, la altura es 0.91 cms más alta. Gráfiquemos esto para que quedé más claro qué fue lo que pasó: ggplot(Howell1Adults) + geom_abline(slope = coef(regresion2)[2],intercept = coef(regresion2)[1]) + geom_point(aes(x=weight, y=height)) + theme_fivethirtyeight() + labs(x=&#39;Peso (kg)&#39;, y = &#39;Altura (kg)&#39;) + theme(axis.title = element_text(size=14)) Los puntos en el gráfico son los mismos de antes, pero ahora agregamos la recta de regresión que hace exactamente lo que esperábamos: pasa por el centro. Nuestro intercepto es ahora un poco más bajo (113.87939) ¿pero tiene la misma interpretación ? ¿es el valor que “arriesgaríamos” para cualquier peso? 6.3.1 Interpretación de los coeficientes (y su incertidumbre) Ya adelantamos una interpretación provisoria de los coeficientes: el aumento esperado en la variable dependiente cuando aumentamos en una unidad el valor de nuestra variable independiente. Pero para uno de nuestros coeficientes en nuestra regresión simple, el coeficiente, esta interpretación no hace mucha justicia. El valor del intercepto es simplementemente el valor de altura esperado cuando una persona pesa 0kgs. Sí, así como leyeron: la altura para alguien que no pesa. No le echen la culpa a la regresión, calculó una recta y, dada la pendiente, cuando el peso es cero la altura es de algo así como 113cm2. Si miran con atención la salida del anterior summary, van a ver que además del estimate se reporta un std. error, un t. value y un Pr(&gt;|t|). Veamos qué significa esta jerga y por qué es importante entender por qué se reportan en la salida típica de una regresión. El standard error o error estándar de un parámetro cuantifica la incertidumbre asociada a que estamos trabajando con una muestra del proceso que genera estos datos, no con “la población completa”. Este hecho necesariamente agrega algo de incertidumbre a nuestras estimaciones ¿Si nuestros datos no son precisamente representativos del resto de las observaciones? El desvío standard es una forma conveniente de comunicar esto. En un modelo tan simple como el que estamos viendo, este error estándar depende solo de 3 variables: la cantidad de observaciones y la variabilidad de nuestra variable independiente (positivamente) y del desvió estándar de los residuos de nuestra regresión (negativamente). Por su parte, el t. value es simplemente el estadístico que usaremos para testear qué tan probable es que, dado el error estándar y el valor del coeficiente estimado, este sea en realidad cero en términos poblacionales. El t.value es fácil de calcular; dividan el valor del parámetro y por el standard error. Finalmente, el Pr(&gt;|t|) es una de las salidas que más suelen abusarse cuando se hace una regresión. Lo que hace es testear el valor de nuestro t value con los valores que toma una distribución de probabilidades cuando el parámetro poblacional es en realidad 0. Si nuestro t value es muy alto / muy bajo, entonces la probabilidad de que el parámetro cero es muy baja. Si no es tan alto o bajo, entonces hay una mayor probabilidad de que el parámetro sea en realidad cero. ¿Cuál es la probabilidad con la que estamos dispuestos a aceptar que un parámetro de nuestro modelo es significativo? No existe ninguna medida absoluta que haga mejor o peor uno de estos valores. Podría ser un 20%, un 50%, un 23% o un 1%. En la práctica, tres valores son los más utilizados: 1%, 5% y 10%, siendo 5% el valor más aceptado por aquellas personas que se dedican a la estadística aplicada. No hay ninguna razón por la cual tenga que ser este valor u otro. En nuestro primer ejemplo, la probabilidad de que nuestro parámetro de peso sea igual a cero es realmente baja. 6.3.2 ¿Entonces pesar un kilo más aumenta la altura en aproximadamente un centímetro? Este caso es utilizado en diversas introducciones por una razón importante: NO hay interpretación causal en una regresión, excepto en casos MUY particulares. Como regla general, podemos decir que cada vez que usen una regresión lineal no van a poder inferir causalidad desde los coeficientes estimados. Aquí entramos en territorio de la inferencia causal. Las regresiones de este estilo con datos observacionales nos permiten establecer relaciones entre las variables. Y, de contar con un modelo teórico de la relación entre las variables, poder testear si los datos son compatible con la teoría. La regresión por si sola no nos va a contar cómo funcionan las cosas en el mundo. 6.3.3 Intervalos de confianza: otra forma de pensar la incertidumbre El p.valor nos sirve para saber si nuestro modelo tiene la evidencia suficiente como para decir que el coeficiente es distinto a cero. A veces queremos saber dentro de qué rangos se encuentra el parámetro con cierto nivel de probabilidad. A estos “rangos” se los llama intervalos de confianza, dentro del cual podemos decir que, si juntáramos muchas veces muestras provenientes de nuestra población bajo estudio, encontraríamos al parámetro X cantidad de veces dentro de ese rango. R calcula eso por nosotros de una manera muy simple: confint(regresion2,level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 110.1207774 117.6380098 ## weight 0.8223315 0.9877267 ¿Qué fue lo que nos devolvió? Dos valores para cada uno de nuestros coeficientes: el intercepto y weight. Uno es un límite inferior (2.5%) y otro un límite superior (97.5%) de nuestro intervalo de confianza con un nivel de 95%. Esto puede interpretarse de la siguiente manera: si tomáramos nuevamente muestras sobre altura y peso de esta población, entonces en 95 de cada 100 casos el coeficiente estarían entre el límite inferior y el superior ¿Magia? No hace falta creerlo, cuando podemos hacer simulaciones: set.seed(4) samples &lt;- c(1:1000) weightCoefs &lt;- c() for(sample in samples) { indices &lt;- sample.int(n = nrow(Howell1Adults),size = nrow(Howell1Adults), replace = TRUE) weightCoefs&lt;- c(weightCoefs, coef(lm(data = Howell1Adults[indices,], formula = height ~ weight))[2]) } En el vector weightCoefs ahora tenemos 1.000 estimaciones del parámetro weight tomando muestras al azar de nuestras observaciones, técnica que se conoce como bootstrapping. Recordemos que nuestra regresión había dicho que el 95% de los valores deberían caer entre 0.8223315 0.9877267. Veamos cuántos cayeron: sum(weightCoefs&gt;=0.8223315 &amp; weightCoefs&lt;=0.9877267)/1000 ## [1] 0.962 quantile(weightCoefs,probs = c(0.025,0.975)) ## 2.5% 97.5% ## 0.8243526 0.9795347 6.3.4 Qué explica y que no nuestra regresión Otra de las preguntas clásicas que le queremos hacer a nuestra regresión es qué proporción de la variabilidad de la variable dependiente explica. Una forma muy expandida de resumir esto es a través de lo que se conoce como “R2” o “R cuadrado”. Si revisan la salida de summary van a ver que abajo de los coeficientes dice “Multiple R-squared” y “Adjusted R-squared”. El primero de ellos nos dice que proporción de la variación al cuadrado de las observaciones de altura son explicadas por la variación de nuestras variables explicativas. Dicho de otra manera, nuestro modelo puede “explicar” (o mejor dicho, predecir) el 57% del movimiento de las alturas. El resto, no puede explicarlo. ¿Cómo se calcula este 57%? Simplemente sumamos la diferencia al cuadrado entre la predicción de nuestro modelo y el promedio de las alturas (“lo que explica el modelo”) y lo dividimos por la sumatoria de la diferencia al cuadrado entre el valor de una observación y el promedio (“lo que hay que explicar”). round(sum((predict(regresion2)-mean(regresion2$model$height))^2)/ sum((regresion2$model$height-mean(regresion2$model$height))^2),2) ## [1] 0.57 ¿Cuánto más explica nuestro modelo si agregamos al sexo de las personas? Howell1Adults$male &lt;- factor(Howell1Adults$male) summary(lm(data = Howell1Adults, formula = height ~ weight + male)) ## ## Call: ## lm(formula = height ~ weight + male, data = Howell1Adults) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.7859 -2.5506 0.4669 2.6278 14.1486 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 122.70338 1.76232 69.63 &lt;2e-16 *** ## weight 0.64117 0.04148 15.46 &lt;2e-16 *** ## male1 6.50031 0.53592 12.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.272 on 349 degrees of freedom ## Multiple R-squared: 0.6973, Adjusted R-squared: 0.6955 ## F-statistic: 401.9 on 2 and 349 DF, p-value: &lt; 2.2e-16 ¿Qué pasó con nuestro R2? Pasó del 57% al 69% tras la inclusión del género ¿Y con nuestro coeficiente de peso? Cambió también, ¿no? Vamos a ver por qué en breve. Pero antes de esto, veamos una diferencia importante entre predecir un valor particular y el promedio. 6.3.5 Incertidumbre en el promedio e incertidumbre en el valor predicho Dado nuestro modelo lineal, si ustedes tuvieran que decir cuanto mide una persona que pesa 50kg ¿qué dirían? yo le sumaría al intercepto el coeficiente de peso por 50: prediccion &lt;- coef(regresion2)[1] + coef(regresion2)[2]*50 prediccion ## (Intercept) ## 159.1308 159.13 sería mi respuesta ¿Y cuál sería el intervalo de confianza? Acá es donde se complica un poco. Nostros estuvimos modelando hasta ahora todo en promedio. Yo mismo les dije que podría definirse a la regresión como una máquina de hacer promedios. Y todo lo que estuvimos viendo hasta ahora es por qué hay incertidumbre con respecto al valor esperado (o promedio) de la altura ante cambios en el peso. Nada dijimos sobre la incertidumbre a nivel puntual. En los modelos de regresión en general asumimos implícitamente que en cada uno de los valores de peso, las alturas siguen una distribución normal, cuyo valor central o promedio es el que está dado por la recta de la regresión ¡pero no los valores puntuales! Estos dependen a su vez del desvío estándar de los errores de nuestro modelo. Una regresión modela a una variable que observamos (la altura) como una función de otro conjunto de variables que observamos (como el peso) y otra parte que corresponde al error. Por construcción, el promedio de los errores es 0. Por lo cual, en promedio, la altura va a estar explicada por el valor del peso, teniendo en cuenta nuestra incertidumbre muestral. Esto ya lo hemos visto. Pero para tener la predicción de un valor necesitamos agregar el término del error, ya que en un caso particular el error no tiene porque ser cero. La diferencia es muy relevante: predict(regresion2,newdata = data.frame(weight=50), interval =&#39;confidence&#39;) ## fit lwr upr ## 1 159.1308 158.4556 159.8061 predict(regresion2,newdata = data.frame(weight=50), interval =&#39;prediction&#39;) ## fit lwr upr ## 1 159.1308 149.1045 169.1572 El primero de los comandos estima el intervalo de confianza al 95% de la altura promedio dado que el peso es de 50kg. La predicción es la que calculamos antes, con un intervalo de confianza entre [158.4556;159.8061]. La segunda calcula el intervalo de confianza de la altura puntual con un intervalo del 95%: si hiciéramos una muestra de 100 personas con peso de 50kg, 95 de ellas caerían en el intervalo contenido entre [149.1045;169.1572]. Este intervalo de confianza depende tanto de la incertidumbre del valor medio como de la varianza de nuestro término de error (“cuánto se aleja de 0”). Veamos cómo podemos visualizar los errores de nuestro modelo: plot(residuals(regresion2)) mean(residuals(regresion2)) # Casi cero ## [1] 5.337416e-17 sd(residuals(regresion2)) # desvio standard ## [1] 5.079086 Reduzcamos el desvío standard de nuestros residuos eliminando los valores donde nuestra recta pasa mas lejos regresion3 &lt;- lm(data = Howell1Adults[abs(residuals(regresion2))&lt;sd(residuals(regresion2)),], formula = height ~ weight) summary(regresion3) ## ## Call: ## lm(formula = height ~ weight, data = Howell1Adults[abs(residuals(regresion2)) &lt; ## sd(residuals(regresion2)), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3280 -2.0682 -0.0105 2.1795 5.0513 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 112.42425 1.15216 97.58 &lt;2e-16 *** ## weight 0.93791 0.02535 36.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.631 on 245 degrees of freedom ## Multiple R-squared: 0.8481, Adjusted R-squared: 0.8475 ## F-statistic: 1368 on 1 and 245 DF, p-value: &lt; 2.2e-16 predict(regresion3,newdata = data.frame(weight=50), interval =&#39;prediction&#39;) ## fit lwr upr ## 1 159.32 154.121 164.5189 ¡Con la caída del desvío estándar de los errores se achicaron los intervalos de confianza de las predicciones! Tal como esperábamos 6.4 Regresión lineal múltiple: controlando por otros factores Uno de los datasets más divertidos para introducir a la regresión lineal múltiple y algunas de sus oportunidades y problemas consiste en el dataset de Waffle House, una cadena de waffles que le compite a la más famosa Ihop. Primero carguen los datos a la sesión: load(url(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/Data/WaterDivorce.RData&quot;)) str(WaffleDivorce) ## &#39;data.frame&#39;: 50 obs. of 13 variables: ## $ Location : Factor w/ 50 levels &quot;Alabama&quot;,&quot;Alaska&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Loc : Factor w/ 50 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 2 1 4 3 5 6 7 9 8 10 ... ## $ Population : num 4.78 0.71 6.33 2.92 37.25 ... ## $ MedianAgeMarriage: num 25.3 25.2 25.8 24.3 26.8 25.7 27.6 26.6 29.7 26.4 ... ## $ Marriage : num 20.2 26 20.3 26.4 19.1 23.5 17.1 23.1 17.7 17 ... ## $ Marriage.SE : num 1.27 2.93 0.98 1.7 0.39 1.24 1.06 2.89 2.53 0.58 ... ## $ Divorce : num 12.7 12.5 10.8 13.5 8 11.6 6.7 8.9 6.3 8.5 ... ## $ Divorce.SE : num 0.79 2.05 0.74 1.22 0.24 0.94 0.77 1.39 1.89 0.32 ... ## $ WaffleHouses : int 128 0 18 41 0 11 0 3 0 133 ... ## $ South : int 1 0 0 1 0 0 0 0 0 1 ... ## $ Slaves1860 : int 435080 0 0 111115 0 0 0 1798 0 61745 ... ## $ Population1860 : int 964201 0 0 435450 379994 34277 460147 112216 75080 140424 ... ## $ PropSlaves1860 : num 0.45 0 0 0.26 0 0 0 0.016 0 0.44 ... WaffleDivorce$WaffleHousesPC &lt;- WaffleDivorce$WaffleHouses/WaffleDivorce$Population Son 50 observaciones, una por cada Estado de EEUU. Ahora hagamos el siguiente gráfico y la siguiente regresión: ggplot(WaffleDivorce, aes(x=WaffleHousesPC, y = Divorce))+ geom_point() + geom_smooth(method=&#39;lm&#39;,se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; summary(lm(WaffleDivorce,formula = Divorce ~ WaffleHousesPC)) ## ## Call: ## lm(formula = Divorce ~ WaffleHousesPC, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5343 -1.2448 -0.0718 1.0552 3.6802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.31980 0.27723 33.617 &lt; 2e-16 *** ## WaffleHousesPC 0.07442 0.02730 2.726 0.00892 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.712 on 48 degrees of freedom ## Multiple R-squared: 0.1341, Adjusted R-squared: 0.116 ## F-statistic: 7.431 on 1 and 48 DF, p-value: 0.008921 ¡La cantidad de Waffle Houses per cápita se asocia positivamente con la tasa de divorcio de los Estados de Estados Unidos! No solo eso, sino que el coeficiente es significativo incluso por debajo del 1%, y el R2 es del 13%. Este resultado debería recordarnos que estos modelos, por si solos, no nos muestran cómo funciona el mundo, sino que hace exactamente lo que dijimos: estima una curva que pasa lo más cerca de todos los puntos. La regresión lineal múltiple puede ayudarnos al menos parcialmente a “controlar” por otras variables para encontrar una relación entre dos variables todo lo demás constante. Veamos en qué casos resulta especialmente útil. 6.4.1 Asociación espuria La tasa de divorcio puede depender de forma razonable de la tasa de matrimonio - después de todo, para divorciarse hay que estar casado -, y también puede estar relacionado con la edad promedio de casamiento, quizás por una menor expectiva de vida mientras más jóven se es y una mayor probabilidad de separarse con el paso de los años. Modelemos estas dos relaciones en dos regresiones distintas y en unos gráficos. require(gridExtra) grid.arrange(ggplot(WaffleDivorce,aes(x=Marriage, y = Divorce))+ geom_point() + geom_smooth(method=&#39;lm&#39;,se = FALSE) , ggplot(WaffleDivorce, aes(x=MedianAgeMarriage, y = Divorce))+ geom_point() + geom_smooth(method=&#39;lm&#39;,se = FALSE)) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; regMarriage &lt;- lm(WaffleDivorce,formula = Divorce ~ Marriage) regEdad &lt;- lm(WaffleDivorce,formula = Divorce ~ MedianAgeMarriage) summary(regMarriage) ## ## Call: ## lm(formula = Divorce ~ Marriage, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0068 -1.2173 0.1214 1.1805 4.4971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.08404 1.31337 4.632 2.78e-05 *** ## Marriage 0.17918 0.06418 2.792 0.00751 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.706 on 48 degrees of freedom ## Multiple R-squared: 0.1397, Adjusted R-squared: 0.1218 ## F-statistic: 7.793 on 1 and 48 DF, p-value: 0.007507 summary(regEdad) ## ## Call: ## lm(formula = Divorce ~ MedianAgeMarriage, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4836 -0.9813 -0.0348 0.9932 3.6146 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.4703 4.4210 7.345 2.18e-09 *** ## MedianAgeMarriage -0.8744 0.1695 -5.159 4.68e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.476 on 48 degrees of freedom ## Multiple R-squared: 0.3567, Adjusted R-squared: 0.3433 ## F-statistic: 26.61 on 1 and 48 DF, p-value: 4.682e-06 Ambos modelos están muy seguros de lo que dicen: el hecho de que haya más casamientos genera más divorcios, mientras que a menor edad promedio de casamiento en un Estado, mayor tasa de divocio. Pero no podemos comparar los coeficientes de los dos modelos sin más: debemos usar ambos en uno solo para conocer si los valores de una variable siguen siendo importantes luego de conocer el valor de la otra variable. regMulti &lt;- lm(WaffleDivorce,formula = Divorce ~ MedianAgeMarriage + Marriage) summary(regMulti) ## ## Call: ## lm(formula = Divorce ~ MedianAgeMarriage + Marriage, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5177 -0.9828 -0.0458 0.9224 3.2818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.87665 7.66104 4.814 1.58e-05 *** ## MedianAgeMarriage -0.99965 0.24593 -4.065 0.000182 *** ## Marriage -0.05686 0.08053 -0.706 0.483594 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.483 on 47 degrees of freedom ## Multiple R-squared: 0.3634, Adjusted R-squared: 0.3364 ## F-statistic: 13.42 on 2 and 47 DF, p-value: 2.455e-05 ¿Qué sucedió? Ahora nuestro modelo nos dice que no hay evidencia para concluir que la relación entre las dos variables es distinta a cero (vean el p valor 48% ¿Qué pasa si hacen el intervalo de confianza de ese parámetro?) , mientras que la edad promedio - en realidad, mediana - de casamiento rechaza sin mayores problemas la hipótesis de nulidad. Cuando hacemos regresiones múltiples, la regresión literalmente estima un coeficiente luego del otro. Hagamos la siguiente prueba: regPorPartesMarriage &lt;- lm(WaffleDivorce,formula = Marriage ~ MedianAgeMarriage) WaffleDivorce$residuosEdad &lt;- resid(regPorPartesMarriage) regPorPartesMarriage &lt;- lm(WaffleDivorce, formula = Divorce ~ residuosEdad) summary(regPorPartesMarriage) ## ## Call: ## lm(formula = Divorce ~ residuosEdad, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6841 -1.3864 -0.0047 1.1876 3.9498 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.68800 0.25929 37.363 &lt;2e-16 *** ## residuosEdad -0.05686 0.09954 -0.571 0.57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.833 on 48 degrees of freedom ## Multiple R-squared: 0.006753, Adjusted R-squared: -0.01394 ## F-statistic: 0.3264 on 1 and 48 DF, p-value: 0.5705 Si prestan atención, el coeficiente Marriage de la regresión simple entre los residuos de la regresión entre Marriage y la edad mediana de casamiento es el mismo que coeficiente de Marriage en la regresión múltiple. Lo mismo es cierto para el coeficiente de la edad mediana. regPorPartesMedianAge &lt;- lm(WaffleDivorce,formula = MedianAgeMarriage ~ Marriage) WaffleDivorce$residuosMarriage &lt;- resid(regPorPartesMedianAge) regPorPartesMedianAge &lt;- lm(WaffleDivorce, formula = Divorce ~ residuosMarriage) summary(regPorPartesMedianAge) ## ## Call: ## lm(formula = Divorce ~ residuosMarriage, data = WaffleDivorce) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4989 -1.1360 0.0917 0.8517 3.5424 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.6880 0.2292 42.27 &lt; 2e-16 *** ## residuosMarriage -0.9996 0.2687 -3.72 0.000522 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.621 on 48 degrees of freedom ## Multiple R-squared: 0.2238, Adjusted R-squared: 0.2076 ## F-statistic: 13.84 on 1 and 48 DF, p-value: 0.000522 ¿Qué significa esto? La regresión múltiple “elimina” lo que ya sabemos de una variable independiente al conocer el resto de las independientes. Solo luego de eso lo regresa contra la variable que queremos explicar. En este caso, eso marcó la diferencia entre ser “confundidos” o no por una regresión espuria. 6.4.2 Relación enmascarada La cantidad de energía contenida en una determinada cantidad de leche suele estar asociada según algunos investigadores de la biologia evolutiva por el tamaño del cerebro del animal: a mayor tamaño de cerebro, mayor necesidad de kilocalorias por gramo de leche para que sea más eficiente el proceso. Para investigar esta hipótesis, vamos a trabajar con un dataset que, aunque tiene pocas observaciones, nos va a servir para estudiar esta relación. load(url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/milk.RData&quot;)) milk &lt;- milk[complete.cases(milk),] El dataset tiene 8 variables y 17 observaciones. La función complete.cases() nos devuelve las filas que efectivamente no tienen ningún valor faltante (o NA) en ninguna de las columnas. Ya podríamos estimar el modelo para ver si nuestra hipótesis se verifica en los datos regresion4 &lt;- lm(data = milk, formula = kcal.per.g ~ neocortex.perc) summary(regresion4) ## ## Call: ## lm(formula = kcal.per.g ~ neocortex.perc, data = milk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19027 -0.14693 -0.03744 0.15613 0.29959 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.353332 0.501120 0.705 0.492 ## neocortex.perc 0.004503 0.007389 0.609 0.551 ## ## Residual standard error: 0.1764 on 15 degrees of freedom ## Multiple R-squared: 0.02417, Adjusted R-squared: -0.04089 ## F-statistic: 0.3715 on 1 and 15 DF, p-value: 0.5513 ggplot(regresion4, aes(x=neocortex.perc, y = kcal.per.g))+ geom_point() + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Nuestra hipótesis, condicional en nuestro modelos y datos, no parece ser corroborada. Ahora bien ¿qué pasa si agregamos el peso promedio de las hembras de estas especies? ¿Podría esta variable estar relacionada tanto con la energía por gramo de leche y el tamaño del neocortex y esconder una relación? regresion5 &lt;- regresion5 &lt;- lm(data = milk, formula = kcal.per.g ~ mass) summary(regresion5) ## ## Call: ## lm(formula = kcal.per.g ~ mass, data = milk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.24028 -0.11410 -0.01743 0.15337 0.27703 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.701516 0.049968 14.039 4.92e-10 *** ## mass -0.002637 0.001766 -1.493 0.156 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1666 on 15 degrees of freedom ## Multiple R-squared: 0.1293, Adjusted R-squared: 0.07129 ## F-statistic: 2.228 on 1 and 15 DF, p-value: 0.1562 Parece un modelo más razonable, aunque el coeficiente de peso de las hermbras parece estar cerca de cero ¿Qué pasa si agregamos ambas variables juntas? regresion6 &lt;- regresion6 &lt;- lm(data = milk, formula = kcal.per.g ~ neocortex.perc + mass) summary(regresion6) ## ## Call: ## lm(formula = kcal.per.g ~ neocortex.perc + mass, data = milk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19551 -0.10061 -0.02868 0.11914 0.20677 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.438441 0.513367 -0.854 0.4075 ## neocortex.perc 0.017541 0.007870 2.229 0.0427 * ## mass -0.005367 0.001992 -2.694 0.0175 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1482 on 14 degrees of freedom ## Multiple R-squared: 0.3574, Adjusted R-squared: 0.2656 ## F-statistic: 3.893 on 2 and 14 DF, p-value: 0.04526 Nuestro modelo mejoró y bastante. Ahora ambas variables lucen significativas, aunque tienen signo inverso: el tamaño parece estar negativamente asociado, mientras que el tamaño relativo del neocortex positivamente asociado ¿Qué fue lo que pasó?. Si miran el gráfico de abajo van a ver que ambas variables explicativias o independientes se encuentran positivamente relacionadas entre ellas. ggplot(milk,aes(x=neocortex.perc, y=mass)) + geom_point() + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Si a su vez la primera variable tiene un impacto positivo y la segunda un impacto negativo, entonces se van a cancelar ambos efectos en nuestras observaciones. Pero una regresión múltiple está perfectamente apta para captar este fenómeno: lo que nos dicen los coeficientes es: dado un determinado tamaño ¿más porcentaje de neocortex se asocia con mayor caloria por gramo de leche? Ahora sí podemos concluir que nuestro modelo y datos son compatibles con esta hipótesis. 6.5 Una razón para ser cuidadoso al interpretar los coeficientes: la multicolinealidad Viendo los méritos de la regresión lineal múltiple (nos ayuda a detectar relaciones espúreas y, además, distinguir entre efectos de variables que “van de la mano”) uno puede tentarse en agregar todas las variables y hacer un modelo “saturado”, total esta máquina sabe hacer todo. Bueno, no tan rápido. Vamos a ver una razón por la cuál hay que ser más cuidadoso: la multicolinealidad. En el próximo capítulo veremos otra razón por la cual también hay que serlo: el overfitting. Imaginense que queremos explicar la altura de una pesona por el alto de sus piernas. Pero en nuestro modelo no vamos a incluir solo una pierna, si no las dos: la izquierda y la derecha. Veamos qué pasa con estos coeficientes. Esta vez no vamos a trabajar con un dataset que tenga esta información (quizás exista, no lo sé), pero vamos a simularlo. set.seed(2) n &lt;- 100 altura &lt;- rnorm(n,10,2) leg_prop &lt;- runif(n,0.4, 0.5) leg_left &lt;- leg_prop*altura + rnorm(n, 0, 0.02) leg_right &lt;- leg_prop*altura + rnorm(n, 0, 0.02) alturas &lt;- data.frame(altura, leg_left, leg_right) Ahora hagamos tres modelos: uno explicando la altura como una función de la pierna izquierda, otro de la pierna derecha y otro de ambos. regresion7 &lt;- lm(data = alturas, formula = altura ~ leg_left) regresion8 &lt;- lm(data = alturas, formula = altura ~ leg_right) regresion9 &lt;- lm(data = alturas, formula = altura ~ leg_left + leg_right) ¿Qué observan? Los modelos que incluyen solo a una de las dos piernas nos devuelven los parámetros que esperábamos, en línea con nuestros datos simulados. El modelo que incluye las dos variables, ¿Por qué? Recuerden que en una regresión lineal múltiple cada coeficiente nos dice cuál es la variación esperada en la variable dependiente cuando cambia en una unidad dado el valor del resto de las variables dependientes. Nuestro modelo nos está diciendo que no sabe exactamente cuál es el valor de saber la altura de la pierna izquierda o derecha, una vez que sabemos el valor de la otra. Lo cual es consistente con nuestros datos, pero bastante poco útil si queremos identificar el efecto de cada uno. Cabe remarcar que, si hacen las predicciones individuales, este modelo lo hará muy bien, quizás mejor que el modelo de predicción simple. El problema es que no será posible, con los conocimientos que tenemos hasta el momento, diferenciar el efecto de una variable de la otra. 6.6 Ejercicios Carguen el dataset que cuenta con los datos de precios y propiedades de los inmuebles de Properati con el siguiente código: barriosOriginal &lt;- read.table(file=&quot;https://github.com/datalab-UTDT/datasets/raw/master/barriosSample.csv&quot;, sep = &quot;;&quot;, header = TRUE, stringsAsFactors = FALSE) barriosOriginal &lt;- barriosOriginal %&gt;% select(price_aprox_usd, surface_in_m2, price_usd_per_m2, rooms) barriosOriginal &lt;- barriosOriginal[complete.cases(barriosOriginal),] ¿Cuál es la relación entre el precio en USD de una propiedad (price_aprox_usd) y la cantidad de habitaciones (rooms)? Estime una regresión lineal y evalue los resultados A la primera regresión agregarle la superficie en metros cuadrados (surface_in_m2) ¿Qué sucedió con los coeficientes? ¿A qué se lo atribuiría? ¿Cuál es la relación entre el precio de los inmuebles en dólares por metro cuadrado (price_usd_per_m2) y el tamaño de la propiedad? 6.7 Lecturas recomendadas Este capítulo está basado en el capítulo 5 del muy buen libro de Richard McElreath “Statistical Rethinking”. Recomiendo la lectura de los capítulos 4 y 5. Al mismo tiempo, este capítulo está inspirado en el libro de Walter Sosa Escudero “Big Data” Pueden consultar dónde conseguir el libro de McElreath aquí y el de Walter Sosa Escuedero aquí "],
["los-beatles-del-machine-learning.html", "7 Los beatles del Machine Learning 7.1 Machine Learning 7.2 ¿Cómo funciona un árbol de decisión? 7.3 ¿Podemos predecir quién se murió en el Titanic? 7.4 Aplicación en el mercado de trabajo: monotributistas y cuentapropistas informales 7.5 Algunos árboles no solo clasifican: árboles de regresión 7.6 Ejercicio 7.7 Lecturas recomendadas", " 7 Los beatles del Machine Learning Si la regresión lineal es el automóvil de la estadística o, como dice Walter Sosa Escudero, los “Rolling Stones” de esa disciplina científica, podríamos decir que los árboles de decisión, quizás la familia de técnicas de machine learning más famosa del mundo, son los Beatles del aprendizaje automático (traducción al español de *machine learning). En esta clase vamos a tener una introducción a qué hacen, cómo lo hacen y para qué sirven. 7.1 Machine Learning El término Machine Learning debe ser uno de los más nombrados en los últimos años, junto a Inteligencia Artificial. Aunque no hay una clara definción de ambos conceptos, vamos a definir al segundo como “la habilidad de las maquinas de comportarse de una manera que nosotros consideramos inteligente”. Con respecto al primer concepto, mucho más estrecho, lo vamos a definir como “La capacidad de un programa de aprender a hacer una tarea cada vez mejor en base a la experiencia”, cerca de la definición del libro de Tom Mitchell, Machine Learning (2017). Notemos que el programa es quien aprende desde la experiencia: nosotros no intervenimos activamente en ese proceso de aprendizaje. Eso es lo que hace especial al Aprendizaje Automático (traducción de machine learning) En términos de Mitchell, “Se dice que un programa de computadora aprende de la experiencia (E) con respecto a una determinada clase de tarea (T) y medida de performance (P) si su performance en la tarea (T), medido por P, mejora con la experiencia E”. En definitiva: Machine Learning es la posibilidad de un programa de computadora de hacer cada vez mejor su trabajo en base a una determinada métrica. 7.2 ¿Cómo funciona un árbol de decisión? Si alguna vez jugaron al ¿Quién es Quién? conocen la principal característica de un árbol de decisión: hace preguntas que pueden ser respondidas con “si o no” (binarias) de tal manera de separar a todas las observaciones (en este caso, los nombres de los personajes) en base a las distintas variables que tienen (color de pelo, si usa o no anteojos, sexo, entre otras). De esta manera, tanto nuestra estrategia en el quién es quién como la de los árboles de decisión coinciden en dividir al espacio de nuestros datos en “segmentos” de acuerdo a los valores que toman en las distintas variables. Lo que muestra el gráfico 1 es un árbol de decisión del Quién es Quién, suponiendo que el personaje que nos tocó es una mujer con anteojos (y hay solo una en todo el tablero). Esto que hacemos intuitivamente en jerga estadística se conoce como Recursive Partitioning. Ahora bien, nuestro objetivo en el juego es identificar a la persona que nos tocó. Acá es donde comienzan las diferencias con respecto a los árboles de decisión. Por un lado, en el quién es quién nosotros, de manera activa, vamos haciendo las preguntas. Por otro lado, si aprendemos a jugar bien probablemente hagamos preguntas en las cuales la respuesta de sí o no nos elimine a la mayor cantidad de casos. Pero un árbol de decisión no requiere nuestra intervención, de allí la parte de “automático” en aprendizaje automático: tiene reglas claras para ir haciendo las preguntas necesarias para hacer la tarea de “encontrar” a nuestro personaje cada vez mejor. Por otro lado, no le interesa conocer dónde está esa única persona, sino que el objetivo es aprender a clasificar cada vez mejor a cierta variable objetivo. Por ejemplo, imaginen que en lugar de encontrar a “Clara” el objetivo sea encontrar a “Mujeres”. Quizás en el Quién es Quién dentro de las personas que tienen pelo largo hay más mujeres que hombres y pueda usarse para eso. De hecho, los árboles de decisión hacen exactamente esto último. Buscan ir segmentando el espacio de nuestras variables en distintos pedazos que logren aislar a las categorias de nuestra variable objetivo (lo que queremos predecir) de una manera más homogénea. En nuestro caso de crear un árbol para encontrar a las mujeres, desearíamos ir segmentando a las personas según preguntas cuya respuesta nos separe todos hombres o todas mujeres (o lo más cercano a eso). Veamos todo esto con un ejemplo cinematográfico. 7.3 ¿Podemos predecir quién se murió en el Titanic? En abril de 1912 el RMS Titanic chocó contra un iceberg y más de 800 de los pasajeros murieron, mientras que aproximadamente 500 sobrevivieron ¿Podemos crear un árbol de decisión que nos permita predecir quienes sobrevivieron y quienes no en base a variables como su edad, género y clase en la que viajaron? Probemoslo con el conocido dataset que simula a los pasajeros del Titanic y las variables con las que vamos a entrenar a nuestro árbol de decisión. titanic &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/martintinch0/CienciaDeDatosParaCuriosos/master/data/titanic.csv&quot;, stringsAsFactors = FALSE, sep = &#39;;&#39;) También vamos a cargar el paquete que nos va a permitir crear nuestro primer modelo de árboles de decisiones C50 (noten la C mayúscula en C50) y tidyverse: library(tidyverse) library(C50) Exploren un poco qué tiene el dataset de Titanic con el siguiente código: glimpse(titanic) ## Rows: 1,045 ## Columns: 4 ## $ survived &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, ... ## $ age &lt;dbl&gt; 29.0000, 0.9167, 2.0000, 30.0000, 25.0000, 48.0000, 63.0000, 39.0000, 53.0000, 71... ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;,... ## $ fare &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 151.5500, 26.5500, 77.9583, 0.0000, 51.47... Las variables son bastante obvias, pero antes que tenemos que hacer un poco de data wrangling, en este caso bastante menor. El paquete C5.0 trabaja mejor con factores como predictoras (las que nos van a ayudar a predecir si una persona sobrevive o no al accidente del Titanic), pero también nos exige que este expresada en ese formato la variable objetivo (en nuestro caso, survived). Por esta razón vamos a convertir ambas variables: titanic &lt;- titanic %&gt;% mutate(survived = factor(survived), sex = factor(sex)) Ya estamos en condiciones de entrenar nuestro primer árbol de decisión en R. La función que entrena al árbol se llama C5.0() y usa un sistema de fórmula muy similar al que se vio en el capítulo 4 cuando introdujimos a las regresiones: primerArbol &lt;- C5.0(formula= survived ~., data = titanic) Para ver qué tiene nuestro árbol, primero vamos a graficarlo. Esto lo podemos hacer con la función plot() plot(primerArbol) Un árbol de decisión está compuesto de nodos. Los que están al final, cuando no se hacen más bifurcaciones en nuestro dataset, se llaman hojas del árbol. Podemos ver que la primera pregunta que hace es si la persona es hombre o mujer. En caso que sea mujer, la siguiente pregunta es sobre distintos valores de la tarifa que se pagó. En caso que sea hombre, la pregunta tiene que ver con la edad. Las hojas del gráfico están acompañadas de una barra que muestra la proporción que sobrevivió (gris oscuro) y la que no lo hizo (gris claro). Por ejemplo, podemos ver que la hoja donde se concentra la mayor proporción de sobrevivientes son las mujeres con una tarifa superior a USD 47.1, mientras que la mayor proporción de muertes se encuentran entre los hombres mayores a 9 años. ¿Cómo elige un árbol de decisión por cuál variable y por cuáles valores de esas variables abrir? elige aquellos cortes de nuestros datos que dejan más homogéneos a la nueva clasificación que la que había antes de hacer el quiebre. Para esto usa el importante concepto de entropia, la cual no desarrollaremos en profundidad pero basta con decir que es una medida que describe qué tan homogeneo es un conjunto de datos. Mientras más bajo sea más homogéneo es. Veamos cómo se calcula para el total de nuestros datos proporcionSobrevivientes &lt;- table(titanic$survived)[2]/nrow(titanic) proporcionSobrevivientes # Aproximadamente un 41% de los pasajeros sobrevivieron ## 1 ## 0.4086124 # Formula de Entropía −0.59*log2(0.59)−(0.41)*log2(0.41) ## [1] 0.9765005 La entropía de nuestra base de datos es alta porque está muy cerca de estar distribuida como 50% y 50%, la situación más “heterogénea” que puede tener nuestra variable objetivo. De hecho, si se calcula la entropía de esa situación llegamos a lo siguiente: −0.5*log2(0.5)−(0.5)*log2(0.5) # Máxima entropía ## [1] 1 ¿Y si tenemos todo de una sola clase (por ejemplo, solo sobrevivientes)? -0.000001*log2(0.000001)−(1)*log2(1) # Casi cero ## [1] 1.993157e-05 Bien, ahora veamos qué pasa con la entropia si abrimos, como hizo nuestro árbol, según el género. Para esto, tenemos que sumar las proporciones al final de cada hoja: table(titanic$survived,titanic$sex) ## ## female male ## 0 96 522 ## 1 292 135 Ahora podríamos calcular la entropía en cada una de las hojas del árbol. Vayamos primero con el de mujeres: # Entropía mujeres -(96/(292+96))*log2(96/(292+96))−(292/(292+96))*log2(292/(292+96)) ## [1] 0.8071676 entropiaMujeres &lt;- -(96/(292+96))*log2(96/(292+96))−(292/(292+96))*log2(292/(292+96)) ¿Y en los hombres? # Entropía hombres -(522/(522+135))*log2(522/(522+135))−(135/(522+135))*log2(135/(522+135)) ## [1] 0.7327525 entropiaHombres &lt;- -(522/(522+135))*log2(522/(522+135))−(135/(522+135))*log2(135/(522+135)) Ahora debemos ponderar la entropía de la variable ponderando las dos hojas que abrió: entropiaGenero &lt;- entropiaHombres*(657/1045)+entropiaMujeres*(388/1045) entropiaGenero ## [1] 0.7603822 La apertura por género da una entropia de 0.76, mientras que aquella que no abre por nada tiene una de 0.9765 ¿Cómo medimos esta mejora? En lo que se conoce como Information Gain, que es tan solo la mejora en la entropia por abrir por una determinada variable con respecto a la entropía antes de abrir. informationGainGenero &lt;- 0.9765-entropiaGenero informationGainGenero ## [1] 0.2161178 En el caso de Genero la mejora es de 0.216 y les garantizo que es la mayor de la apertura de todas las variables, ya que así trabaja C5.0 7.3.1 ¿Cómo podemos medir qué tan bien clasifica nuestro árbol? Existen diversas maneras de medir la efectividad de la clasificación de un modelo de machine learning. Para este tipo de objetivo (clasificar) suele ser útil usar la matriz de confusión, que simplemente distribuye en celdas la clasificación de un determinado caso y el valor que tenía en nuestro dataset. Podemos acceder a ella mediante el método summary() aplicado a nuestro árbol summary(primerArbol) ## ## Call: ## C5.0.formula(formula = survived ~ ., data = titanic) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue Apr 14 17:58:25 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1045 cases (4 attributes) from undefined.data ## ## Decision tree: ## ## sex = male: ## :...age &lt;= 9: 1 (43/18) ## : age &gt; 9: 0 (614/110) ## sex = female: ## :...fare &gt; 47.1: 1 (118/3) ## fare &lt;= 47.1: ## :...fare &gt; 10.4625: 1 (197/56) ## fare &lt;= 10.4625: ## :...fare &lt;= 7.725: 1 (16/3) ## fare &gt; 7.725: 0 (57/23) ## ## ## Evaluation on training data (1045 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 6 213(20.4%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 538 80 (a): class 0 ## 133 294 (b): class 1 ## ## ## Attribute usage: ## ## 100.00% sex ## 62.87% age ## 37.13% fare ## ## ## Time: 0.0 secs Ya nos dice que tiene una tasa de error de 20,4% ¿Cómo podemos ver esto en la tabla? si sumamos los falsos positivos y los falsos negativos, que están en las celdas de abajo a la izquierda y arriba a la derecha (133+80) y lo dividimos por todos los casos que clasificó. ¿Es esto mucho o poco? Para responder esta pregunta es siempre necesario pensar cómo se distribuía la variable en nuestro dataset. Ya sabemos que aproximadamente el 41% de las personas se salvó, por lo que si clasificaramos a todos como sobrevivientes, tendríamos una tasa de acierto del 41% y una tasa de error del 59%. Con nuestro árbol de decisión ahora tenemos una tasa de error de 20,4% (y de acierto de 79,6%)! Otra forma de pensar esto es mediante el lift que es la división entre la proporción de acierto en nuestro árbol y la del dataset original: 79.6/41= 1.94. Cualquier valor mayor a uno muestra que la tasa de acierto es mayor a la del denominador. 7.3.2 Un árbol puede reducirse a reglas Una de las principales ventajas de los árboles de decisión de este estilo es que podemos reducir su complejidad a un conjunto de reglas que nos permite clasificar los casos. Para esto solo tenemos que cambiar un parámetro al entrenar el árbol de decisión primerArbol &lt;- C5.0(formula= survived ~., data = titanic, rules=TRUE) summary(primerArbol) ## ## Call: ## C5.0.formula(formula = survived ~ ., data = titanic, rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue Apr 14 17:58:26 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1045 cases (4 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (614/110, lift 1.4) ## age &gt; 9 ## sex = male ## -&gt; class 0 [0.820] ## ## Rule 2: (246/54, lift 1.3) ## fare &gt; 7.725 ## fare &lt;= 10.4625 ## -&gt; class 0 [0.778] ## ## Rule 3: (315/59, lift 2.0) ## sex = female ## fare &gt; 10.4625 ## -&gt; class 1 [0.811] ## ## Rule 4: (16/3, lift 1.9) ## sex = female ## fare &lt;= 7.725 ## -&gt; class 1 [0.778] ## ## Rule 5: (82/32, lift 1.5) ## age &lt;= 9 ## -&gt; class 1 [0.607] ## ## Default class: 0 ## ## ## Evaluation on training data (1045 cases): ## ## Rules ## ---------------- ## No Errors ## ## 5 215(20.6%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 538 80 (a): class 0 ## 135 292 (b): class 1 ## ## ## Attribute usage: ## ## 90.43% sex ## 66.60% age ## 55.22% fare ## ## ## Time: 0.0 secs No todos los modelos de Machine Learning tienen la posiblidad de mostrar de manera tan intuitiva las reglas para clasificar o predecir un determinado caso. Esta es una importante ventaja de los árboles de decisión. Algo importante a aclarar de estas reglas es que no son exactamente las mismas que las que componen el árbol y, además, un caso puede estar cubierto más de una vez por alguna de las reglas. Esto es porque al no estar “obligado” a mostrar bifurcaciones en el árbol de decisión, lo que entrega son reglas y, al clasificar, elige la que tiene mayor accuracy. 7.4 Aplicación en el mercado de trabajo: monotributistas y cuentapropistas informales El sistema estadístico nacional tiene un serio problema para captar la naturaleza del trabajo independiente a lo largo del país. Una excepción a este problema generalizado fue la ENAPROSS del año 2015, en la cual se preguntó a los trabajadores independientes, entre otra cosas, si facturaban por su trabajo o no, es decir si eran monotributistas o no. Podemos aprender de esta encuesta para luego predecir, en base a variables que sí están en otras encuestas, como la Encuesta Permanente de Hogares (EPH). Usemos lo que aprendimos sobre el algoritmo C5.0 y los árboles de decisión más en general. load(file=url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/independientes.RData&quot;)) str(independientes) ## &#39;data.frame&#39;: 2783 obs. of 6 variables: ## $ NIVEL_ED : Factor w/ 7 levels &quot;Sin_instruccion&quot;,..: 3 4 4 4 3 5 2 3 4 4 ... ## $ REGISTRADO: Factor w/ 2 levels &quot;No_registrado&quot;,..: 1 1 1 1 1 2 1 1 1 1 ... ## $ INGRESO : Factor w/ 10 levels &quot;Decil1&quot;,&quot;Decil2&quot;,..: 1 10 1 3 7 7 1 8 5 7 ... ## $ EDAD : int 54 30 20 21 32 38 67 27 22 25 ... ## $ CAT_OCUP : Factor w/ 2 levels &quot;Patron&quot;,&quot;Independiente&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ REGION : Factor w/ 3 levels &quot;CABA&quot;,&quot;CONURBANO&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... El data frame independientes es una muestra de la ENAPROSS 2015, una encuesta a nivel nacional cuyo objetivo fue relevar ciertas características relacionados con la cobertura y calidad de la seguridad social en la Argentina y el empleo, entre otras condiciones sociales. Acá tenemos seis variables: el nivel educativo, si el trabajador independiente se encuentra registrado o no, el ingreso (según decil), la edad en años cumplidos, la categoría ocupacional (en este caso, si es independiente o patrón) y la Región del país, que en este caso queda segmentada entre CABA, CONURBANO y RESTO DEL PAÍS. primerArbol &lt;- C5.0(formula = REGISTRADO ~., data = independientes) summary(primerArbol) ## ## Call: ## C5.0.formula(formula = REGISTRADO ~ ., data = independientes) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue Apr 14 17:58:26 2020 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 2783 cases (6 attributes) from undefined.data ## ## Decision tree: ## ## NIVEL_ED = Superior_completo: Registrado (235/38) ## NIVEL_ED in {Sin_instruccion,Primaria_incompleta,Primaria_completa, ## : Secundaria_incompleta,Secundaria_completa,Superior_incompleta}: ## :...INGRESO in {Decil1,Decil2,Decil3,Decil4,Decil5,Decil6, ## : Decil7}: No_registrado (1984/320) ## INGRESO in {Decil8,Decil9,Decil10}: ## :...NIVEL_ED = Sin_instruccion: Registrado (0) ## NIVEL_ED in {Secundaria_completa,Superior_incompleta}: ## :...CAT_OCUP = Patron: Registrado (68/7) ## : CAT_OCUP = Independiente: ## : :...EDAD &lt;= 34: No_registrado (59/24) ## : EDAD &gt; 34: Registrado (194/48) ## NIVEL_ED in {Primaria_incompleta,Primaria_completa, ## : Secundaria_incompleta}: ## :...INGRESO = Decil8: No_registrado (100/25) ## INGRESO in {Decil9,Decil10}: ## :...CAT_OCUP = Patron: Registrado (17/4) ## CAT_OCUP = Independiente: ## :...INGRESO = Decil9: No_registrado (80/32) ## INGRESO = Decil10: Registrado (46/18) ## ## ## Evaluation on training data (2783 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 9 516(18.5%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 1822 115 (a): class No_registrado ## 401 445 (b): class Registrado ## ## ## Attribute usage: ## ## 100.00% NIVEL_ED ## 91.56% INGRESO ## 16.67% CAT_OCUP ## 9.09% EDAD ## ## ## Time: 0.0 secs Si les es más fácil para entenderlo, pueden plotearlo ¿Qué podemos decir del árbol que se creó? Enfoquémonos en Attribute usage: lo que hace es asignar la importancia de las variables según cuántos casos fueron clasificados usando a esa varaible. Por ejemplo, EDAD es usado en 253 (59+194) casos, que dividido por los 2783 casos que tenemos en este dataset dan 9,09%. De manera trivial, la primera de las variables es usada para clasificar todos los casos, por lo cual tiene 100% de importancia. Podríamos concluir que para nuestro modelo el nivel educativo y los ingresos son variables claves para asignar a un trabajador independiente como formal o no. Por otro lado podemos ver que tiene una tasa de error de 18,5% ¿Es mucho o poco? De nuevo, averiguemos cuántos trabajadores no registrados hay en nuestro dataset: table(independientes$REGISTRADO)/nrow(independientes) ## ## No_registrado Registrado ## 0.6960115 0.3039885 El 69.6% de los trabajadores independientes en nuestro dataset no se encuentra registrado, con lo cual si dijeramos que todos los trabajadores independientes son no regisitrados nos equivocaríamos en 30,4%. Nuestro árbol de decisión llegó a reducirlo al 18,5% Usemos nuestro modelo, ahora, para predecir nuestro dataset. La función predict hace exactamente esto: independientes &lt;- independientes %&gt;% mutate(PREDICCION = predict(primerArbol, newdata = independientes %&gt;% select(-REGISTRADO))) table(independientes$REGISTRADO, independientes$PREDICCION) ## ## No_registrado Registrado ## No_registrado 1822 115 ## Registrado 401 445 Esta es una tabla de confusión, como la que anteriormente vimos en el ejemplo del Titanic con summary(). Las filas indican la clasificación “real” de los casos, mientras que las columnas indican la que asignó nuestro modelo. La diagonal principal indica los casos correctamente clasificados, mientras que el que está arriba a la derecha nos marcan los falsos positivos, mientras que el elemento de abajo a la izquierda indica los falsos negativos. Si sumamos la diagonal (los correctamente clasificados) y lo dividimos por el total de casos obtenemos una importa medida de la performance de nuestro modelo: la accuracy sum(diag(table(independientes$REGISTRADO, independientes$PREDICCION))) / nrow(independientes) * 100 ## [1] 81.45886 La accuracy es solo una forma de medir la performance de nuestro modelo y nos va a servir en este tutorial para elegir entre modelos: el que tenga mayor accuracy es el que vamos a elegir. En este caso tenemos una accuracy de 81,5%, lo que implica que nuestro modelo tiene un lift de 81,5/69,6=1,18. Nuestro modelo es un 18% mejor que haber asignados a todos los casos con la proporción que conocemos de nuestra muestra. 7.4.1 Overfitting: aprendiendo demasiado de nuestra muestra Si tienen que estudiar para un examen en algún momento de su formación es muy probable que lo hayan hecho a través de modelos de exámenes anteriores. El objetivo no es solo prácticar lo que vieron en el curso, sino aprender sobre cómo toma examen la docemente. En general, esto suele funcionar, pero tiene un límite al cual probablemente llegaron: si aprenden estrictamente a resolver los parciales que tuvieron como prueba es sumamente probable que solo sepan responder con eficiencia esos parciales pero no otros con pequeñas diferencias. Como los humanos, los algoritmos pueden caer en el problema de aprender demasiado las especificidades de una muestra. El overfitting es uno de los principales problemas al entrenar un modelo de aprendizaje automático. Debemos garantizar que nuestro modelo NO funciona solo para la muestra, sino que los nuevos casos - los que queremos producir - también serán predichos de una manera razonable. De hecho, lo único que nos importa es la accuracy sobre una parte de la muestra que separamos y llamamos dataset de testing. Lo que pasa sobre nuestro dataset de training es secundario y solo lo utilizamos para detectar signos de overfiting. Vamos a ver un caso en el que la mejora en la eficiencia en training no redunda en mejoras en testing, es decir un caso de overfitting. No se preocupen por el código, es un poco complejo pero más adelante vamos a usar a la librería caret para que haga todo este trabajo de una manera más eficiente que nosotros. El código lo que hace es ir realizando una grid search en alguno de los parámetros de nuestro modelo. Los parámetros de los modelos definen, entre otras cosas, la estructura y la “velocidad” de aprendizaje del árbol, aunque siempre son específicas a los modelos. Una búsqueda en grid search (búsqueda en grilla) solo prueba un montón de valores para distintos parámetros y testea su accuracy. Una vez que se encuentra el valor máximo, esos serán los parámetros elegidos del modelo. No se preocupen si les lleva un tiempo, es natural ya que está entrenando muchos modelos # Eliminamos la variable que tiene la predección independientes &lt;- independientes %&gt;% select(-PREDICCION) set.seed(2) # Creamos la &quot;Grid Search&quot; de dos parámetros cfOpciones &lt;- seq(0.8,1,0.01) minCasesOpciones &lt;- seq(0,50,1) # Generamos los índices (números de filas) que van a ser de testing indexTest &lt;- sample.int(n = nrow(independientes),size = 0.3*nrow(independientes)) independientesTest &lt;- independientes[indexTest, ] independientesTraining &lt;- independientes[-indexTest, ] modelPerformance &lt;- list() for(cf in cfOpciones){ for(minCases in minCasesOpciones) { # Para cambiar los parámetros presten atención a que debemos usar la función C5.0Control model &lt;- C5.0(REGISTRADO ~., data = independientesTraining, control= C5.0Control(CF = cf, minCases = minCases)) prediccionesTrain &lt;- predict(model, independientesTraining) trainAcc &lt;- sum(prediccionesTrain==independientesTraining$REGISTRADO)/nrow(independientesTraining) prediccionesTest &lt;- predict(model, newdata = independientesTest) testAcc &lt;- sum(prediccionesTest==independientesTest$REGISTRADO)/nrow(independientesTest) salida &lt;- data.frame(cf, minCases,trainAcc,testAcc) modelPerformance &lt;- c(modelPerformance, list(salida)) } } modelPerformance &lt;- plyr::rbind.fill(modelPerformance) Ahora veamos cómo fue la evolución de la accuracy tanto en training como testing (y de paso aprendemos un poco más sobre ggplot2) modelPerformance &lt;- modelPerformance %&gt;% group_by(minCases) %&gt;% summarise(Training = mean(trainAcc), Testing = mean(testAcc)) %&gt;% gather(key = &quot;dataset&quot;,value=&quot;acc&quot;,-minCases) # Esta librería nos da la opción de agregar nuevos &quot;temas&quot; de ggplot # que no vienen con la librería library(ggthemes) ggplot(modelPerformance) + geom_line(aes(x = minCases,y = acc, color = dataset), size = 1.5) + theme_fivethirtyeight() + scale_color_fivethirtyeight() + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + scale_x_reverse() + labs(title = &quot;La forma del overfitting&quot;, subtitle = &quot;Accuracy según el valor del parámetro minCases&quot;, caption = &quot;Elaboración propia con base en datos de ENAPROSS 2015&quot;) + theme(legend.title = element_blank()) En el gráfico queda bastante claro como desde aproximadamente el valor minCases = 15 la accuracy en el dataset de testing crece sin parar pasando de aproximadamente 81% a 86%, mientras que la de testing tiene una leve tendencia a la caída. En este caso, estos parámetros no muestran un elevado overfitting. En otras situaciones, el overfitting puede ser tal que la accuracy sobre el dataset de testing caiga (y mucho) siempre hay que tenerlo en cuenta. 7.5 Algunos árboles no solo clasifican: árboles de regresión Aunque suene contraintuitivo, algunos árboles de decisión pueden dividir el espacio de nuestras variables en base a valores no solo categóricos (como cuando clasificamos), sino en valores numéricos continuos. Aunque suene raro, veremos que lo que hace es relativamente fácil de comprender. Para esto, vamos a trabajar con un dataset sobre el precio de los inmuebles en la Ciudad de Buenos Aires que descargué desde la división de datos de Properati. Pero para eso vamos a tener que hacer un Data Wrangling un poco más intenso. 7.5.1 Poniendo en forma los datos Los datos que descargué pueden bajarlos ustedes, como siempre, con read.table(): avisosInmuebles &lt;-read.table(file = url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/datosProperati.csv&quot;), sep=&#39;;&#39;,header = TRUE,stringsAsFactors = FALSE) Tenemos unas cuantas variables, usemos glimpse() para ver cuáles son y su título: glimpse(avisosInmuebles) ## Rows: 62,009 ## Columns: 12 ## $ created_on &lt;chr&gt; &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;2019-02-23&quot;, &quot;201... ## $ rooms &lt;int&gt; 3, 4, 1, 3, 4, 2, 5, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA, 1, ... ## $ bathrooms &lt;int&gt; 1, 2, 1, 1, 2, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, 1, 1,... ## $ surface_total &lt;int&gt; 62, 200, 28, 55, 200, 54, 113, 441, 1296, 13, 12, 10, 12, 12, 13, 12, 29, ... ## $ surface_covered &lt;int&gt; 62, 100, 28, 55, 100, 44, 88, NA, NA, 13, 12, NA, 12, 12, 13, 12, 29, 39, ... ## $ price &lt;int&gt; 170000, 237000, 83000, 85000, 237000, 75000, 690000, 1100000, 40000, 16000... ## $ currency &lt;chr&gt; &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD... ## $ title &lt;chr&gt; &quot;PH - Almagro&quot;, &quot;PH En Venta - Valez Sarsfield&quot;, &quot;Monoambiente Caballito. ... ## $ description &lt;chr&gt; &quot;&lt;br&gt;Lindísmo PH de 62 m2. Renovado. Sin Expensas. Apto Crédito.&lt;br&gt;&lt;br&gt;Li... ## $ property_type &lt;chr&gt; &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Lote&quot;, &quot;Lote&quot;, &quot;Cochera&quot;, &quot;... ## $ operation_type &lt;chr&gt; &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;V... ## $ BARRIO &lt;chr&gt; &quot;ALMAGRO&quot;, &quot;VELEZ SARSFIELD&quot;, &quot;VILLA GRAL. MITRE&quot;, &quot;MATADEROS&quot;, &quot;VELEZ SAR... Los nombres de las variables parecen bastante descriptivos. Podemos ver, además, que nuestro data frame cuenta con información sobre diversos tipos de propiedades: nosotros queremos trabajar con inmuebles aptos para vivienda ya que son los únicos para los que aplican varias de las variables del dataset: avisosInmuebles &lt;- avisosInmuebles %&gt;% filter(property_type %in% c(&quot;Casa&quot;,&quot;Departamento&quot;,&quot;PH&quot;)) Además, con glimpse() pudimos ver que algunas de nuestras variables tienen datos faltantes: rooms, bathrooms y surface_covered. Veamos cuántos de cada uno sum(is.na(avisosInmuebles$rooms)) ## [1] 3827 sum(is.na(avisosInmuebles$bathrooms)) ## [1] 2088 sum(is.na(avisosInmuebles$surface_covered)) ## [1] 1409 La que parece tener más datos faltantes es rooms, hagamos un poco de data wrangling para poder completar estos casos en base al título o descripción del inmueble: avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(ambientes=str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string= title)) %&gt;% mutate(ambientes=ifelse(is.na(ambientes), str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string=description), ambientes)) %&gt;% mutate(ambientes=as.numeric(str_extract(pattern=&#39;\\\\d&#39;,ambientes))) %&gt;% mutate(ambientes=ifelse(ambientes == 0,NA,ambientes)) ¿Qué es lo que hicimos? Varias cosas, pero vayamos por partes. En primer lugar, creamos una variable ambientes para la que usamos la función str_extract(), ya sea en title o description usando el pattern ‘(?i)\\d.amb’ ¿Qué es lo que hace? (?i) dice que no le preste atención si una parte del texto está en mayúscula o minúscula (es decir, que haga una búsqueda que no sea case sensitive). Luego, \\d.amb devuelve el primer dígito que encuentra a la izquierda de las palabras “amb” ¿Para qué hacemos esto? para que si un título dice “3 Ambientes”, levante el “3 Amb”, o si dice 2 AMB, que retenga todo. Luego de que creamos esta variable, nos quedamos solo con el número aplicando str_extract(pattern=“\\d”,…). Finalmente, si lo que devolvió de ambientes fue igual 0, entonces que le ponga NA porque eso no es un número válido de ambientes. Si se fijan cuántos datos faltantes tiene nuestra variable van a ver que son muchos (13.313, para ser exactos). Pero en el resto de los casos ¿cuántos coincide con la variable rooms, provista por Properati? table(avisosInmuebles$ambientes==avisosInmuebles$rooms) ## ## FALSE TRUE ## 2853 32890 No parece estar nada mal ! en 32890 de los 35743 casos donde coinciden arrojan la misma cantidad de ambientes. Vamos a completar la varaible rooms con estos datos: avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(rooms = ifelse(is.na(rooms), ambientes, rooms)) sum(is.na(avisosInmuebles$rooms)) ## [1] 1011 Pueden replicar la misma idea para superficies cubierta o para los baños. Para lo que sigue de este capítulo podemos trabajar simplemente quedándonos con los casos completos de nuestro data frame, pero antes vamos a eliminar tambien algunas variables que no usaremos para la predicción: avisosInmuebles &lt;- avisosInmuebles %&gt;% select(-created_on,-currency,-title,-description,-operation_type,-ambientes) %&gt;% filter(complete.cases(.)) Listo, ya estamos en condiciones de crear nuestro primer árbol de regresión, pero esta vez deberemos usar otra implementación de los árboles de regresión que nos brinda el paquete rpart() Si prestaron atención, el último código de R usamos la función complete.cases() dentro del verb filter(). Pero cuando lo hicimos, dentro de la primera función usamos un punto ¿Qué representa ese punto en ese contexto? los datos hasta ese momento. Es decir, le estamos diciendo que aplique la función complete.cases() a todas las filas y las columnas que quedaron luego de select y que las filtre. Esta forma de usar funciones nos ahorra tener que asignar nuevamente los datos y encadenar todo en un mismo conjunto de pipes. 7.5.2 Recursive PARTitioning (RPART) El paquete RPart nos brinda otra implementación de los árboles de decisión, una que nos permite trabajar con una variable numérica como variable a la que queremos predecir. Como siempre, debemos instalar nuestros paquetes antes de usarlos. Una vez que lo tengan instalado, solo tienen que cargarlo. Para hacer gráficos de rplot, van a tener que instalar otro paquete: rpart.plot(). require(rpart) require(rpart.plot) avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(USDm2=price/surface_total) arbolRegresion &lt;- rpart(formula = USDm2 ~ rooms + BARRIO + bathrooms + property_type, data = avisosInmuebles,control = rpart.control(cp = 0.01)) rpart.plot(arbolRegresion) En mi experiencia, la mejor forma de entender los árboles de RPart no son sus gráficos, sino usar rpart.rules(). Pero antes de hacer eso, usemos el gráfico para ver el primero de los valores, el que está en el primer nodo: dice 2751. Ahora saquemos el promedio de los precios de los inmuebles round(mean(avisosInmuebles$USDm2),0) ## [1] 2751 ¡Coincide! Lo que nos muestra este árbol es, para cada nodo, el promedio de los precios de los inmuebles y la cantidad de casos cubiertos desde ahí en adelante. Sin embargo, las “reglas” por las que va a clasificar se encuentran solo en los nodos raíz de más bajo nivel, las que dicen 1747 (13%), 2230 (21%), 2659 (29%), 3363 (36%) y 6137 (1%). Para ver mejor cuáles son las reglas ejecutemos la función rpart.rules() View(rpart.rules(arbolRegresion)) La variable que más usó fue barrios, y solo usa la variable property_type para algunos subconjuntos de barrios. Este árbol dirá que el precio en dólares por metro cuadrado para Puerto Madero es de USD 6.137, por ejemplo. Ahora bien ¿Es el promedio observado? round(mean(avisosInmuebles$USDm2[avisosInmuebles$BARRIO==&quot;PUERTO MADERO&quot;]),0) ## [1] 6137 Sí, coincide. Y eso es exactamente lo que hace un árbol de regresión: elige cómo segmentar a las variables y a cada nodo le asigna como valor el promedio. Ahora bien, antes introdujimos la idea de entropía como guía para ir particionando nuestro espacio de varaibles, pero ¿Qué usó ahora?. La respuesta es el RMSE (Root Mean Squared Error), es decir el promedio de la raiz cuadrada de los errores de predicción. Para cada nodo de nuestro árbol, él se va a preguntar: ¿qué variables y qué valores de esas variables maximizan la caída en el RMSE? Y con ese principio en mente termina de cubrir todos los casos. Veamos cuál es la caída en el RMSE entre asignar para cada uno de los inmuebles el valor del promedio de los inmuebles y cuánto cambia con la primera apertura, en la que usa la variable BARRIOS prediccionInicial &lt;- mean(avisosInmuebles$USDm2) rmseInicial &lt;- sqrt(mean((prediccionInicial-avisosInmuebles$USDm2)^2)) rmseInicial ## [1] 1446.731 Ahora veamos qué pasa con este error al abrir por la primera variable. No se ve del todo claro, pero en el gráfico y en las reglas podemos entender que el árbol pregunta de que barrio es y genera tres bifucarciones: 1) PUERTO MADERO, 2) BELGRANO, COUGHLAN, COLEGIALES, NUÑEZ, PALERMO, RECOLETA Y RETIRO, 3) Otros barrios. Veamos el RMSE de esta clasificacion prediccionBarrios &lt;- ifelse(avisosInmuebles$BARRIO == &quot;PUERTO MADERO&quot;, 6137, ifelse(avisosInmuebles$BARRIO %in% c(&quot;BELGRANO, COUGHLAN&quot;,&quot;COLEGIALES&quot;,&quot;NUÑEZ&quot;,&quot;PALERMO&quot;,&quot;RECOLETA&quot;,&quot;RETIRO&quot;),3363, 2332)) rmseBarrios &lt;- sqrt(mean((prediccionBarrios-avisosInmuebles$USDm2)^2)) rmseBarrios ## [1] 1332.865 rmseBarrios / rmseInicial - 1 ## [1] -0.0787057 Esa apertura generó una caída de aproximadamente 8% en el RMSE de las predicciones y, dado el algoritmo de generación del árbol y los párametros elegidos, es la apertura que más mejora este indicador. 7.6 Ejercicio En base a lo aprendido en este capítulo, entrenar un árbol de decisión con el dataset de Titanic, pero esta vez separando entre training (70% del dataset) y testing (30%) del dataset. Además, prueben dos parámetros distintos (mincases 5 y mincases 100) y estimen la accuracy (o tasa de acierto) tanto en el dataset de training como testing ¿Con cuál de los dos modelos se quedarían para predecir quién sobrevivió o no en el Titanic? ¿Por qué? 7.7 Lecturas recomendadas Para profundizar y/o reforzar algunos de los puntos de este capítulo recomiendo la lectura del Capítulo 8 de Introduction to Statistical Learning de James, Witten, Hastie y Tibsharani Para un tratamiento de divulgación, didáctico y estimulante recomiendo nuevamente la lectura del libro de Walter Sosa Escudero linkeado al final del capítulo 4. Para una aproximación más teórica de Machine Learning recomiendo la lectura del libro de Thomas Mitchell: Machine Learning. "],
["un-paquete-para-dominarlos-a-todos.html", "8 Un paquete para dominarlos a todos 8.1 Classification And Regression Training (CARET) 8.2 Entrenando un árbol de decisión 8.3 Entrenando un árbol de regresión 8.4 Ejercicio", " 8 Un paquete para dominarlos a todos En los últimos dos capítulos del libro nos hemos introdujido en dos de las técnicas más difundidas en la estadística y en el aprendizaje automático. En este capítulo vamos a introducirnos a un paquete que nos permite estimar estos y otros modelos de una manera unificada y ahorrándonos los ciclos de programación que usamos, por ejemplo, en el capítulo 5 para optimizar los parámetros del Árbol de Decisión. 8.1 Classification And Regression Training (CARET) Como ya se adelantó, CARET es una librería que brinda varias funcionalidades útiles para crear modelos predictivos. Esas funcionalidades incluyen transformaciones de datos, visualizaciones y distintos métodos de análisis de capacidad predictiva. Para un análisis exhaustivo de las funcionalidades de este paquete les recomiendo visitar la muy buena página de ayuda del paquete: https://topepo.github.io/caret/ En este capítulo haremos especial énfasis en la forma en la cual podemos usar distintos algoritmos de aprendizaje automático y qué estrategias tenemos para optimizar los parámetros de tal manera que maximicen su performance sobre nuevos datos y no caer ni en el underfitting ni en el overfitting. 8.2 Entrenando un árbol de decisión Con el objetivo de aprender a crear distintos algoritmos de aprendizaje automático, primero vamos a replicar en caret lo que hicimos en el capítulo 5 con el dataset de trabajadores independientes. load(file=url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/independientes.RData&quot;)) El data frame independientes cuenta con 2.783 observaciones de trabajadores independientes. La variable REGISTRADO indica si ese trabajador se encuentra registrado o no, mientras que el resto de las variables potencialmente podrían ser útiles para clasifiar a un trabajador independiente como registrado o no. Usaremos nuevamente la implementación de árboles de decisión y también partiremos a nuestros datos en un 70% que será el dataset de entrenamiento y un 30% que se convertirá en el dataset de testing. require(caret) require(tidyverse) set.seed(4) # Esta línea solo intenta que tengamos los mismos números aleatorios trainingIndexes &lt;- createDataPartition(y = independientes$REGISTRADO, p = 0.7, list = FALSE) La función createDataPartition() de CARET nos devuelve un conjunto de índices tomados al azar en base a algunos parámetros tales como la proporción (parámetro p) que queremos que tenga de tamaño nuestro dataset. Si prestan atención a trainingIndexes se darán cuenta que la función devuelve una matriz de una sola columna con 1949 números, que son los índices de nuestro dataset de entrenamiento: 1949 es exactamente el 70% de 2783, la cantidad de filas de nuestro dataset. Con estos índices ya podemos crear nuestro dataset de training y testing: training &lt;- independientes[ trainingIndexes,] testing &lt;- independientes[-trainingIndexes,] Una vez que tenemos un dataset de entrenamiento y validación (o testing) estamos en condiciones de entrenar un modelo ¿Cómo se hace con caret? con la función train(). Pero a esta función debemos pasarle algunos controles, como por ejemplo qué parámetros tunear y cómo hacerlo. Para la primera de estas preguntas, qué parámetros tunear, debemos pasar una matriz que tenga en las columnas los parámetros a tunear y en las filas los valores que queremos probar. Aunque esto suena difícil, si usamos la función expand.grid() se hace mucho más fácil. No todos los parámetros de todos los modelos de CARET pueden ser “afinados”. En https://topepo.github.io/caret/available-models.html podemos ver qué modelos pueden usarse en CARET y cuáles son los parámetros que podemos optimizar. Para nuestor modelo C5.0, estos son trials, model y winnow. La documentación de C5.0 en R explica en detalle qué hace cada uno de estos modelos, a modo de resumen: Trials: Define la cantidad de boosting que tiene nuestro árbol. Boosting es tan solo un método para combinar diversos clasificadores (más de un árbol) para poder mejorar la performance de nuestro modelo. En el ejemplo del capítulo 5 lo dejamos en 1 (teníamos solo un árbol), aquí probaremos que pasa si aumentamos este valor Model: indicamos si queremos un modelo de árboles o de reglas, dejaremos siempre árboles. Winnow: Este parámetro nos dice si queremos que el árbol elija descartar algunas variables para mejorar la capacidad predictiva del modelo Creemos la grilla con los valores que queremos probar: grilla &lt;- expand.grid(trials=c(1:4), winnow=c(TRUE,FALSE), model=&quot;tree&quot;) glimpse(grilla) ## Rows: 8 ## Columns: 3 ## $ trials &lt;int&gt; 1, 2, 3, 4, 1, 2, 3, 4 ## $ winnow &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE ## $ model &lt;fct&gt; tree, tree, tree, tree, tree, tree, tree, tree Bien, ahora debemos decirle cómo queremos que vaya evaluando a los modelos, eso lo podemos hacer con la función trainControl(). Podemos customizar varios parámetros, por ejemplo si queremos hacer cross validation o no, y la cantidad de bins que queremos que haga. Vamos a crear las instrucciones para exactamente esto: trainingInstructions &lt;- trainControl(method = &quot;cv&quot;, number = 3) En method escribimos “cv”, por Cross Validiation, mientras que en number incluímos la cantidad de conjuntos en los que queremos que divida a nuestro dataset de training. Ahora estamos en condiciones de entrenar nuestro primer modelo con Caret: CaretC50 &lt;- train( y=training %&gt;% select(REGISTRADO) %&gt;% unlist(), x = training %&gt;% select(-REGISTRADO), method = &quot;C5.0&quot;, metric = &quot;Accuracy&quot;, tuneGrid = grilla, trControl = trainingInstructions ) Veamos que nos devuelve cuando usamos summary(): summary(CaretC50) Debería haber devuelto algo muy similar a lo que veíamos con el paquete C5.0. Esto tiene lógica porque lo que devuelve Caret es el modelo que mejor medida de accuracy tuvo, en el mismo “formato” que devolvería el paquete que tiene el código con el que se entrena el modelo, en este caso C5.0 ¿Y cómo sabemos cual combinación de nuestra grilla es la que mostró la mejor performance? CaretC50 ## C5.0 ## ## 1949 samples ## 5 predictor ## 2 classes: &#39;No_registrado&#39;, &#39;Registrado&#39; ## ## No pre-processing ## Resampling: Cross-Validated (3 fold) ## Summary of sample sizes: 1300, 1299, 1299 ## Resampling results across tuning parameters: ## ## winnow trials Accuracy Kappa ## FALSE 1 0.7927123 0.4621135 ## FALSE 2 0.7901466 0.4543004 ## FALSE 3 0.7937387 0.4852959 ## FALSE 4 0.7942515 0.4631932 ## TRUE 1 0.7922002 0.4572151 ## TRUE 2 0.7916874 0.4584683 ## TRUE 3 0.7957923 0.4912082 ## TRUE 4 0.8019478 0.4844389 ## ## Tuning parameter &#39;model&#39; was held constant at a value of tree ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were trials = 4, model = tree and winnow = TRUE. La salida print() de nuestro objeto de Caret nos da información sobre la performance de cada uno de los parámetros que probamos. Dependiendo del azar, les aparecerá uno como el mejor, probablemente por una diferencia muy pequeña en la accuracy. finalmente, en la última linea de la salida les dice cual fue el que se eligió y por qué criterio. Con nuestro modelo ya estimado y entrenado podemos predecir, como siempre, sobre otro dataset. En este caso, el que tenemos de validación o testing: testing &lt;- testing %&gt;% mutate(prediccion=predict(CaretC50,newdata = testing)) La función que tenemos que usar para predecir en base a un modelo es siempre la misma: predict(). Para usarlo con datos nuevos que no sean con los que se entrenó el modelo, debemos pasarlo en “newdata”, pero este data frame debe tener variables con los mismos nombres y, en caso de no ser númerica, tomar los mismos valores ya que sobre esos aprendió el modelo. Calculemos la accuracy sobre los datos de validación: sum(testing$REGISTRADO==testing$prediccion)/nrow(testing)*100 ## [1] 79.01679 8.3 Entrenando un árbol de regresión En Caret es realmente simple entrenar distintos modelos. Solo tenemos que conocer si está disponible dentro del paquete, saber cómo debemos llamarlo en el parámetro method dentro de train y conocer también cuáles parámetros podemos optimizar. Probemos esta posibilidad de cambiar de modelos usando el mismo dataset de inmuebles de Properati que usamos en el capítulo anterior. En el siguiente código de R voy a hacer todos los pasos de Data Wrangling que se describen en ese capítulo avisosInmuebles &lt;-read.table(file = url(&quot;https://github.com/martintinch0/CienciaDeDatosParaCuriosos/raw/master/data/datosProperati.csv&quot;), sep=&#39;;&#39;,header = TRUE,stringsAsFactors = FALSE) avisosInmuebles &lt;- avisosInmuebles %&gt;% filter(property_type %in% c(&quot;Casa&quot;,&quot;Departamento&quot;,&quot;PH&quot;)) avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(ambientes=str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string= title)) %&gt;% mutate(ambientes=ifelse(is.na(ambientes), str_extract(pattern = &quot;(?i)\\\\d.amb&quot;, string=description), ambientes)) %&gt;% mutate(ambientes=as.numeric(str_extract(pattern=&#39;\\\\d&#39;,ambientes))) %&gt;% mutate(ambientes=ifelse(ambientes == 0,NA,ambientes)) avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(rooms = ifelse(is.na(rooms), ambientes, rooms)) avisosInmuebles &lt;- avisosInmuebles %&gt;% select(-created_on,-currency,-operation_type,-ambientes) %&gt;% filter(complete.cases(.)) avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(USDm2=price/surface_total) Si se fijan en https://topepo.github.io/caret/available-models.html van a ver que existen algunas implementaciones distintas de rpart. Usaremos la que pide usar el método ‘rpart2’ porque nos deja mejorar el parámetro maxdepth, que determina que tan “profundo” puede ser el árbol resultante. Como solo es una variable, usar expand.grid() no tiene mucho sentido, pero para acostumbrar a trabajar de una manera ordenada vamos a usarlo igual: grilla &lt;- expand.grid(maxdepth=c(1:10)) También vamos a cambiar levemente los controles de entrenamiento y elegir que divida a nuetro dataset en 5 partes para el cross validation: trainingInstructions &lt;- trainControl(method = &quot;cv&quot;, number = 5) Ya estamos en condiciones para entrenar al modelo de rpart: rpartModel &lt;- train( y=avisosInmuebles %&gt;% select(USDm2) %&gt;% unlist(), x = avisosInmuebles %&gt;% select(rooms,BARRIO, bathrooms, property_type, surface_covered, surface_total), method = &quot;rpart2&quot;, metric = &quot;RMSE&quot;, tuneGrid = grilla, trControl = trainingInstructions ) ¿Cuál fue el modelo que eligió? Puede variar según los números aleatorios que toma para segmentar a los conjuntos sobre los que se entrena y valida, pero nuevamente pueden detectarlo usando el método print() print(rpartModel) # Es lo mismo que poner solo &quot;rpartModel&quot; ## CART ## ## 47752 samples ## 6 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 38202, 38202, 38201, 38201, 38202 ## Resampling results across tuning parameters: ## ## maxdepth RMSE Rsquared MAE ## 1 1272.367 0.1859278 634.2621 ## 2 1234.055 0.2399316 609.0526 ## 3 1207.551 0.2760798 574.3846 ## 4 1242.231 0.2391191 559.7036 ## 5 1242.231 0.2391191 559.7036 ## 6 1242.231 0.2391191 559.7036 ## 7 1242.231 0.2391191 559.7036 ## 8 1242.231 0.2391191 559.7036 ## 9 1242.231 0.2391191 559.7036 ## 10 1242.231 0.2391191 559.7036 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was maxdepth = 3. También podríamos intentar verlo gráficamente (dónde minimiza RMSE): plot(rpartModel) 8.4 Ejercicio Usando los datos de avisosInmuebles ya procesados, hagan el siguiente trabajo de Data Wrangling: avisosInmuebles &lt;- avisosInmuebles %&gt;% mutate(gimnasio = ifelse(grepl(pattern = &quot;gym|gimn&quot;,x = description) | grepl(pattern = &quot;gym|gimn&quot;, x = title), TRUE, FALSE), cochera = ifelse(grepl(pattern = &quot;coch|garage&quot;,x = description) | grepl(pattern = &quot;coch|garage&quot;, x = title), TRUE, FALSE), pileta = ifelse(grepl(pattern = &quot;pileta|piscina&quot;,x = description) | grepl(pattern = &quot;pileta|piscina&quot;, x = title), TRUE, FALSE)) Lo que hace el código es crear tres variables (gimnasio, cochera, pileta) que intenta identificar la presencia de esas tres características de los inmuebles en cada una de las publicaciones ¿Pueden explicarlo en palabras cómo lo hace? Tengan en cuenta que la función grepl() devuelve una lista de TRUE/FALSE dependiendo de si se encuentra lo que está dentro de “pattern” en el vector que se provee en el parámetro “x”. Independientemente de esa interpretación, usando Caret entrenen un árbol de regresión con rpart2 y con los mismos parámetros que están en el cuerpo del capítulo ¿Cuánto cambió el RMSE con la inclusión de las tres variables que creamos? ¿Sugiere un parámetro de maxdepth distinto al que sugería cuando no incorporamos estas variables? "],
["anexo-1-datasets.html", "9 Anexo 1 - Datasets 9.1 Encuesta Permanente de Hogares (EPH) 9.2 Precios de los inmuebles", " 9 Anexo 1 - Datasets Para practicar algunas de las herrmientas disponibles en este libro es relevante contar con algunos conjuntos de datos que sean aptos para aplicarlos. En este anexo se irán disponibilizando diversos datasets y su metadata para poder trabajar con ellos. 9.1 Encuesta Permanente de Hogares (EPH) La Encuesta Permanente de Hogares (EPH) es una encuesta que, con sus hiatos, se realiza trimestre tras trimestre por el INDEC de Argentina. La encuesta es representativa de aproximadamente el 64% de la Argentina, en todos los casos relevando áreas urbanas. Desde la sección de Bases de Datos del INDEC van a encontrar las últimas bases de datos puestas a disposición por el INDEC. Desde este link van a poder descargar las encuestas para el cuarto trimestre de 2018. Se trata de una individual, que tiene los datos a nivel de personas de los hogares, y otra de hogares que tiene variables sobre características específicas de los hogares y sus viviendas también. La versión de esta encuesta es igual a la original, con una sola salvedad: existe una variable POBREZA en los datos de los hogares que fue creada en base a la línea de pobreza para cada una de las regiones para ese momento. Las variables de la EPH son muchas y les recomiendo que ingresen aquí para ver cuáles son las variables disponibles y cuáles valores pueden tomar. 9.2 Precios de los inmuebles Otro interesante dataset para trabajar consiste en la información sobre los anuncios que Properati hace pública. Aunque para hacer la descarga de los datos hoy en día hay que usar el servicio de Big Query de Google, haciendo click aquí pueden descargar una consulta que yo realicé. Las variables pueden comprenderse simplemente por el nombre que traen. Los campos title y description son muy relevantes para poder crear nuevas variables, tal como hicimos en los capítulos 5 y 6 de este libro. "],
["anexo-2-otras-funciones-de-data-wrangling.html", "10 Anexo 2 - Otras funciones de Data Wrangling 10.1 Convirtiendo una variable númerica a categórica", " 10 Anexo 2 - Otras funciones de Data Wrangling Las herramientas del Data Wrangling del capítulo 2 son tan solo una introducción a las múltiples tareas que hay que realizar. Es imposible ser exhaustivo en las funciones que existen para cada una de las tareas, y con el tiempo van a tener que aprender a Googlear estas soluciones. Con todo, aquí van algunas funciones que son tan necesarias como las descriptas en el capítulo 2 y que las complementan. 10.1 Convirtiendo una variable númerica a categórica En distintas ocasiones uno quiere convertir una variable númerica a una variable categórica. Las razones pueden ser variadas. Por ejemplo, un problema típico con los datos es que a veces se comparten con variables numéricas que representan una categoría ¿Los hace acordar a algo esto? Son los factor que R reconoce como una clase de vectores específica. Por momentos esto no trae muchos problemas, porque si sabemos qué significa cada código podemos hacer filtros y agrupar sin grandes dificultades. El problema surge cuando usamos modelos de regresión o de árboles, que no tienen razón para saber que esa variable es categórica. Lo mismo sucede si quieren hacer gráficos. En esos casos debemos convertirla en factores. En rigor, ya vimos como hacer esto en otros capítulos de este libro. La diferencia aquí es que, además de reforzar el concepto, vamos a aprender a usar una función importante. Primero, voy a cargar los datos para los individuos de la Encuesta Permanente de Hogares (EPH) del corte del cuarto trimestre de 2018 (pueden encontrar estos datos en el Anexo I) datos &lt;- data.table::fread(&quot;Data/Datasets/datosIndividuos4t2018.csv&quot;) Varias variables de este dataset son, en realidad, factores. Una de ellas es la variable REGION. Tidyverse (en realidad, uno de sus paquetes, dplyr) tiene una función que nos va a servir para usarla correctamente en modelos y gráficos: recode() library(tidyverse) datos &lt;- datos %&gt;% mutate(REGION_CAT = recode(REGION, `1`=&quot;GBA&quot;,`40`=&quot;NOA&quot;,`41`=&quot;NEA&quot;,`42`=&quot;Cuyo&quot;,`43`=&quot;Pampeana&quot;,`44`=&quot;Patagónica&quot;)) unique(datos$REGION) ## [1] 43 41 44 42 40 1 unique(datos$REGION_CAT) ## [1] &quot;Pampeana&quot; &quot;NEA&quot; &quot;Patagónica&quot; &quot;Cuyo&quot; &quot;NOA&quot; &quot;GBA&quot; Como pueden ver en el ejemplo, recode() solo necesita que le pasemos el vector a modificar, y luego separado por coma todos los valores que hay que cambiar y luego, con un igual el valor que queremos que tome. Por ejemplo, el número 1 ahora será GBA y el 40, NOA. Noten que al tratarse de números en este caso tenemos que ponerles los siguientes signos ``. Si bien existen formas de solucionar esto, para mí el paquete plyr tiene una función que hace lo mismo pero un poco más simple (¡recuerden que para que esto funcione tienen que tener instalado plyr!) datos &lt;- datos %&gt;% mutate(REGION_CATPLYR = plyr::mapvalues(REGION, from=c(1,40,41,42,43,44), to=c(&quot;GBA&quot;,&quot;NOA&quot;,&quot;NEA&quot;,&quot;Cuyo&quot;,&quot;Pampeana&quot;,&quot;Patagónica&quot;))) all(datos$REGION_CAT==datos$REGION_CATPLYR) # Mismo resultado ## [1] TRUE La función mapvalue() pide los mismos datos que recode, pero en distinto formato. Solo tenemos que pasarle dos vectores: uno que tenga los valores que queremos convertir y otro a los valores que queremos convertirlos. Tienen que ser del mismo tamañao ya que va a fijarse en cada posición de asignarle el valor correspondiente en el otro valor. Así el valor 1 corresponde a GBA, 40 a NOA y así en adelante. Ninguna de las dos funciones devuelve factores. No siempre necesitan que sean factores, pero en caso que sea necesario pueden usar la función ya vista en el cuerpo del libro: datos &lt;- datos %&gt;% mutate(REGION_CAT = factor(REGION_CAT)) unique(datos$REGION_CAT) ## [1] Pampeana NEA Patagónica Cuyo NOA GBA ## Levels: Cuyo GBA NEA NOA Pampeana Patagónica En otras ocasiones queremos convertir a variables númericas a categorías, pero no con una correspondencia de “1 a 1”. Esto puede pasar, por ejemplo, con los ingresos. Imaginen que queremos separar a aquellos que ganan mas de 20.000 pesos de aquellos que ganan ese número o menos de ese valor en la ocupación principal (variable P21). Podemos usar una función que ya vimos varias veces en el libro: ifelse() datos &lt;- datos %&gt;% mutate(INGRESOS20K=ifelse(P21&gt;20000,&quot;Más de 20k&quot;,&quot;Menos de 20k&quot;)) table(datos$INGRESOS20K) ## ## Más de 20k Menos de 20k ## 5729 51689 ifelse() solo nos pide que le pasemos una condición que va a fijarse si es verdadera o falsa (si el valor de P21 es mayor a 20000, en este caso). Si es verdadero, hará lo que ponemos inmediatamente después de la coma. Si es falso, hará lo que va después de la segunda coma, fácil ¿No?. Si quieren agregar más puntos de quiebre solo tienen que añidar los ifelse() datos &lt;- datos %&gt;% mutate(INGRESOSCAT=ifelse(P21&gt;30000,&quot;Más 30k&quot;, ifelse(P21&gt;20000,&quot;Más de 20k&quot;,&quot;Menos de 20k&quot;))) "]
]
